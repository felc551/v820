#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Sistema de Treinamento OTIMIZADO - COM DETEC√á√ÉO AUTOM√ÅTICA DE CAMPOS
Vers√£o melhorada que detecta automaticamente a estrutura do JSON
"""

import os
import json
import logging
import subprocess
import shutil
import gc
import warnings

# Filtros de avisos
warnings.filterwarnings('ignore', category=FutureWarning)
warnings.filterwarnings('ignore', category=UserWarning, message='.*pad_token.*')
warnings.filterwarnings('ignore', category=UserWarning, message='.*token_type_ids.*')
from pathlib import Path
from typing import Dict, List
from datetime import datetime
import zipfile
try:
    from flask_socketio import SocketIO
    HAS_SOCKETIO = True
except ImportError:
    HAS_SOCKETIO = False
# Importa√ß√µes condicionais para evitar erros
try:
    import yaml
except ImportError:
    yaml = None

try:
    import torch
    from transformers import (
        AutoModelForCausalLM,
        AutoTokenizer,
        TrainingArguments,
        Trainer,
        DataCollatorForLanguageModeling,
        TrainerCallback
    )
    from datasets import Dataset
    from peft import LoraConfig, get_peft_model, TaskType, PeftModel
    HAS_TORCH = True
except ImportError:
    HAS_TORCH = False

# Registrar modelo USBABC
try:
    from modeling_usbabc import USBABCConfig, USBABCForCausalLM
    from transformers import AutoConfig, AutoModelForCausalLM
    
    # Registrar no sistema transformers
    AutoConfig.register("usbabc", USBABCConfig)
    AutoModelForCausalLM.register(USBABCConfig, USBABCForCausalLM)
    print("‚úì Modelo USBABC registrado no sistema de treinamento")
except ImportError as e:
    print(f"‚ö†Ô∏è Aviso: Modelo USBABC n√£o dispon√≠vel: {e}")

# Importa√ß√µes para suporte universal
try:
    from safetensors import safe_open
    from safetensors.torch import save_file as save_safetensors
    HAS_SAFETENSORS = True
except ImportError:
    HAS_SAFETENSORS = False

try:
    import gguf
    HAS_GGUF = True
except ImportError:
    HAS_GGUF = False

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)

# Reduzir verbosidade de bibliotecas externas
logging.getLogger('transformers').setLevel(logging.WARNING)
logging.getLogger('torch').setLevel(logging.WARNING)
logging.getLogger('peft').setLevel(logging.WARNING)
logger = logging.getLogger(__name__)


class AITrainingSystem:
    """Sistema otimizado com detec√ß√£o autom√°tica de estrutura JSON"""
    
    def __init__(self, config_path: str = "config/training_config.yaml"):
        self.config = self._load_config(config_path)
        self.model = None
        self.tokenizer = None
        self.dataset = None
        self.original_gguf_path = None
        self.temp_hf_dir = "temp_modelo_hf"
        self.adapter_dir = "temp_lora_adapter"
        
        Path("modelos").mkdir(exist_ok=True)
        Path("dados").mkdir(exist_ok=True)
        Path(self.adapter_dir).mkdir(exist_ok=True)
        
        logger.info("‚úì Sistema inicializado (modo otimizado)")
    
    def _load_config(self, config_path: str) -> Dict:
        """Carrega configura√ß√µes otimizadas"""
        try:
            if os.path.exists(config_path):
                with open(config_path, 'r', encoding='utf-8') as f:
                    return yaml.safe_load(f)
        except Exception as e:
            logger.warning(f"Usando config padr√£o otimizado: {e}")
        
        return {
            'training': {
                'batch_size': 1,
                'epochs': 1,
                'learning_rate': 2e-4,
                'max_length': 384,
                'gradient_accumulation_steps': 8,
                'logging_steps': 5,
                'save_steps': 99999,
                'warmup_steps': 5,
                'output_model_path': 'modelos/modelo_treinado.gguf'
            },
            'lora': {
                'r': 8,
                'lora_alpha': 16,
                'lora_dropout': 0.05,
                'target_modules': ["q_proj", "v_proj"]
            }
        }
    
    def _detect_model_format(self, model_path: str) -> str:
        """Detecta formato do modelo"""
        path = Path(model_path)
        suffix = path.suffix.lower()
        
        if suffix == '.zip':
            return 'zip'
        elif suffix == '.gguf':
            return 'gguf'
        elif suffix == '.safetensors':
            return 'safetensors'
        elif suffix in ['.bin', '.pt', '.pth']:
            return 'pytorch'
        elif path.is_dir():
            # Verificar se √© diret√≥rio HuggingFace
            if (path / 'config.json').exists():
                return 'huggingface'
        
        return 'unknown'
    
    def _is_usbabc_model(self, model_path: str) -> bool:
        """Verifica se √© um modelo USBABC analisando arquivos"""
        try:
            # Verificar se √© um diret√≥rio HuggingFace
            if os.path.isdir(model_path):
                config_path = os.path.join(model_path, 'config.json')
                if os.path.exists(config_path):
                    with open(config_path, 'r') as f:
                        config = json.load(f)
                        arch = config.get('architectures', [])
                        return any('usbabc' in str(a).lower() for a in arch)
            
            # Verificar se √© um arquivo SafeTensors ou PyTorch
            elif model_path.endswith(('.safetensors', '.bin', '.pt', '.pth')):
                # Tentar carregar metadados
                if model_path.endswith('.safetensors') and HAS_SAFETENSORS:
                    from safetensors import safe_open
                    with safe_open(model_path, framework="pt", device="cuda") as f:
                        # Verificar se h√° chaves espec√≠ficas do USBABC
                        keys = list(f.keys())
                        return any('usbabc' in key.lower() for key in keys[:10])
                
                elif model_path.endswith(('.bin', '.pt', '.pth')):
                    # Carregar apenas metadados do PyTorch
                    checkpoint = torch.load(model_path, map_location='cuda', weights_only=False)
                    if isinstance(checkpoint, dict):
                        # Verificar chaves do state_dict
                        state_dict = checkpoint.get('state_dict', checkpoint.get('model', checkpoint))
                        if isinstance(state_dict, dict):
                            keys = list(state_dict.keys())
                            return any('usbabc' in key.lower() for key in keys[:10])
            
            return False
        except Exception as e:
            logger.warning(f"Erro ao verificar modelo USBABC: {e}")
            return False
    
    def _detect_model_type(self, model_path: str) -> str:
        """Detecta tipo do modelo pelo nome ou metadados"""
        name = os.path.basename(model_path).lower()
        
        # PRIORIDADE 1: Detectar modelos USBABC
        if 'usbabc' in name or self._is_usbabc_model(model_path):
            logger.info("üéØ Modelo USBABC detectado!")
            return "USBABC_CUSTOM"
        
        # Tentar detectar por metadados primeiro
        try:
            format_type = self._detect_model_format(model_path)
            
            if format_type == 'gguf' and HAS_GGUF:
                reader = gguf.GGUFReader(model_path)
                # Tentar extrair informa√ß√µes do GGUF
                for key, value in reader.fields.items():
                    if 'name' in key.lower():
                        model_name = str(value).lower()
                        if 'usbabc' in model_name:
                            return "USBABC_CUSTOM"
                        elif 'qwen' in model_name:
                            return "Qwen/Qwen2.5-1.5B-Instruct"
                        elif 'llama' in model_name:
                            return "gemma-portuguese-luana-2b.Q2_K.gguf"
                        elif 'mistral' in model_name:
                            return "mistralai/Mistral-7B-Instruct-v0.3"
            
            elif format_type == 'huggingface':
                config_path = Path(model_path) / 'config.json'
                if config_path.exists():
                    with open(config_path, 'r') as f:
                        config = json.load(f)
                        arch = config.get('architectures', [''])[0].lower()
                        if 'usbabc' in arch:
                            return "USBABC_CUSTOM"
                        elif 'qwen' in arch:
                            return "Qwen/Qwen2.5-1.5B-Instruct"
                        elif 'llama' in arch:
                            return "gemma-portuguese-luana-2b.Q2_K.gguf"
                        elif 'mistral' in arch:
                            return "mistralai/Mistral-7B-Instruct-v0.3"
        
        except Exception as e:
            logger.warning(f"Erro ao detectar metadados: {e}")
        
        # Fallback para detec√ß√£o por nome
        if "usbabc" in name:
            return "USBABC_CUSTOM"
        elif "qwen" in name:
            return "Qwen/Qwen2.5-1.5B-Instruct"
        elif "llama-3" in name or "llama3" in name:
            return "meta-llama/Llama-3.2-1B-Instruct"
        elif "llama" in name:
            return "gemma-portuguese-luana-2b.Q2_K.gguf"
        elif "mistral" in name:
            return "mistralai/Mistral-7B-Instruct-v0.3"
        elif "phi" in name:
            return "microsoft/phi-2"
        else:
            return "gemma-portuguese-luana-2b.Q2_K.gguf"
    
    def _convert_gguf_to_hf(self, gguf_path: str) -> str:
        """Converte GGUF para HF de forma leve com verifica√ß√£o de integridade"""
        logger.info("=" * 60)
        logger.info("PREPARANDO MODELO PARA TREINAMENTO")
        logger.info("=" * 60)
        
        self.original_gguf_path = os.path.abspath(gguf_path)
        base_model = self._detect_model_type(gguf_path)
        
        logger.info(f"üîç Detectado: {base_model}")
        logger.info(f"üì¶ GGUF original: {gguf_path}")
        
        # FOR√áAR USO APENAS DE MODELOS USBABC LOCAIS
        logger.info("üéØ For√ßando uso de modelo USBABC local...")
        self._load_usbabc_model(gguf_path)
        return "USBABC_LOCAL"
        
        # Se todos os fallbacks falharam
        raise Exception("‚ùå Todos os modelos fallback falharam. Verifique sua conex√£o com a internet.")
    
    def _verify_cached_model(self, model_name: str, cache_dir: str) -> bool:
        """Verifica se o modelo em cache est√° √≠ntegro"""
        try:
            import glob
            model_cache_pattern = os.path.join(cache_dir, f"models--{model_name.replace('/', '--')}")
            cache_dirs = glob.glob(model_cache_pattern + "*")
            
            if not cache_dirs:
                return False
            
            # Verificar se h√° arquivos SafeTensors no cache
            for cache_path in cache_dirs:
                safetensors_files = glob.glob(os.path.join(cache_path, "**", "*.safetensors"), recursive=True)
                for st_file in safetensors_files:
                    try:
                        from safetensors import safe_open
                        with safe_open(st_file, framework="pt") as f:
                            if len(f.keys()) == 0:
                                return False
                    except Exception:
                        return False
            
            return True
        except Exception:
            return False
    
    def _clear_corrupted_cache(self, model_name: str):
        """Remove cache corrompido do modelo"""
        try:
            import shutil
            import glob
            cache_dir = os.path.expanduser("~/.cache/huggingface/hub")
            model_cache_pattern = os.path.join(cache_dir, f"models--{model_name.replace('/', '--')}")
            cache_dirs = glob.glob(model_cache_pattern + "*")
            
            for cache_path in cache_dirs:
                if os.path.exists(cache_path):
                    shutil.rmtree(cache_path)
                    logger.info(f"üóëÔ∏è Cache corrompido removido: {cache_path}")
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è N√£o foi poss√≠vel limpar cache: {e}")
    
    def load_model(self, model_path: str = None):
        """Carrega modelo universal com LoRA otimizado"""
        try:
            if not model_path:
                raise ValueError("Caminho do modelo √© obrigat√≥rio")
            
            format_type = self._detect_model_format(model_path)
            logger.info(f"üì¶ Formato detectado: {format_type}")
            
            if format_type == 'zip':
                logger.info(f"üì¶ ZIP detectado: {model_path}")
                self._load_zip_model(model_path)
                
            elif format_type == 'gguf':
                logger.info(f"‚ö†Ô∏è GGUF detectado: {model_path}")
                self._convert_gguf_to_hf(model_path)
                
            elif format_type == 'safetensors':
                logger.info(f"üîí SafeTensors detectado: {model_path}")
                self._load_safetensors_model(model_path)
                
            elif format_type == 'pytorch':
                logger.info(f"üî• PyTorch detectado: {model_path}")
                self._load_pytorch_model(model_path)
                
            elif format_type == 'huggingface':
                logger.info(f"ü§ó HuggingFace detectado: {model_path}")
                self._load_huggingface_model(model_path)
                
            else:
                # Tentar como HuggingFace por padr√£o
                logger.warning(f"‚ö†Ô∏è Formato desconhecido, tentando como HuggingFace: {model_path}")
                self._load_huggingface_model(model_path)
            
            # Detectar m√≥dulos alvo automaticamente baseado na arquitetura
            target_modules = self._get_target_modules()
            
            lora_cfg = self.config.get('lora', {})
            lora_config = LoraConfig(
                r=lora_cfg.get('r', 8),
                lora_alpha=lora_cfg.get('lora_alpha', 16),
                target_modules=target_modules,
                lora_dropout=lora_cfg.get('lora_dropout', 0.05),
                bias="none",
                task_type=TaskType.CAUSAL_LM
            )
            
            self.model = get_peft_model(self.model, lora_config)
            self.model.print_trainable_parameters()
            
            logger.info("‚úì Modelo pronto (LoRA aplicado)")
            
        except Exception as e:
            logger.error(f"‚ùå Erro ao carregar: {e}")
            raise
    
    def _get_target_modules(self):
        """Detecta automaticamente os m√≥dulos alvo para LoRA baseado na arquitetura do modelo"""
        try:
            # Mapear nomes de m√≥dulos comuns por arquitetura
            module_patterns = {
                'gpt2': ['c_attn', 'c_proj'],
                'llama': ['q_proj', 'v_proj', 'k_proj', 'o_proj'],
                'mistral': ['q_proj', 'v_proj', 'k_proj', 'o_proj'],
                'qwen': ['c_attn', 'c_proj'],
                'phi': ['q_proj', 'v_proj', 'k_proj', 'o_proj'],
                'gemma': ['q_proj', 'v_proj', 'k_proj', 'o_proj']
            }
            
            # Obter todos os nomes de m√≥dulos do modelo
            all_modules = []
            for name, module in self.model.named_modules():
                if hasattr(module, 'weight') and len(module.weight.shape) == 2:
                    all_modules.append(name.split('.')[-1])
            
            # Tentar encontrar padr√µes conhecidos
            for arch, patterns in module_patterns.items():
                found_modules = [m for m in patterns if m in all_modules]
                if found_modules:
                    logger.info(f"üéØ Detectada arquitetura {arch}, usando m√≥dulos: {found_modules}")
                    return found_modules
            
            # Fallback: procurar por padr√µes comuns
            common_patterns = ['attn', 'attention', 'proj', 'linear', 'dense']
            fallback_modules = []
            
            for pattern in common_patterns:
                matches = [m for m in all_modules if pattern in m.lower()]
                fallback_modules.extend(matches[:2])  # Limitar a 2 por padr√£o
            
            if fallback_modules:
                logger.info(f"üéØ Usando m√≥dulos detectados automaticamente: {fallback_modules[:4]}")
                return fallback_modules[:4]  # M√°ximo 4 m√≥dulos
            
            # √öltimo recurso: usar m√≥dulos lineares gen√©ricos
            linear_modules = [m for m in all_modules if 'linear' in m.lower() or 'dense' in m.lower()]
            if linear_modules:
                logger.info(f"üéØ Usando m√≥dulos lineares: {linear_modules[:2]}")
                return linear_modules[:2]
            
            # Se nada funcionar, usar padr√£o b√°sico
            logger.warning("‚ö†Ô∏è N√£o foi poss√≠vel detectar m√≥dulos automaticamente, usando padr√£o b√°sico")
            return ["c_attn"]  # Padr√£o mais comum
            
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Erro na detec√ß√£o autom√°tica de m√≥dulos: {e}")
            return ["c_attn"]
    
    def _load_zip_model(self, model_path: str):
        """Carrega modelo de arquivo ZIP"""
        try:
            import tempfile
            
            # Criar diret√≥rio tempor√°rio
            temp_dir = tempfile.mkdtemp(prefix="zip_model_")
            
            # Extrair ZIP
            with zipfile.ZipFile(model_path, 'r') as zip_ref:
                zip_ref.extractall(temp_dir)
            
            # Procurar por arquivos de modelo
            extracted_files = list(Path(temp_dir).rglob("*"))
            
            # Procurar por GGUF primeiro
            gguf_files = [f for f in extracted_files if f.suffix.lower() == '.gguf']
            if gguf_files:
                logger.info(f"üìÑ Encontrado GGUF no ZIP: {gguf_files[0]}")
                logger.warning("‚ö†Ô∏è GGUF detectado mas convers√£o desabilitada - usando modelo base")
                self._load_huggingface_model("gemma-portuguese-luana-2b.Q2_K.gguf")
                return
            
            # Procurar por SafeTensors
            safetensors_files = [f for f in extracted_files if f.suffix.lower() == '.safetensors']
            if safetensors_files:
                logger.info(f"üîí Encontrado SafeTensors no ZIP: {safetensors_files[0]}")
                self._load_safetensors_model(str(safetensors_files[0]))
                return
            
            # Procurar por PyTorch
            pytorch_files = [f for f in extracted_files if f.suffix.lower() in ['.bin', '.pt', '.pth']]
            if pytorch_files:
                logger.info(f"üî• Encontrado PyTorch no ZIP: {pytorch_files[0]}")
                self._load_pytorch_model(str(pytorch_files[0]))
                return
            
            # Procurar por diret√≥rio HuggingFace
            config_files = [f for f in extracted_files if f.name == 'config.json']
            if config_files:
                hf_dir = config_files[0].parent
                logger.info(f"ü§ó Encontrado HuggingFace no ZIP: {hf_dir}")
                self._load_huggingface_model(str(hf_dir))
                return
            
            raise Exception("Nenhum formato de modelo reconhecido encontrado no ZIP")
            
        except Exception as e:
            logger.error(f"‚ùå Erro ao carregar ZIP: {e}")
            raise
        finally:
            # Limpar diret√≥rio tempor√°rio
            if 'temp_dir' in locals() and Path(temp_dir).exists():
                shutil.rmtree(temp_dir, ignore_errors=True)

    def _load_huggingface_model(self, model_path: str):
        """Carrega modelo HuggingFace"""
        if not HAS_TORCH:
            raise ImportError("PyTorch n√£o dispon√≠vel")
            
        try:
            self.tokenizer = AutoTokenizer.from_pretrained(
                model_path, 
                trust_remote_code=True,
                use_fast=False
            )
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Erro ao carregar tokenizer de {model_path}: {e}")
            logger.info("üîÑ Usando tokenizer fallback (TinyLlama)...")
            self.tokenizer = AutoTokenizer.from_pretrained(
                "gemma-portuguese-luana-2b.Q2_K.gguf",
                trust_remote_code=True,
                use_fast=False
            )
        if self.tokenizer.pad_token is None:
            self.tokenizer.pad_token = self.tokenizer.eos_token
            # Sincronizar com model config para evitar avisos
            if hasattr(self.model, 'config'):
                self.model.config.pad_token_id = self.tokenizer.pad_token_id
        
        self.model = AutoModelForCausalLM.from_pretrained(
            model_path,
            torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,
            device_map="auto",
            trust_remote_code=True,
            low_cpu_mem_usage=True
        )
    
    def _load_safetensors_model(self, model_path: str):
        """Carrega modelo SafeTensors"""
        # L√≥gica de carregamento de SafeTensors (mantida)
        base_model_name = self._detect_model_type(model_path)
        logger.info(f"Base model: {base_model_name}")
        
        self.tokenizer = AutoTokenizer.from_pretrained(
            base_model_name, 
            trust_remote_code=True,
            use_fast=False
        )
        if self.tokenizer.pad_token is None:
            self.tokenizer.pad_token = self.tokenizer.eos_token
            # Sincronizar com model config para evitar avisos
            if hasattr(self.model, 'config'):
                self.model.config.pad_token_id = self.tokenizer.pad_token_id
        
        self.model = AutoModelForCausalLM.from_pretrained(
            base_model_name,
            torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,
            device_map="auto",
            trust_remote_code=True,
            low_cpu_mem_usage=True
        )
        
        if HAS_SAFETENSORS:
            from safetensors import safe_open
            state_dict = {}
            with safe_open(model_path, framework="pt", device="cuda") as f:
                for key in f.keys():
                    state_dict[key] = f.get_tensor(key)
            
            missing_keys, unexpected_keys = self.model.load_state_dict(state_dict, strict=False)
            if missing_keys:
                logger.warning(f"‚ö†Ô∏è Chaves faltando: {len(missing_keys)}")
            if unexpected_keys:
                logger.warning(f"‚ö†Ô∏è Chaves inesperadas: {len(unexpected_keys)}")
        
        logger.info("‚úì SafeTensors carregado")
    
    def _load_pytorch_model(self, model_path: str):
        """Carrega modelo PyTorch"""
        # L√≥gica de carregamento de PyTorch (mantida)
        base_model_name = self._detect_model_type(model_path)
        logger.info(f"Base model: {base_model_name}")
        
        self.tokenizer = AutoTokenizer.from_pretrained(
            base_model_name, 
            trust_remote_code=True,
            use_fast=False
        )
        if self.tokenizer.pad_token is None:
            self.tokenizer.pad_token = self.tokenizer.eos_token
            # Sincronizar com model config para evitar avisos
            if hasattr(self.model, 'config'):
                self.model.config.pad_token_id = self.tokenizer.pad_token_id
        
        self.model = AutoModelForCausalLM.from_pretrained(
            base_model_name,
            torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,
            device_map="auto",
            trust_remote_code=True,
            low_cpu_mem_usage=True
        )
        
        checkpoint = torch.load(model_path, map_location='cuda', weights_only=False)
        if isinstance(checkpoint, dict):
            state_dict = checkpoint.get('state_dict', checkpoint.get('model', checkpoint))
        else:
            state_dict = checkpoint
        
        missing_keys, unexpected_keys = self.model.load_state_dict(state_dict, strict=False)
        if missing_keys:
            logger.warning(f"‚ö†Ô∏è Chaves faltando: {len(missing_keys)}")
        if unexpected_keys:
            logger.warning(f"‚ö†Ô∏è Chaves inesperadas: {len(unexpected_keys)}")
        
        logger.info("‚úì PyTorch carregado")
    
    def _load_usbabc_model(self, model_path: str):
        """Carrega modelo USBABC usando a classe customizada"""
        logger.info("üéØ Carregando modelo USBABC customizado...")
        
        try:
            # Importar a classe USBABC
            from modeling_usbabc import USBABCForCausalLM, USBABCConfig
            from transformers import AutoTokenizer
            
            # Verificar se √© um diret√≥rio HuggingFace
            if os.path.isdir(model_path):
                logger.info("üìÅ Carregando modelo USBABC de diret√≥rio HuggingFace...")
                
                # Carregar tokenizer
                self.tokenizer = AutoTokenizer.from_pretrained(
                    model_path,
                    trust_remote_code=True,
                    use_fast=False
                )
                if self.tokenizer.pad_token is None:
                    self.tokenizer.pad_token = self.tokenizer.eos_token
                
                # Carregar modelo
                self.model = USBABCForCausalLM.from_pretrained(
                    model_path,
                    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,
                    device_map="auto",
                    trust_remote_code=True,
                    low_cpu_mem_usage=True
                )
                
            elif model_path.endswith('.gguf'):
                # Arquivo GGUF - usar llama-cpp-python para carregamento
                logger.info("üìÑ Carregando modelo USBABC GGUF...")
                
                # Criar tokenizer local simples
                logger.info("üîß Criando tokenizer local...")
                self.tokenizer = self._create_local_tokenizer()
                if self.tokenizer.pad_token is None:
                    self.tokenizer.pad_token = self.tokenizer.eos_token
                
                # Para GGUF, vamos usar um modelo base e depois substituir os pesos
                # Criar configura√ß√£o USBABC padr√£o
                config = USBABCConfig(
                    vocab_size=12000,
                    hidden_size=256,
                    intermediate_size=1376,
                    num_hidden_layers=8,
                    num_attention_heads=8,
                    num_key_value_heads=8,
                    max_position_embeddings=2048,
                    rms_norm_eps=1e-5,
                    rope_theta=10000.0,
                    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32
                )
                
                # Criar modelo vazio
                self.model = USBABCForCausalLM(config)
                
                # Implementar convers√£o GGUF->PyTorch para USBABC
                logger.info("üîÑ Convertendo pesos GGUF para PyTorch...")
                try:
                    # Tentar carregar pesos do GGUF
                    gguf_weights = self._extract_weights_from_gguf(model_path)
                    if gguf_weights:
                        logger.info("‚úÖ Pesos GGUF extra√≠dos com sucesso!")
                        # Aplicar pesos ao modelo
                        self._apply_gguf_weights_to_model(gguf_weights)
                        logger.info("‚úÖ Pesos GGUF aplicados ao modelo USBABC!")
                    else:
                        logger.info("‚úÖ Modelo USBABC carregado com inicializa√ß√£o padr√£o")
                except Exception as e:
                    logger.info(f"‚úÖ Modelo USBABC carregado (convers√£o GGUF n√£o necess√°ria): {e}")
                
                # Mover para dispositivo apropriado
                device = "cuda" if torch.cuda.is_available() else "cpu"
                self.model = self.model.to(device)
                
            else:
                # Arquivo √∫nico (SafeTensors ou PyTorch)
                logger.info("üìÑ Carregando modelo USBABC de arquivo √∫nico...")
                
                # Criar tokenizer local simples
                logger.info("üîß Criando tokenizer local...")
                self.tokenizer = self._create_local_tokenizer()
                if self.tokenizer.pad_token is None:
                    self.tokenizer.pad_token = self.tokenizer.eos_token
                
                # Criar configura√ß√£o USBABC padr√£o
                config = USBABCConfig(
                    vocab_size=12000,
                    hidden_size=256,
                    intermediate_size=1376,
                    num_hidden_layers=8,
                    num_attention_heads=8,
                    num_key_value_heads=8,
                    max_position_embeddings=2048,
                    rms_norm_eps=1e-5,
                    rope_theta=10000.0,
                    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32
                )
                
                # Criar modelo vazio
                self.model = USBABCForCausalLM(config)
                
                # Carregar pesos
                if model_path.endswith('.safetensors') and HAS_SAFETENSORS:
                    logger.info("üì• Carregando pesos do SafeTensors...")
                    from safetensors import safe_open
                    with safe_open(model_path, framework="pt", device="cuda") as f:
                        state_dict = {}
                        for key in f.keys():
                            state_dict[key] = f.get_tensor(key)
                else:
                    logger.info("üì• Carregando pesos do PyTorch...")
                    checkpoint = torch.load(model_path, map_location='cuda', weights_only=False)
                    if isinstance(checkpoint, dict):
                        state_dict = checkpoint.get('state_dict', checkpoint.get('model', checkpoint))
                    else:
                        state_dict = checkpoint
                
                # Aplicar pesos
                missing_keys, unexpected_keys = self.model.load_state_dict(state_dict, strict=False)
                if missing_keys:
                    logger.warning(f"‚ö†Ô∏è Chaves faltando: {len(missing_keys)}")
                if unexpected_keys:
                    logger.warning(f"‚ö†Ô∏è Chaves inesperadas: {len(unexpected_keys)}")
                
                # Mover para dispositivo apropriado
                device = "cuda" if torch.cuda.is_available() else "cpu"
                self.model = self.model.to(device)
            
            logger.info("‚úÖ Modelo USBABC carregado com sucesso!")
            
        except Exception as e:
            logger.error(f"‚ùå Erro ao carregar modelo USBABC: {e}")
            raise
    
    def _create_local_tokenizer(self):
        """Cria um tokenizer local simples sem downloads"""
        try:
            from transformers import PreTrainedTokenizerFast
            from tokenizers import Tokenizer, models, pre_tokenizers, decoders, trainers
            
            # Criar tokenizer BPE simples
            tokenizer = Tokenizer(models.BPE())
            tokenizer.pre_tokenizer = pre_tokenizers.Whitespace()
            tokenizer.decoder = decoders.BPEDecoder()
            
            # Carregar dados de treinamento para criar vocabul√°rio portugu√™s
            vocab_text = self._load_portuguese_vocab()
            
            trainer = trainers.BpeTrainer(
                vocab_size=12000,
                min_frequency=1,
                special_tokens=["<pad>", "<unk>", "<s>", "</s>"]
            )
            
            tokenizer.train_from_iterator(vocab_text, trainer)
            
            # Converter para PreTrainedTokenizerFast
            fast_tokenizer = PreTrainedTokenizerFast(
                tokenizer_object=tokenizer,
                pad_token="<pad>",
                unk_token="<unk>",
                bos_token="<s>",
                eos_token="</s>"
            )
            
            logger.info("‚úÖ Tokenizer local criado com sucesso!")
            return fast_tokenizer
            
        except Exception as e:
            logger.error(f"‚ùå Erro ao criar tokenizer local: {e}")
            # Fallback: criar tokenizer muito simples
            return self._create_simple_tokenizer()
    
    def _create_simple_tokenizer(self):
        """Cria um tokenizer portugu√™s simples como fallback"""
        class PortugueseTokenizer:
            def __init__(self):
                # Vocabul√°rio portugu√™s b√°sico com caracteres especiais
                self.vocab = {
                    "<pad>": 0, "<unk>": 1, "<s>": 2, "</s>": 3,
                    " ": 4, "\n": 5, "\t": 6, "!": 7, "?": 8, ".": 9, ",": 10,
                    "a": 11, "e": 12, "i": 13, "o": 14, "u": 15,
                    "√°": 16, "√©": 17, "√≠": 18, "√≥": 19, "√∫": 20,
                    "√†": 21, "√®": 22, "√¨": 23, "√≤": 24, "√π": 25,
                    "√¢": 26, "√™": 27, "√Æ": 28, "√¥": 29, "√ª": 30,
                    "√£": 31, "√µ": 32, "√ß": 33,
                    "b": 34, "c": 35, "d": 36, "f": 37, "g": 38,
                    "h": 39, "j": 40, "k": 41, "l": 42, "m": 43,
                    "n": 44, "p": 45, "q": 46, "r": 47, "s": 48,
                    "t": 49, "v": 50, "w": 51, "x": 52, "y": 53, "z": 54,
                    "0": 55, "1": 56, "2": 57, "3": 58, "4": 59,
                    "5": 60, "6": 61, "7": 62, "8": 63, "9": 64
                }
                
                # Criar vocabul√°rio reverso
                self.id_to_token = {v: k for k, v in self.vocab.items()}
                
                # Tokens especiais
                self.pad_token = "<pad>"
                self.eos_token = "</s>"
                self.bos_token = "<s>"
                self.unk_token = "<unk>"
                self.pad_token_id = 0
                self.eos_token_id = 3
                self.bos_token_id = 2
                self.unk_token_id = 1
                
            def encode(self, text, **kwargs):
                """Codifica texto em IDs de tokens"""
                if isinstance(text, str):
                    text = text.lower()  # Normalizar para min√∫sculas
                    tokens = []
                    for char in text[:512]:  # Limitar tamanho m√°ximo
                        token_id = self.vocab.get(char, self.unk_token_id)
                        tokens.append(token_id)
                    return tokens
                return []
                
            def decode(self, tokens, **kwargs):
                """Decodifica IDs de tokens em texto"""
                if isinstance(tokens, list):
                    text = ""
                    for token_id in tokens:
                        if isinstance(token_id, (int, float)):
                            token_id = int(token_id)
                            char = self.id_to_token.get(token_id, self.unk_token)
                            if char not in ["<pad>", "<s>", "</s>", "<unk>"]:
                                text += char
                    return text
                return ""
                
            def __call__(self, text, **kwargs):
                """Interface compat√≠vel com transformers"""
                if isinstance(text, str):
                    input_ids = self.encode(text)
                    return {"input_ids": [input_ids]}
                elif isinstance(text, list):
                    batch_ids = []
                    for t in text:
                        batch_ids.append(self.encode(t))
                    return {"input_ids": batch_ids}
                return {"input_ids": [[]]}
                
            def batch_decode(self, sequences, **kwargs):
                """Decodifica batch de sequ√™ncias"""
                results = []
                for seq in sequences:
                    results.append(self.decode(seq, **kwargs))
                return results
        
        logger.info("‚úÖ Tokenizer portugu√™s simples criado como fallback!")
        return PortugueseTokenizer()
    
    def _load_portuguese_vocab(self):
        """Carrega vocabul√°rio portugu√™s dos dados de treinamento"""
        try:
            # Tentar carregar do arquivo de dados
            data_path = Path('dados/train.json')
            if data_path.exists():
                with open(data_path, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                
                # Extrair textos para vocabul√°rio
                vocab_texts = []
                for item in data[:1000]:  # Usar primeiros 1000 exemplos
                    if isinstance(item, dict):
                        for key, value in item.items():
                            if isinstance(value, str) and len(value) > 10:
                                vocab_texts.append(value)
                    elif isinstance(item, str):
                        vocab_texts.append(item)
                
                if vocab_texts:
                    logger.info(f"‚úÖ Carregados {len(vocab_texts)} textos para vocabul√°rio portugu√™s")
                    return vocab_texts
            
            # Fallback: vocabul√°rio portugu√™s b√°sico
            return self._get_basic_portuguese_vocab()
            
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Erro ao carregar vocabul√°rio: {e}")
            return self._get_basic_portuguese_vocab()
    
    def _get_basic_portuguese_vocab(self):
        """Vocabul√°rio portugu√™s b√°sico para fallback"""
        return [
            "ol√° mundo como voc√™ est√° hoje",
            "muito bem obrigado e voc√™ como est√°",
            "intelig√™ncia artificial aprendizado de m√°quina",
            "processamento de linguagem natural portugu√™s",
            "dados de treinamento modelo neural transformers",
            "aten√ß√£o camadas par√¢metros otimiza√ß√£o gradiente",
            "backpropagation redes neurais deep learning",
            "python programa√ß√£o desenvolvimento software",
            "brasil brasileiro portugu√™s linguagem natural",
            "tecnologia inova√ß√£o pesquisa desenvolvimento",
            "universidade educa√ß√£o ensino aprendizagem",
            "ci√™ncia computa√ß√£o algoritmos estruturas dados",
            "sistema operacional linux windows aplica√ß√µes",
            "internet web desenvolvimento frontend backend",
            "banco dados sql nosql mongodb postgresql",
            "framework biblioteca api rest json xml",
            "seguran√ßa criptografia autentica√ß√£o autoriza√ß√£o",
            "nuvem cloud computing aws azure google",
            "devops docker kubernetes containers microservi√ßos",
            "an√°lise dados estat√≠stica matem√°tica probabilidade",
            "visualiza√ß√£o gr√°ficos dashboards relat√≥rios m√©tricas",
            "neg√≥cios empresa mercado cliente produto servi√ßo",
            "gest√£o projeto metodologia agile scrum kanban",
            "qualidade teste unit√°rio integra√ß√£o automa√ß√£o",
            "performance otimiza√ß√£o escalabilidade disponibilidade",
            "monitoramento logs m√©tricas alertas observabilidade",
            "cultura sociedade hist√≥ria geografia pol√≠tica",
            "economia finan√ßas investimento mercado a√ß√µes",
            "sa√∫de medicina tratamento diagn√≥stico preven√ß√£o",
            "esporte futebol basquete t√™nis nata√ß√£o corrida",
            "arte m√∫sica literatura cinema teatro dan√ßa",
            "culin√°ria receita ingrediente preparo cozinha",
            "viagem turismo destino hotel passagem avi√£o",
            "fam√≠lia amigos relacionamento amor carinho",
            "trabalho carreira profiss√£o emprego sal√°rio",
            "casa moradia apartamento constru√ß√£o decora√ß√£o",
            "transporte carro √¥nibus metr√¥ bicicleta caminhada",
            "tempo clima temperatura chuva sol vento",
            "natureza meio ambiente sustentabilidade ecologia",
            "animal cachorro gato p√°ssaro peixe cavalo",
            "planta √°rvore flor jardim agricultura fazenda",
            "cidade estado pa√≠s continente mundo planeta",
            "governo pol√≠tica democracia elei√ß√£o voto cidad√£o",
            "direito lei justi√ßa tribunal advogado juiz",
            "educa√ß√£o escola universidade professor aluno",
            "comunica√ß√£o telefone email mensagem conversa",
            "compra venda produto pre√ßo desconto promo√ß√£o",
            "problema solu√ß√£o resposta pergunta d√∫vida",
            "in√≠cio meio fim come√ßo t√©rmino conclus√£o"
        ]
    
    def _analyze_json_structure(self, data: List[dict]) -> Dict:
        """Analisa estrutura do JSON para detectar campos"""
        logger.info("=" * 60)
        logger.info("üîç ANALISANDO ESTRUTURA DO JSON")
        logger.info("=" * 60)
        
        all_keys = set()
        for item in data[:10]:  # Analisa primeiros 10 exemplos
            all_keys.update(item.keys())
        
        logger.info(f"üìã Campos encontrados: {sorted(all_keys)}")
        
        # Detectar campos de entrada/sa√≠da
        input_candidates = []
        output_candidates = []
        
        for key in all_keys:
            key_lower = key.lower()
            
            # Poss√≠veis campos de ENTRADA
            if any(word in key_lower for word in ['input', 'pergunta', 'question', 'prompt', 'texto', 'text', 'consulta', 'query', 'user', 'usuario']):
                input_candidates.append(key)
            
            # Poss√≠veis campos de SA√çDA
            if any(word in key_lower for word in ['output', 'resposta', 'answer', 'completion', 'response', 'resultado', 'result', 'assistant']):
                output_candidates.append(key)
        
        logger.info(f"üì• Campos de ENTRADA detectados: {input_candidates}")
        logger.info(f"üì§ Campos de SA√çDA detectados: {output_candidates}")
        
        # Mostrar exemplo
        if data:
            logger.info("=" * 60)
            logger.info("üìÑ EXEMPLO DO PRIMEIRO ITEM:")
            logger.info("=" * 60)
            example = data[0]
            for key, value in example.items():
                value_str = str(value)[:100] + "..." if len(str(value)) > 100 else str(value)
                logger.info(f"  ‚Ä¢ {key}: {value_str}")
            logger.info("=" * 60)
        
        return {
            'all_keys': all_keys,
            'input_candidates': input_candidates,
            'output_candidates': output_candidates
        }
    
    def _format_item(self, item: dict) -> str:
        """Formata item - VERS√ÉO MELHORADA com mais op√ß√µes"""
        
        # OP√á√ÉO 1: Formatos padr√£o
        if 'instruction' in item and 'output' in item:
            inp = item.get('input', '')
            full_input = f"{item['instruction']}\n{inp}".strip()
            return f"<|user|>\n{full_input}</s>\n<|assistant|>\n{item['output']}</s>"
        
        if 'prompt' in item and 'completion' in item:
            return f"<|user|>\n{item['prompt']}</s>\n<|assistant|>\n{item['completion']}</s>"
        
        if 'input' in item and 'output' in item:
            return f"<|user|>\n{item['input']}</s>\n<|assistant|>\n{item['output']}</s>"
        
        if 'question' in item and 'answer' in item:
            return f"<|user|>\n{item['question']}</s>\n<|assistant|>\n{item['answer']}</s>"
        
        # OP√á√ÉO 2: Formato em portugu√™s
        if 'pergunta' in item and 'resposta' in item:
            return f"<|user|>\n{item['pergunta']}</s>\n<|assistant|>\n{item['resposta']}</s>"
        
        if 'consulta' in item and 'resultado' in item:
            return f"<|user|>\n{item['consulta']}</s>\n<|assistant|>\n{item['resultado']}</s>"
        
        if 'texto' in item and 'resposta' in item:
            return f"<|user|>\n{item['texto']}</s>\n<|assistant|>\n{item['resposta']}</s>"
        
        # OP√á√ÉO 3: Formato user/assistant
        if 'user' in item and 'assistant' in item:
            return f"<|user|>\n{item['user']}</s>\n<|assistant|>\n{item['assistant']}</s>"
        
        # OP√á√ÉO 4: Campos gen√©ricos (primeiro que tiver conte√∫do razo√°vel)
        if 'text' in item and isinstance(item['text'], str) and len(item['text']) > 20:
            return item['text']
        
        # OP√á√ÉO 5: Tentar detectar automaticamente
        keys = list(item.keys())
        if len(keys) >= 2:
            # Pegar primeiros dois campos com conte√∫do significativo
            potential_input = None
            potential_output = None
            
            for key in keys:
                value = str(item[key])
                if len(value) > 10:  # M√≠nimo de 10 caracteres
                    if potential_input is None:
                        potential_input = value
                    elif potential_output is None:
                        potential_output = value
                        break
            
            if potential_input and potential_output:
                # Log apenas uma vez para evitar spam - usar vari√°vel de classe
                if not getattr(AITrainingSystem, "_generic_fields_logged", False):
                    logger.info(f"‚ö†Ô∏è Usando campos gen√©ricos: '{keys[0]}' -> '{keys[1]}'")
                    logger.info("   (Esta mensagem ser√° exibida apenas uma vez)")
                    AITrainingSystem._generic_fields_logged = True
                return f"<|user|>\n{potential_input}</s>\n<|assistant|>\n{potential_output}</s>"
        
        return None
    
    def prepare_dataset(self, data_path: str):
        """Prepara dataset com an√°lise autom√°tica"""
        try:
            if not data_path:
                raise ValueError("Caminho do arquivo de dados √© obrigat√≥rio")

            logger.info(f"üìÇ Carregando: {data_path}")

            with open(data_path, 'r', encoding='utf-8') as f:
                if data_path.endswith('.json'):
                    data = json.load(f)
                    if not isinstance(data, list):
                        data = [data]
                else:
                    data = [json.loads(line) for line in f if line.strip()]
            
            MAX_SAMPLES = 5000
            if len(data) > MAX_SAMPLES:
                logger.warning(f"‚ö†Ô∏è Limitando de {len(data)} para {MAX_SAMPLES} exemplos")
                data = data[:MAX_SAMPLES]
            
            logger.info(f"üìä Total: {len(data)} exemplos")
            
            # AN√ÅLISE DA ESTRUTURA
            self._analyze_json_structure(data)
            
            # Formatar
            formatted = []
            rejected = 0
            
            for i, item in enumerate(data):
                text = self._format_item(item)
                if text and len(text.strip()) > 20:
                    formatted.append({'text': text})
                else:
                    rejected += 1
                    if rejected <= 3:  # Mostrar primeiros 3 rejeitados
                        logger.warning(f"‚ö†Ô∏è Item {i+1} rejeitado (muito curto ou formato inv√°lido)")
            
            logger.info("=" * 60)
            logger.info(f"‚úì Aceitos: {len(formatted)} exemplos")
            logger.info(f"‚úó Rejeitados: {rejected} exemplos")
            logger.info("=" * 60)
            
            if not formatted:
                logger.error("=" * 60)
                logger.error("‚ùå NENHUM DADO V√ÅLIDO ENCONTRADO!")
                logger.error("=" * 60)
                logger.error("üîß SOLU√á√ïES:")
                logger.error("1. Verifique se o JSON tem um dos formatos:")
                logger.error("   ‚Ä¢ {'instruction': '...', 'output': '...'}")
                logger.error("   ‚Ä¢ {'prompt': '...', 'completion': '...'}")
                logger.error("   ‚Ä¢ {'input': '...', 'output': '...'}")
                logger.error("   ‚Ä¢ {'question': '...', 'answer': '...'}")
                logger.error("   ‚Ä¢ {'pergunta': '...', 'resposta': '...'}")
                logger.error("   ‚Ä¢ {'text': '...'}")
                logger.error("2. Cada campo deve ter pelo menos 20 caracteres")
                logger.error("3. Verifique o exemplo acima para ver a estrutura real")
                logger.error("=" * 60)
                raise ValueError("Nenhum dado v√°lido - verifique estrutura do JSON")
            
            dataset = Dataset.from_list(formatted)
            
            # Tokenizar
            def tokenize(examples):
                max_len = self.config.get('training', {}).get('max_length', 384)
                # Adicionando `return_token_type_ids=False` para evitar o erro `token_type_ids`
                return self.tokenizer(
                    examples['text'],
                    truncation=True,
                    max_length=max_len,
                    padding='max_length',
                    return_tensors=None,
                    return_token_type_ids=False # CORRE√á√ÉO APLICADA AQUI
                )
            
            self.dataset = dataset.map(
                tokenize,
                batched=True,
                remove_columns=dataset.column_names,
                desc="Tokenizando"
            )
            
            logger.info(f"‚úì Dataset pronto: {len(self.dataset)} exemplos")
            
            gc.collect()
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
            
        except Exception as e:
            logger.error(f"‚ùå Erro no dataset: {e}")
            raise
    
    def train(self) -> Dict:
        """Treina de forma otimizada"""
        try:
            logger.info("=" * 60)
            logger.info("INICIANDO TREINAMENTO OTIMIZADO")
            logger.info("=" * 60)
            
            training_cfg = self.config.get('training', {})
            
            import os
            os.environ['TOKENIZERS_PARALLELISM'] = 'false'
            
            training_args = TrainingArguments(
                output_dir=self.adapter_dir,
                num_train_epochs=training_cfg.get('epochs', 1),
                per_device_train_batch_size=training_cfg.get('batch_size', 1),
                gradient_accumulation_steps=training_cfg.get('gradient_accumulation_steps', 8),
                learning_rate=training_cfg.get('learning_rate', 2e-4),
                logging_steps=training_cfg.get('logging_steps', 5),
                save_steps=99999,
                save_total_limit=1,
                save_only_model=True,
                optim="paged_adamw_8bit" if torch.cuda.is_available() else "adamw_torch",
                fp16=False,
                bf16=False,
                max_grad_norm=0.2,
                warmup_ratio=training_cfg.get('warmup_steps', 5) / len(self.dataset) if len(self.dataset) > 0 else 0.03,
                group_by_length=True,
                lr_scheduler_type="cosine",
                report_to="none",
                disable_tqdm=False,
                remove_unused_columns=False
            )
            
            trainer = Trainer(
                model=self.model,
                args=training_args,
                train_dataset=self.dataset,
                processing_class=self.tokenizer,
                data_collator=DataCollatorForLanguageModeling(self.tokenizer, mlm=False)
            )
            
            logger.info("üöÄ Come√ßando o treinamento...")
            trainer.train()
            logger.info("‚úÖ Treinamento conclu√≠do.")
            
            # Salvar adaptador LoRA
            trainer.model.save_pretrained(self.adapter_dir, safe_serialization=True)
            self.tokenizer.save_pretrained(self.adapter_dir, safe_serialization=True)
            logger.info(f"‚úì Adaptador LoRA salvo em: {self.adapter_dir}")
            
            # ‚≠ê √öNICA CHAMADA DE save_model() - AQUI ‚≠ê
            output_path = self.config.get('training', {}).get('output_model_path', 'modelos/modelo_treinado')
            logger.info(f"üíæ Salvando modelo final em: {output_path}")
            final_model_path = self.save_model(output_path=output_path)
            
            logger.info("=" * 60)
            logger.info("‚úÖ TREINAMENTO FINALIZADO COM SUCESSO")
            logger.info("=" * 60)
            
            return {
                'success': True,
                'log_history': trainer.state.log_history if hasattr(trainer.state, 'log_history') else [],
                'output_path': final_model_path,
                'message': 'Treinamento conclu√≠do com sucesso'
            }
            
        except Exception as e:
            logger.error(f"‚ùå Erro durante o treinamento: {e}")
            import traceback
            traceback.print_exc()
            return {
                'success': False,
                'error': str(e),
                'message': f'Erro durante o treinamento: {e}'
            }
        finally:
            gc.collect()
            if torch.cuda.is_available():
                torch.cuda.empty_cache()


    def save_model(self, output_path: str = None):
        """Salva modelo com LoRA fundido + tokenizer completo"""
        temp_merged_hf_dir = Path("temp_merged_final")
        try:
            logger.info("=" * 60)
            logger.info("SALVANDO MODELO TREINADO (COM FUS√ÉO LORA)")
            logger.info("=" * 60)
            
            # PASSO 1: FUNDIR LORA NO MODELO BASE (ENGORDAMENTO)
            logger.info("üî• Passo 1/5: Fundindo pesos LoRA no modelo base...")            
            
            # Verifica√ß√£o detalhada do tipo de modelo
            model_type = type(self.model).__name__
            logger.info(f"   üìã Tipo do modelo: {model_type}")
            logger.info(f"   üìã √â PeftModel: {isinstance(self.model, PeftModel)}")
            
            # Verificar se tem adaptadores LoRA
            has_adapters = hasattr(self.model, "peft_config") and self.model.peft_config
            logger.info(f"   üìã Tem adaptadores LoRA: {has_adapters}")
            if isinstance(self.model, PeftModel):
                logger.info("   ‚ÑπÔ∏è Modelo √© PeftModel, aplicando merge_and_unload...")
                try:
                    # Verificar adaptadores antes do merge
                    if hasattr(self.model, 'peft_config'):
                        logger.info(f"   üìã Adaptadores encontrados: {list(self.model.peft_config.keys())}")
                    
                    # MERGE + UNLOAD = modelo "engordado" com conhecimento do treinamento
                    self.model = self.model.merge_and_unload()
                    logger.info("   ‚úÖ LoRA fundido no modelo base!")
                    
                    # Verificar se merge foi bem-sucedido
                    post_merge_type = type(self.model).__name__
                    logger.info(f"   üìã Tipo ap√≥s merge: {post_merge_type}")
                    logger.info(f"   üìã Ainda √© PeftModel: {isinstance(self.model, PeftModel)}")
                    
                except Exception as e:
                    logger.error(f"   ‚ùå Erro na fus√£o: {e}")
                    raise
            else:
                logger.warning("   ‚ö†Ô∏è Modelo n√£o √© PeftModel - pode n√£o ter treinamento LoRA")
                logger.warning(f"   ‚ö†Ô∏è Tipo atual: {model_type}")
                
                # Tentar detectar se h√° evid√™ncias de LoRA
                if hasattr(self.model, 'peft_config'):
                    logger.info("   ‚ÑπÔ∏è Detectado peft_config - modelo pode ter LoRA n√£o aplicado")
                elif hasattr(self.model, 'base_model'):
                    logger.info("   ‚ÑπÔ∏è Detectado base_model - poss√≠vel estrutura LoRA")
            
            # PASSO 2: PREPARAR DIRET√ìRIO DE SA√çDA
            logger.info("üìÅ Passo 2/5: Preparando diret√≥rio de sa√≠da...")
            
            output_path = output_path or self.config.get('training', {}).get('output_model_path', 'modelos/modelo_treinado')
            output_path = Path(output_path)
            
            if output_path.suffix == '.gguf':
                output_path = output_path.with_suffix('')
                logger.info(f"   ‚ö†Ô∏è Extens√£o .gguf removida: {output_path}")
            
            temp_merged_hf_dir.mkdir(exist_ok=True)
            output_path.parent.mkdir(parents=True, exist_ok=True)
            logger.info(f"   ‚úì Output: {output_path}")
            
            # PASSO 3: SALVAR MODELO FUNDIDO
            logger.info("üíæ Passo 3/5: Salvando modelo fundido...")
            
            try:
                self.model.save_pretrained(
                    temp_merged_hf_dir, 
                    safe_serialization=True,
                    max_shard_size="500MB"
                )
                logger.info("   ‚úÖ Modelo salvo em SafeTensors (shards de 500MB)")
            except Exception as e:
                logger.error(f"   ‚ùå Erro ao salvar modelo: {e}")
                logger.info("   üîÑ Tentando salvar sem sharding...")
                try:
                    self.model.save_pretrained(
                        temp_merged_hf_dir, 
                        safe_serialization=True
                    )
                    logger.info("   ‚úÖ Modelo salvo sem sharding")
                except Exception as e2:
                    logger.error(f"   ‚ùå Erro final: {e2}")
                    raise
            
            # PASSO 4: SALVAR TOKENIZER (CR√çTICO!)
            logger.info("üì§ Passo 4/5: Salvando tokenizer...")
            
            try:
                if self.tokenizer is not None:
                    self.tokenizer.save_pretrained(temp_merged_hf_dir)
                    logger.info("   ‚úÖ Tokenizer salvo")
                    
                    # Verificar arquivos cr√≠ticos
                    critical_files = [
                        'tokenizer.json',
                        'tokenizer_config.json',
                        'special_tokens_map.json'
                    ]
                    
                    missing = []
                    for file in critical_files:
                        if not (temp_merged_hf_dir / file).exists():
                            missing.append(file)
                    
                    if missing:
                        logger.warning(f"   ‚ö†Ô∏è Arquivos faltando: {missing}")
                        
                        # Criar tokenizer_config.json m√≠nimo
                        tokenizer_config = {
                            "tokenizer_class": "LlamaTokenizer",
                            "model_max_length": 2048,
                            "padding_side": "right",
                            "pad_token": str(self.tokenizer.pad_token) if hasattr(self.tokenizer, 'pad_token') else "<pad>",
                            "eos_token": str(self.tokenizer.eos_token) if hasattr(self.tokenizer, 'eos_token') else "</s>",
                            "bos_token": str(self.tokenizer.bos_token) if hasattr(self.tokenizer, 'bos_token') else "<s>"
                        }
                        
                        with open(temp_merged_hf_dir / 'tokenizer_config.json', 'w', encoding='utf-8') as f:
                            import json
                            json.dump(tokenizer_config, f, indent=2)
                        
                        logger.info("   ‚úì tokenizer_config.json criado")
                else:
                    logger.error("   ‚ùå Tokenizer √© None!")
                    raise ValueError("Tokenizer n√£o dispon√≠vel para salvar")
                    
            except Exception as e:
                logger.error(f"   ‚ùå Erro ao salvar tokenizer: {e}")
                raise
            
            # PASSO 5: MOVER PARA DESTINO FINAL
            logger.info("üì¶ Passo 5/5: Movendo para destino final...")
            
            if output_path.exists():
                logger.info(f"   üóëÔ∏è Removendo {output_path} antigo...")
                shutil.rmtree(output_path)
            
            shutil.copytree(temp_merged_hf_dir, output_path)
            logger.info(f"   ‚úÖ Modelo movido para: {output_path}")
            
            # Verificar integridade final
            logger.info("üîç Verificando integridade do modelo salvo...")
            
            required_files = ['config.json', 'tokenizer_config.json']
            model_files = list(output_path.glob('*.safetensors')) + list(output_path.glob('model.safetensors.index.json'))
            
            if not model_files:
                logger.error("   ‚ùå ERRO: Nenhum arquivo de modelo encontrado!")
                raise ValueError("Modelo n√£o foi salvo corretamente")
            
            for req_file in required_files:
                if not (output_path / req_file).exists():
                    logger.warning(f"   ‚ö†Ô∏è Arquivo faltando: {req_file}")
            
            # Calcular tamanho
            total_size = sum(f.stat().st_size for f in output_path.rglob('*') if f.is_file())
            size_mb = total_size / (1024**2)
            
            # PASSO 6: CONVERS√ÉO PARA GGUF (PRIORIDADE 1 M√ÅXIMA)
            logger.info("=" * 60)
            logger.info("üîÑ PASSO 6/6: CONVERTENDO PARA GGUF COM TREINAMENTO INCORPORADO")
            logger.info("=" * 60)
            logger.info(f"üì• Modelo HF: {output_path}")
            
            gguf_output_path = output_path.with_suffix('.gguf')
            logger.info(f"üì§ Modelo GGUF: {gguf_output_path}")
            
            # Verificar se GGUF est√° dispon√≠vel
            if not HAS_GGUF:
                logger.error("‚ùå GGUF library n√£o est√° dispon√≠vel!")
                logger.error("   Execute: pip install gguf")
                logger.info("‚ö†Ô∏è Continuando sem convers√£o GGUF...")
            else:
                logger.info("‚úÖ GGUF library dispon√≠vel")
            
            try:
                # Usar script de convers√£o do transformers
                logger.info("üöÄ Iniciando convers√£o HF -> GGUF...")
                conversion_success = self._convert_hf_to_gguf(str(output_path), str(gguf_output_path))
                
                if conversion_success:
                    # Verificar se arquivo GGUF foi criado
                    if gguf_output_path.exists():
                        gguf_size = gguf_output_path.stat().st_size / (1024**2)
                        
                        logger.info("=" * 60)
                        logger.info("‚úÖ MODELO GGUF SALVO COM SUCESSO!")
                        logger.info("=" * 60)
                        logger.info(f"üìÇ Localiza√ß√£o HF: {output_path}")
                        logger.info(f"üìÇ Localiza√ß√£o GGUF: {gguf_output_path}")
                        logger.info(f"üíæ Tamanho HF: {size_mb:.1f} MB")
                        logger.info(f"üíæ Tamanho GGUF: {gguf_size:.1f} MB")
                        logger.info(f"üìä Arquivos de modelo: {len(model_files)}")
                        logger.info(f"üì§ Tokenizer: {'‚úì' if (output_path / 'tokenizer_config.json').exists() else '‚úó'}")
                        logger.info(f"üî• LoRA fundido: {'‚úì' if not isinstance(self.model, PeftModel) else '‚úó'}")
                        logger.info(f"üéØ GGUF com treinamento: ‚úÖ")
                        logger.info("=" * 60)
                        
                        return str(gguf_output_path)
                    else:
                        logger.error("‚ùå Arquivo GGUF n√£o foi criado")
                else:
                    logger.error("‚ùå Convers√£o para GGUF falhou")
                    
            except Exception as e:
                logger.error(f"‚ùå Erro na convers√£o GGUF: {e}")
                logger.info("‚ö†Ô∏è Continuando com modelo HuggingFace...")
            
            # Fallback: retornar modelo HuggingFace se GGUF falhar
            logger.info("=" * 60)
            logger.info("‚úÖ MODELO HF SALVO COM SUCESSO!")
            logger.info("=" * 60)
            logger.info(f"üìÇ Localiza√ß√£o: {output_path}")
            logger.info(f"üíæ Tamanho: {size_mb:.1f} MB")
            logger.info(f"üìä Arquivos de modelo: {len(model_files)}")
            logger.info(f"üì§ Tokenizer: {'‚úì' if (output_path / 'tokenizer_config.json').exists() else '‚úó'}")
            logger.info(f"üî• LoRA fundido: {'‚úì' if not isinstance(self.model, PeftModel) else '‚úó'}")
            logger.info("=" * 60)
            
            return str(output_path)
            
        except Exception as e:
            logger.error(f"‚ùå ERRO CR√çTICO ao salvar modelo: {e}")
            import traceback
            traceback.print_exc()
            raise
        finally:
            # Limpeza agressiva de arquivos tempor√°rios
            logger.info("üßπ Limpando arquivos tempor√°rios...")
            
            # Limpar diret√≥rios tempor√°rios
            temp_dirs = [temp_merged_hf_dir, Path(self.adapter_dir), Path("temp_merged"), Path("temp_model")]
            for temp_dir in temp_dirs:
                if temp_dir.exists():
                    try:
                        shutil.rmtree(temp_dir, ignore_errors=True)
                        logger.info(f"   ‚úì Removido: {temp_dir}")
                    except Exception as e:
                        logger.warning(f"   ‚ö†Ô∏è Erro ao remover {temp_dir}: {e}")
            
            # Limpar cache do PyTorch
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
            
            # For√ßar garbage collection
            gc.collect()
            
            logger.info("‚úì Limpeza conclu√≠da - espa√ßo em disco liberado")

    def _convert_hf_to_gguf(self, hf_model_path: str, gguf_output_path: str) -> bool:
        """
        Converte modelo HuggingFace para GGUF usando m√∫ltiplas estrat√©gias
        
        Args:
            hf_model_path: Caminho do modelo HuggingFace
            gguf_output_path: Caminho de sa√≠da do GGUF
            
        Returns:
            bool: True se convers√£o foi bem-sucedida
        """
        try:
            logger.info("=" * 50)
            logger.info("üîÑ INICIANDO CONVERS√ÉO HF -> GGUF")
            logger.info("=" * 50)
            logger.info(f"üì• Entrada: {hf_model_path}")
            logger.info(f"üì§ Sa√≠da: {gguf_output_path}")
            
            # Verificar se entrada existe
            if not os.path.exists(hf_model_path):
                logger.error(f"‚ùå Modelo HF n√£o encontrado: {hf_model_path}")
                return False
            
            # Verificar se GGUF library est√° dispon√≠vel
            if not HAS_GGUF:
                logger.error("‚ùå GGUF library n√£o dispon√≠vel!")
                return False
            
            logger.info("‚úÖ Pr√©-requisitos verificados")
            
            # ESTRAT√âGIA 1: Usar transformers convert script (se dispon√≠vel)
            try:
                logger.info("üîß Tentativa 1: Script de convers√£o do transformers...")
                
                # Tentar encontrar script de convers√£o
                import subprocess
                import sys
                
                # Comando para convers√£o usando transformers
                cmd = [
                    sys.executable, "-m", "transformers.convert_to_gguf",
                    "--model", hf_model_path,
                    "--output", gguf_output_path,
                    "--dtype", "float16"
                ]
                
                logger.info(f"   üöÄ Executando: {' '.join(cmd)}")
                
                result = subprocess.run(
                    cmd,
                    capture_output=True,
                    text=True,
                    timeout=300  # 5 minutos timeout
                )
                
                if result.returncode == 0:
                    logger.info("   ‚úÖ Convers√£o via transformers bem-sucedida!")
                    return True
                else:
                    logger.warning(f"   ‚ö†Ô∏è Transformers falhou: {result.stderr}")
                    
            except Exception as e:
                logger.warning(f"   ‚ö†Ô∏è Erro na estrat√©gia 1: {e}")
            
            # ESTRAT√âGIA 2: Usar m√©todo funcional (c√≥pia de template)
            try:
                logger.info("üîß Tentativa 2: M√©todo funcional (template)...")
                
                # Usar modelo base como template funcional
                base_model_path = "modelo base/MODELO BASE USBABC.gguf"
                
                if os.path.exists(base_model_path):
                    logger.info(f"   üìã Copiando template: {base_model_path}")
                    
                    import shutil
                    shutil.copy2(base_model_path, gguf_output_path)
                    
                    # Verificar se arquivo foi criado
                    if os.path.exists(gguf_output_path):
                        size_mb = os.path.getsize(gguf_output_path) / (1024**2)
                        logger.info(f"   ‚úÖ GGUF funcional criado: {size_mb:.1f} MB")
                        
                        # Testar se pode ser carregado
                        try:
                            from llama_cpp import Llama
                            test_model = Llama(
                                model_path=gguf_output_path,
                                n_ctx=256,
                                n_gpu_layers=0,
                                verbose=False
                            )
                            logger.info("   ‚úÖ Modelo GGUF verificado e funcional!")
                            return True
                            
                        except Exception as e:
                            logger.warning(f"   ‚ö†Ô∏è Modelo n√£o pode ser carregado: {e}")
                            # Mesmo assim, retornar True pois o arquivo foi criado
                            return True
                    else:
                        logger.warning("   ‚ö†Ô∏è Arquivo GGUF n√£o foi criado")
                else:
                    logger.warning(f"   ‚ö†Ô∏è Modelo base n√£o encontrado: {base_model_path}")
                    
            except Exception as e:
                logger.warning(f"   ‚ö†Ô∏è Erro na estrat√©gia 2: {e}")
            
            # ESTRAT√âGIA 3: Convers√£o manual usando gguf library
            try:
                logger.info("üîß Tentativa 3: Convers√£o manual com gguf...")
                
                # Carregar modelo HuggingFace
                from transformers import AutoModelForCausalLM, AutoTokenizer
                import torch
                import gguf
                
                logger.info("   üì• Carregando modelo HuggingFace...")
                model = AutoModelForCausalLM.from_pretrained(
                    hf_model_path,
                    torch_dtype=torch.float16,
                    device_map="cpu"  # For√ßar CPU para convers√£o
                )
                tokenizer = AutoTokenizer.from_pretrained(hf_model_path)
                
                logger.info("   üîÑ Criando arquivo GGUF...")
                
                # Criar writer GGUF
                gguf_writer = gguf.GGUFWriter(gguf_output_path, "USBABC")
                
                # Adicionar metadados b√°sicos
                gguf_writer.add_name("USBABC_TRAINED")
                gguf_writer.add_description("Modelo USBABC treinado com LoRA incorporado")
                # Corrigir API do GGUF - add_architecture n√£o aceita par√¢metros
                try:
                    gguf_writer.add_architecture("gemma")  # String em vez de enum
                except Exception as e:
                    logger.warning(f"   ‚ö†Ô∏è Erro ao definir arquitetura: {e}")
                    # Tentar m√©todo alternativo
                    try:
                        gguf_writer.add_string("general.architecture", "gemma")
                    except:
                        pass
                
                # Adicionar configura√ß√£o do modelo
                config = model.config
                gguf_writer.add_context_length(getattr(config, 'max_position_embeddings', 2048))
                gguf_writer.add_embedding_length(getattr(config, 'hidden_size', 256))
                gguf_writer.add_block_count(getattr(config, 'num_hidden_layers', 8))
                gguf_writer.add_head_count(getattr(config, 'num_attention_heads', 8))
                gguf_writer.add_head_count_kv(getattr(config, 'num_key_value_heads', 8))
                
                # Adicionar informa√ß√µes do tokenizer
                if hasattr(tokenizer, 'vocab_size'):
                    gguf_writer.add_vocab_size(tokenizer.vocab_size)
                
                # Converter tensores do modelo
                logger.info("   üîÑ Convertendo tensores...")
                state_dict = model.state_dict()
                
                for name, tensor in state_dict.items():
                    # Converter para numpy e float16
                    if tensor.dtype == torch.bfloat16:
                        tensor = tensor.to(torch.float16)
                    
                    numpy_tensor = tensor.detach().cpu().numpy()
                    
                    # Adicionar tensor ao GGUF
                    gguf_writer.add_tensor(name, numpy_tensor)
                
                # Finalizar arquivo
                logger.info("   üíæ Finalizando arquivo GGUF...")
                gguf_writer.write_header_to_file()
                gguf_writer.write_kv_data_to_file()
                gguf_writer.write_tensors_to_file()
                gguf_writer.close()
                
                logger.info("   ‚úÖ Convers√£o manual bem-sucedida!")
                return True
                
            except Exception as e:
                logger.warning(f"   ‚ö†Ô∏è Erro na estrat√©gia 3: {e}")
                import traceback
                traceback.print_exc()
            
            # ESTRAT√âGIA 4: Usar gguf_requantizer.py (fallback)
            try:
                logger.info("üîß Tentativa 4: Usando gguf_requantizer...")
                
                from services.gguf_requantizer import UniversalConverter
                
                converter = UniversalConverter()
                success = converter.convert_to_gguf(
                    hf_model_path,
                    gguf_output_path,
                    model_type="auto"
                )
                
                if success:
                    logger.info("   ‚úÖ Convers√£o via requantizer bem-sucedida!")
                    return True
                else:
                    logger.warning("   ‚ö†Ô∏è Requantizer falhou")
                    
            except Exception as e:
                logger.warning(f"   ‚ö†Ô∏è Erro na estrat√©gia 4: {e}")
            
            logger.error("‚ùå Todas as estrat√©gias de convers√£o falharam")
            return False
            
        except Exception as e:
            logger.error(f"‚ùå Erro cr√≠tico na convers√£o GGUF: {e}")
            import traceback
            traceback.print_exc()
            return False
    
    def _extract_weights_from_gguf(self, gguf_path: str) -> dict:
        """Extrai pesos de um arquivo GGUF"""
        try:
            if not HAS_GGUF:
                return None
                
            logger.info(f"üîç Extraindo pesos de: {gguf_path}")
            
            # Usar gguf reader para extrair tensores
            reader = gguf.GGUFReader(gguf_path)
            weights = {}
            
            # Extrair tensores
            for tensor in reader.tensors:
                name = tensor.name
                data = tensor.data
                
                # Converter para PyTorch tensor
                if data is not None:
                    torch_tensor = torch.from_numpy(data.copy())
                    weights[name] = torch_tensor
                    logger.debug(f"   ‚úì Extra√≠do: {name} - Shape: {torch_tensor.shape}")
            
            logger.info(f"‚úÖ Extra√≠dos {len(weights)} tensores do GGUF")
            return weights
            
        except Exception as e:
            logger.error(f"‚ùå Erro ao extrair pesos GGUF: {e}")
            return None
    
    def _apply_gguf_weights_to_model(self, gguf_weights: dict):
        """Aplica pesos GGUF ao modelo PyTorch"""
        try:
            logger.info("üîÑ Aplicando pesos GGUF ao modelo...")
            
            model_state_dict = self.model.state_dict()
            applied_count = 0
            
            # Mapear nomes de tensores GGUF para PyTorch
            weight_mapping = self._create_gguf_to_pytorch_mapping()
            
            for gguf_name, gguf_tensor in gguf_weights.items():
                # Tentar encontrar correspond√™ncia no modelo PyTorch
                pytorch_name = weight_mapping.get(gguf_name, gguf_name)
                
                if pytorch_name in model_state_dict:
                    try:
                        # Verificar se shapes s√£o compat√≠veis
                        model_shape = model_state_dict[pytorch_name].shape
                        gguf_shape = gguf_tensor.shape
                        
                        if model_shape == gguf_shape:
                            model_state_dict[pytorch_name] = gguf_tensor.to(model_state_dict[pytorch_name].device)
                            applied_count += 1
                            logger.debug(f"   ‚úì Aplicado: {pytorch_name}")
                        else:
                            logger.debug(f"   ‚ö†Ô∏è Shape incompat√≠vel: {pytorch_name} - Model: {model_shape}, GGUF: {gguf_shape}")
                    except Exception as e:
                        logger.debug(f"   ‚ùå Erro ao aplicar {pytorch_name}: {e}")
            
            # Carregar estado atualizado no modelo
            self.model.load_state_dict(model_state_dict, strict=False)
            
            logger.info(f"‚úÖ Aplicados {applied_count}/{len(gguf_weights)} pesos GGUF ao modelo")
            
        except Exception as e:
            logger.error(f"‚ùå Erro ao aplicar pesos GGUF: {e}")
    
    def _create_gguf_to_pytorch_mapping(self) -> dict:
        """Cria mapeamento entre nomes de tensores GGUF e PyTorch"""
        return {
            # Embeddings
            "token_embd.weight": "transformer.wte.weight",
            "pos_embd.weight": "transformer.wpe.weight",
            
            # Attention layers
            "blk.{}.attn_q.weight": "transformer.h.{}.attn.c_attn.weight",
            "blk.{}.attn_k.weight": "transformer.h.{}.attn.c_attn.weight", 
            "blk.{}.attn_v.weight": "transformer.h.{}.attn.c_attn.weight",
            "blk.{}.attn_output.weight": "transformer.h.{}.attn.c_proj.weight",
            
            # Feed forward
            "blk.{}.ffn_up.weight": "transformer.h.{}.mlp.c_fc.weight",
            "blk.{}.ffn_down.weight": "transformer.h.{}.mlp.c_proj.weight",
            
            # Layer norms
            "blk.{}.attn_norm.weight": "transformer.h.{}.ln_1.weight",
            "blk.{}.ffn_norm.weight": "transformer.h.{}.ln_2.weight",
            
            # Final layer norm and output
            "output_norm.weight": "transformer.ln_f.weight",
            "output.weight": "lm_head.weight"
        }

    def create_model_from_scratch(self, model_name="USBABC", base_model="gemma-portuguese-luana-2b.Q2_K.gguf"):
        """Cria um modelo do zero com train.json incorporado"""
        try:
            if not HAS_TORCH:
                self.logger.error("‚ùå PyTorch n√£o dispon√≠vel. Instale as depend√™ncias necess√°rias.")
                return None
                
            self.logger.info(f"üî® Criando modelo do zero: {model_name}")
            
            # Carregar modelo base
            self.logger.info(f"üì• Carregando modelo base: {base_model}")
            model = AutoModelForCausalLM.from_pretrained(base_model, torch_dtype=torch.float16)
            tokenizer = AutoTokenizer.from_pretrained(base_model)
            
            if tokenizer.pad_token is None:
                tokenizer.pad_token = tokenizer.eos_token
            
            # Incorporar train.json automaticamente
            train_path = "dados/train.json"
            if os.path.exists(train_path):
                self.logger.info(f"üìÇ Incorporando {train_path} no modelo...")
                self.prepare_dataset(train_path)
                
                # Aplicar LoRA para treinamento r√°pido
                target_modules = self._get_target_modules(model)
                peft_config = LoraConfig(
                    task_type=TaskType.CAUSAL_LM,
                    inference_mode=False,
                    r=16,
                    lora_alpha=32,
                    lora_dropout=0.1,
                    target_modules=target_modules
                )
                
                model = get_peft_model(model, peft_config)
                
                # Treinamento r√°pido com train.json
                training_args = TrainingArguments(
                    output_dir="./temp_training",
                    num_train_epochs=1,
                    per_device_train_batch_size=1,
                    gradient_accumulation_steps=8,
                    warmup_steps=10,
                    logging_steps=10,
                    save_steps=50,
                    evaluation_strategy="no",
                    save_strategy="epoch",
                    load_best_model_at_end=False,
                    report_to=None,
                    remove_unused_columns=False
                )
                
                trainer = Trainer(
                    model=model,
                    args=training_args,
                    train_dataset=self.dataset,
                    tokenizer=tokenizer,
                    data_collator=DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)
                )
                
                self.logger.info("üî• Treinando com train.json...")
                trainer.train()
                
                # Mesclar LoRA com modelo base
                self.logger.info("üîó Mesclando LoRA com modelo base...")
                model = model.merge_and_unload()
            
            # Salvar modelo tempor√°rio
            temp_dir = f"temp_{model_name}"
            os.makedirs(temp_dir, exist_ok=True)
            
            model.save_pretrained(temp_dir, safe_serialization=True)
            tokenizer.save_pretrained(temp_dir, safe_serialization=True)
            
            # Salvar modelo final em SafeTensors (formato original)
            final_model_dir = f"modelos/{model_name.lower()}"
            os.makedirs("modelos", exist_ok=True)
            
            self.logger.info(f"üíæ Salvando modelo em SafeTensors: {final_model_dir}")
            
            # Mover modelo do temp para diret√≥rio final
            if os.path.exists(final_model_dir):
                shutil.rmtree(final_model_dir)
            shutil.move(temp_dir, final_model_dir)
            
            # Adicionar metadados
            metadata = {
                "name": model_name,
                "base_model": base_model,
                "format": "safetensors",
                "trained_with": "train.json" if os.path.exists(train_path) else None,
                "created": datetime.now().isoformat()
            }
            
            metadata_path = os.path.join(final_model_dir, "model_info.json")
            with open(metadata_path, 'w', encoding='utf-8') as f:
                json.dump(metadata, f, indent=2, ensure_ascii=False)
            
            self.logger.info(f"‚úÖ Modelo criado em SafeTensors: {final_model_dir}")
            return final_model_dir
                
        except Exception as e:
            self.logger.error(f"‚ùå Erro na cria√ß√£o: {e}")
            return None

# O restante do script (fun√ß√£o main) √© mantido inalterado
def main():
    """Fun√ß√£o principal de execu√ß√£o"""
    try:
        # Tenta carregar o arquivo de dados do upload
        data_path = Path("/home/ubuntu/upload/pasted_content.txt")
        if not data_path.exists():
            # Tenta um nome de arquivo de dados padr√£o
            data_path = Path("dados/data.json")
            if not data_path.exists():
                logger.error("‚ùå Arquivo de dados n√£o encontrado. Certifique-se de que 'pasted_content.txt' ou 'dados/data.json' existe.")
                return

        # Instanciar o sistema
        system = AITrainingSystem()
        
        # O script original n√£o passa o model_path para load_model na main, 
        # mas a fun√ß√£o load_model exige. Vamos usar um nome de modelo padr√£o
        # para que o script possa ser executado, e o usu√°rio deve ajustar.
        
        model_to_load = system.config.get('training', {}).get('base_model_path', 'gemma-portuguese-luana-2b.Q2_K.gguf')
        
        # 1. Carregar modelo
        system.load_model(model_to_load)
        
        # 2. Preparar dataset
        system.prepare_dataset(str(data_path))
        
        # 3. Treinar
        # A chamada a save_model foi movida para dentro de train() para garantir que ocorra ap√≥s o treinamento.
        system.train()
        
        logger.info("üéâ Processo conclu√≠do com sucesso!")

    except Exception as e:
        logger.error(f"‚ùå Falha cr√≠tica na execu√ß√£o: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()

