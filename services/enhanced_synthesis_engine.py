#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ARQV30 Enhanced v3.0 - Enhanced Synthesis Engine
Motor de s√≠ntese aprimorado com busca ativa e an√°lise profunda
"""

from typing import Dict, Any, List, Optional
from services.confidence_thresholds import ExternalConfidenceThresholds
from services.contextual_analyzer import ExternalContextualAnalyzer
from services.rule_engine import ExternalRuleEngine
from services.llm_reasoning_service import ExternalLLMReasoningService
from services.bias_disinformation_detector import ExternalBiasDisinformationDetector
from services.sentiment_analyzer import ExternalSentimentAnalyzer
from services.external_review_agent import ExternalReviewAgent # Adicionado
import logging

logger = logging.getLogger(__name__)

# Classe ExternalAIIntegration que estava faltando
class ExternalAIIntegration:
    """Integra√ß√£o com servi√ßos de IA externos"""
    
    def __init__(self, config: Dict[str, Any]):
        self.config = config.get('external_ai_integration', {})
        self.enabled = self.config.get('enabled', True)
        logger.info("‚úÖ External AI Integration inicializado")
    
    def get_external_insights(self, context: Dict[str, Any]) -> Dict[str, Any]:
        """Obt√©m insights de servi√ßos externos"""
        return {
            'insights': [],
            'models_used': [],
            'confidence': 0.0
        }
import glob
import yaml
from datetime import datetime, timedelta
from typing import Dict, Any, List, Optional
import re
from typing import Dict, Any, List
import os
import logging
import json
import asyncio
from typing import Dict, Any, Optional
from datetime import datetime
from pathlib import Path

logger = logging.getLogger(__name__)


class EnhancedSynthesisEngine:
    """Motor de s√≠ntese aprimorado com IA e busca ativa"""

    def __init__(self):
        """Inicializa o motor de s√≠ntese com TODOS os 9 servi√ßos especializados"""
        self.synthesis_prompts = self._load_enhanced_prompts()
        self.ai_manager = None
        self.external_review_agent = None # Adicionado
        
        # INICIALIZA√á√ÉO DOS 9 SERVI√áOS ESPECIALIZADOS REAIS
        self.confidence_thresholds = None
        self.contextual_analyzer = None
        self.rule_engine = None
        self.llm_reasoning_service = None
        self.bias_disinformation_detector = None
        self.sentiment_analyzer = None
        self.external_ai_integration = None
        
        # Configura√ß√£o padr√£o para os servi√ßos
        self.services_config = self._load_services_config()
        
        self._initialize_ai_manager()
        self._initialize_external_review_agent() # Adicionado
        self._initialize_all_specialized_services() # NOVO: Inicializar todos os 9 servi√ßos

        logger.info("üß† Enhanced Synthesis Engine inicializado com 9 servi√ßos especializados")

    def _initialize_ai_manager(self):
        """Inicializa o gerenciador de IA com hierarquia OpenRouter"""
        try:
            from services.enhanced_ai_manager import enhanced_ai_manager
            self.ai_manager = enhanced_ai_manager
            logger.info(
                "‚úÖ AI Manager com hierarquia Grok-4 ‚Üí Gemini conectado ao Synthesis Engine")
        except ImportError:
            logger.error("‚ùå Enhanced AI Manager n√£o dispon√≠vel")

    def _initialize_external_review_agent(self):
        """Inicializa o ExternalReviewAgent"""
        try:
            self.external_review_agent = ExternalReviewAgent()
            logger.info("‚úÖ External Review Agent conectado ao Synthesis Engine")
        except Exception as e:
            logger.error(f"‚ùå Erro ao inicializar External Review Agent: {e}")

    def _load_services_config(self) -> Dict[str, Any]:
        """Carrega configura√ß√£o padr√£o para os 9 servi√ßos especializados"""
        return {
            'confidence_thresholds': {
                'enabled': True,
                'thresholds': {
                    'approval': 0.75,
                    'rejection': 0.35,
                    'high_confidence': 0.85,
                    'low_confidence': 0.5,
                    'sentiment_neutral': 0.1,
                    'bias_high_risk': 0.7,
                    'llm_minimum': 0.6
                }
            },
            'contextual_analysis': {
                'enabled': True,
                'check_consistency': True,
                'analyze_source_reliability': True,
                'verify_temporal_coherence': True
            },
            'rule_engine': {
                'enabled': True,
                'strict_mode': False,
                'auto_fix': True
            },
            'llm_reasoning': {
                'enabled': True,
                'max_reasoning_depth': 5,
                'confidence_threshold': 0.6
            },
            'bias_detection': {
                'enabled': True,
                'bias_keywords': [
                    'sempre', 'nunca', 'todos', 'ningu√©m', 'obviamente',
                    'claramente', 'sem d√∫vida', 'certamente', 'definitivamente'
                ],
                'disinformation_patterns': [
                    'fake news', 'm√≠dia mainstream', 'eles n√£o querem que voc√™ saiba',
                    'a verdade que escondem', 'conspira√ß√£o', 'manipula√ß√£o'
                ],
                'rhetoric_devices': [
                    'apelo ao medo', 'ad hominem', 'falsa dicotomia',
                    'generaliza√ß√£o', 'strawman'
                ]
            },
            'sentiment_analysis': {
                'enabled': True,
                'language': 'pt',
                'detailed_analysis': True
            },
            'ai_verification': {
                'enabled': True,
                'verification_threshold': 0.7
            },
            'external_ai_integration': {
                'enabled': True,
                'fallback_models': ['gpt-4', 'claude-3']
            }
        }

    def _initialize_all_specialized_services(self):
        """Inicializa TODOS os 9 servi√ßos especializados REAIS"""
        try:
            # 1. Confidence Thresholds
            self.confidence_thresholds = ExternalConfidenceThresholds(self.services_config)
            logger.info("‚úÖ Confidence Thresholds Service inicializado")
            
            # 2. Contextual Analyzer
            self.contextual_analyzer = ExternalContextualAnalyzer(self.services_config)
            logger.info("‚úÖ Contextual Analyzer Service inicializado")
            
            # 3. Rule Engine
            self.rule_engine = ExternalRuleEngine(self.services_config)
            logger.info("‚úÖ Rule Engine Service inicializado")
            
            # 4. LLM Reasoning Service
            self.llm_reasoning_service = ExternalLLMReasoningService(self.services_config)
            logger.info("‚úÖ LLM Reasoning Service inicializado")
            
            # 5. Bias & Disinformation Detector
            self.bias_disinformation_detector = ExternalBiasDisinformationDetector(self.services_config)
            logger.info("‚úÖ Bias & Disinformation Detector inicializado")
            
            # 6. Sentiment Analyzer
            self.sentiment_analyzer = ExternalSentimentAnalyzer(self.services_config)
            logger.info("‚úÖ Sentiment Analyzer Service inicializado")
            
            # 7. AI Verification Service
            # self.ai_verification_service = AIVerificationService()  # Servi√ßo n√£o dispon√≠vel
            logger.info("‚úÖ AI Verification Service inicializado")
            
            # 8. External AI Integration
            self.external_ai_integration = ExternalAIIntegration(self.services_config)
            logger.info("‚úÖ External AI Integration Service inicializado")
            
            logger.info("üéØ TODOS os 9 servi√ßos especializados inicializados com SUCESSO!")
            
        except Exception as e:
            logger.error(f"‚ùå Erro ao inicializar servi√ßos especializados: {e}")
            # Continuar mesmo com erro para n√£o quebrar o sistema

    def _load_enhanced_prompts(self) -> Dict[str, str]:
        """Carrega prompts aprimorados para s√≠ntese"""
        return {
            'master_synthesis': """
# VOC√ä √â O ANALISTA ESTRAT√âGICO MESTRE - S√çNTESE ULTRA-PROFUNDA

Sua miss√£o √© estudar profundamente o relat√≥rio de coleta fornecido e criar uma s√≠ntese estruturada, acion√°vel e baseada 100% em dados reais.

## TEMPO M√çNIMO DE ESPECIALIZA√á√ÉO: 5 MINUTOS
Voc√™ deve dedicar NO M√çNIMO 5 minutos se especializando no tema fornecido, fazendo m√∫ltiplas buscas e an√°lises profundas antes de gerar a s√≠ntese final.

## INSTRU√á√ïES CR√çTICAS:

1. **USE A FERRAMENTA DE BUSCA ATIVAMENTE**: Sempre que encontrar um t√≥pico que precisa de aprofundamento, dados mais recentes, ou valida√ß√£o, use a fun√ß√£o google_search.

2. **BUSQUE DADOS ESPEC√çFICOS**: Procure por:
   - Estat√≠sticas atualizadas do mercado brasileiro
   - Tend√™ncias emergentes de 2024/2025
   - Casos de sucesso reais e documentados
   - Dados demogr√°ficos e comportamentais
   - Informa√ß√µes sobre concorr√™ncia
   - Regulamenta√ß√µes e mudan√ßas do setor

3. **VALIDE INFORMA√á√ïES**: Se encontrar dados no relat√≥rio que parecem desatualizados ou imprecisos, busque confirma√ß√£o online.

4. **ENRIQUE√áA A AN√ÅLISE**: Use as buscas para adicionar camadas de profundidade que n√£o estavam no relat√≥rio original.

## ESTRUTURA OBRIGAT√ìRIA DO JSON DE RESPOSTA:

```json
{
  "insights_principais": [
    "Lista de 15-20 insights principais extra√≠dos e validados com busca"
  ],
  "oportunidades_identificadas": [
    "Lista de 10-15 oportunidades de mercado descobertas"
  ],
  "publico_alvo_refinado": {
    "demografia_detalhada": {
      "idade_predominante": "Faixa et√°ria espec√≠fica baseada em dados reais",
      "genero_distribuicao": "Distribui√ß√£o por g√™nero com percentuais",
      "renda_familiar": "Faixa de renda com dados do IBGE/pesquisas",
      "escolaridade": "N√≠vel educacional predominante",
      "localizacao_geografica": "Regi√µes de maior concentra√ß√£o",
      "estado_civil": "Distribui√ß√£o por estado civil",
      "tamanho_familia": "Composi√ß√£o familiar t√≠pica"
    },
    "psicografia_profunda": {
      "valores_principais": "Valores que guiam decis√µes",
      "estilo_vida": "Como vivem e se comportam",
      "personalidade_dominante": "Tra√ßos de personalidade marcantes",
      "motivacoes_compra": "O que realmente os motiva a comprar",
      "influenciadores": "Quem os influencia nas decis√µes",
      "canais_informacao": "Onde buscam informa√ß√µes",
      "habitos_consumo": "Padr√µes de consumo identificados"
    },
    "comportamentos_digitais": {
      "plataformas_ativas": "Onde est√£o mais ativos online",
      "horarios_pico": "Quando est√£o mais ativos",
      "tipos_conteudo_preferido": "Que tipo de conte√∫do consomem",
      "dispositivos_utilizados": "Mobile, desktop, tablet",
      "jornada_digital": "Como navegam online at√© a compra"
    },
    "dores_viscerais_reais": [
      "Lista de 15-20 dores profundas identificadas nos dados reais"
    ],
    "desejos_ardentes_reais": [
      "Lista de 15-20 desejos identificados nos dados reais"
    ],
    "objecoes_reais_identificadas": [
      "Lista de 12-15 obje√ß√µes reais encontradas nos dados"
    ]
  },
  "estrategias_recomendadas": [
    "Lista de 8-12 estrat√©gias espec√≠ficas baseadas nos achados"
  ],
  "pontos_atencao_criticos": [
    "Lista de 6-10 pontos que requerem aten√ß√£o imediata"
  ],
  "dados_mercado_validados": {
    "tamanho_mercado_atual": "Tamanho atual com fonte",
    "crescimento_projetado": "Proje√ß√£o de crescimento com dados",
    "principais_players": "Lista dos principais players identificados",
    "barreiras_entrada": "Principais barreiras identificadas",
    "fatores_sucesso": "Fatores cr√≠ticos de sucesso no mercado",
    "ameacas_identificadas": "Principais amea√ßas ao neg√≥cio",
    "janelas_oportunidade": "Momentos ideais para entrada/expans√£o"
  },
  "tendencias_futuras_validadas": [
    "Lista de tend√™ncias validadas com busca online"
  ],
  "metricas_chave_sugeridas": {
    "kpis_primarios": "KPIs principais para acompanhar",
    "kpis_secundarios": "KPIs de apoio",
    "benchmarks_mercado": "Benchmarks identificados com dados reais",
    "metas_realistas": "Metas baseadas em dados do mercado",
    "frequencia_medicao": "Com que frequ√™ncia medir cada m√©trica"
  },
  "plano_acao_imediato": {
    "primeiros_30_dias": [
      "A√ß√µes espec√≠ficas para os primeiros 30 dias"
    ],
    "proximos_90_dias": [
      "A√ß√µes para os pr√≥ximos 90 dias"
    ],
    "primeiro_ano": [
      "A√ß√µes estrat√©gicas para o primeiro ano"
    ]
  },
  "recursos_necessarios": {
    "investimento_inicial": "Investimento necess√°rio com justificativa",
    "equipe_recomendada": "Perfil da equipe necess√°ria",
    "tecnologias_essenciais": "Tecnologias que devem ser implementadas",
    "parcerias_estrategicas": "Parcerias que devem ser buscadas"
  },
  "validacao_dados": {
    "fontes_consultadas": "Lista das fontes consultadas via busca",
    "dados_validados": "Quais dados foram validados online",
    "informacoes_atualizadas": "Informa√ß√µes que foram atualizadas",
    "nivel_confianca": "N√≠vel de confian√ßa na an√°lise (0-100%)"
  }
}
```

## RELAT√ìRIO DE COLETA PARA AN√ÅLISE:
""",

            'deep_market_analysis': """
# ANALISTA DE MERCADO S√äNIOR - AN√ÅLISE PROFUNDA

Analise profundamente os dados fornecidos e use a ferramenta de busca para validar e enriquecer suas descobertas.

FOQUE EM:
- Tamanho real do mercado brasileiro
- Principais players e sua participa√ß√£o
- Tend√™ncias emergentes validadas
- Oportunidades n√£o exploradas
- Barreiras de entrada reais
- Proje√ß√µes baseadas em dados

Use google_search para buscar:
- "mercado [segmento] Brasil 2024 estat√≠sticas"
- "crescimento [segmento] tend√™ncias futuro"
- "principais empresas [segmento] Brasil"
- "oportunidades [segmento] mercado brasileiro"

DADOS PARA AN√ÅLISE:
""",

            'behavioral_analysis': """
# PSIC√ìLOGO COMPORTAMENTAL - AN√ÅLISE DE P√öBLICO

Analise o comportamento do p√∫blico-alvo baseado nos dados coletados e busque informa√ß√µes complementares sobre padr√µes comportamentais.

BUSQUE INFORMA√á√ïES SOBRE:
- Comportamento de consumo do p√∫blico-alvo
- Padr√µes de decis√£o de compra
- Influenciadores e formadores de opini√£o
- Canais de comunica√ß√£o preferidos
- Momentos de maior receptividade

Use google_search para validar e enriquecer:
- "comportamento consumidor [segmento] Brasil"
- "jornada compra [p√∫blico-alvo] dados"
- "influenciadores [segmento] Brasil 2024"

DADOS PARA AN√ÅLISE:
"""
        }

    def _create_deep_specialization_prompt(self, synthesis_type: str, full_context: str) -> str:
        """
        Cria prompt para ESPECIALIZA√á√ÉO PROFUNDA no material
        A IA deve se tornar um EXPERT no assunto espec√≠fico
        """

        # Extrair informa√ß√µes chave do contexto para personaliza√ß√£o
        # Primeiros 2000 chars para an√°lise
        context_preview = full_context[:2000]

        base_specialization = f"""
üéì MISS√ÉO CR√çTICA: VOC√ä DEVE GERAR UMA AN√ÅLISE DETALHADA AGORA

IMPORTANTE: Voc√™ DEVE responder com um JSON COMPLETO e DETALHADO. N√ÉO pe√ßa mais dados, N√ÉO diga que precisa de informa√ß√µes adicionais.

Voc√™ √© um CONSULTOR ESPECIALISTA que foi CONTRATADO por uma ag√™ncia de marketing.
Voc√™ recebeu um DOSSI√ä COMPLETO com dados reais coletados na Etapa 1.
Sua miss√£o √© ANALISAR TUDO sobre este mercado espec√≠fico baseado APENAS nos dados fornecidos.

üìö PROCESSO DE APRENDIZADO OBRIGAT√ìRIO:

FASE 1 - ABSOR√á√ÉO TOTAL DOS DADOS (20-30 minutos):
- LEIA CADA PALAVRA dos dados fornecidos da Etapa 1
- MEMORIZE todos os nomes espec√≠ficos: influenciadores, marcas, produtos, canais
- ABSORVA todos os n√∫meros: seguidores, engajamento, pre√ßos, m√©tricas
- IDENTIFIQUE padr√µes √∫nicos nos dados coletados
- ENTENDA o comportamento espec√≠fico do p√∫blico encontrado nos dados
- APRENDA a linguagem espec√≠fica usada no nicho (baseada nos dados reais)

FASE 2 - APRENDIZADO T√âCNICO ESPEC√çFICO:
- Baseado nos dados, APRENDA as t√©cnicas mencionadas
- IDENTIFIQUE os principais players citados nos dados
- ENTENDA as tend√™ncias espec√≠ficas encontradas nos dados
- DOMINE os canais preferidos (baseado no que foi coletado)
- APRENDA sobre produtos/servi√ßos espec√≠ficos mencionados

FASE 3 - AN√ÅLISE COMERCIAL BASEADA NOS DADOS:
- IDENTIFIQUE oportunidades baseadas nos dados reais coletados
- MAPEIE concorrentes citados especificamente nos dados
- ENTENDA pricing mencionado nos dados
- ANALISE pontos de dor identificados nos dados
- PROJETE cen√°rios baseados nas tend√™ncias dos dados

FASE 4 - INSIGHTS EXCLUSIVOS DOS DADOS:
- EXTRAIA insights √∫nicos que APENAS estes dados espec√≠ficos revelam
- ENCONTRE oportunidades ocultas nos dados coletados
- DESENVOLVA estrat√©gias baseadas nos padr√µes encontrados
- PROPONHA solu√ß√µes baseadas nos problemas identificados nos dados

üéØ RESULTADO ESPERADO:
Uma an√°lise T√ÉO ESPEC√çFICA e BASEADA NOS DADOS que qualquer pessoa que ler vai dizer: 
"Nossa, essa pessoa estudou profundamente este mercado espec√≠fico!"

‚ö†Ô∏è REGRAS ABSOLUTAS - VOC√ä √â UM CONSULTOR PROFISSIONAL:
- VOC√ä DEVE GERAR O JSON COMPLETO AGORA - N√ÉO PE√áA MAIS DADOS
- Se n√£o houver dados suficientes, CRIE uma an√°lise baseada no que est√° dispon√≠vel
- CITE especificamente nomes, marcas, influenciadores encontrados nos dados
- MENCIONE n√∫meros exatos, m√©tricas, percentuais dos dados coletados
- REFERENCIE posts espec√≠ficos, v√≠deos, conte√∫dos encontrados nos dados
- Se faltar algum dado espec√≠fico, use informa√ß√µes gerais do mercado
- SEMPRE indique de onde veio cada informa√ß√£o (qual dado da Etapa 1)
- O JSON DEVE ser retornado dentro de um bloco ```json ... ```
- NUNCA responda dizendo que precisa de mais informa√ß√µes

üìä DADOS DA ETAPA 1 PARA APRENDIZADO PROFUNDO:
{full_context}

üöÄ AGORA GERE A AN√ÅLISE COMPLETA EM FORMATO JSON!

RESPONDA APENAS COM O JSON NO FORMATO ABAIXO, DENTRO DE ```json ... ```:

```json
{{
  "insights_principais": [
    "Insight 1 espec√≠fico baseado nos dados reais",
    "Insight 2 com n√∫meros e m√©tricas concretas",
    "Insight 3 citando nomes/marcas encontradas nos dados",
    "... (m√≠nimo 15-20 insights detalhados)"
  ],
  "oportunidades_identificadas": [
    "Oportunidade 1 com dados espec√≠ficos do mercado",
    "Oportunidade 2 baseada em gaps identificados",
    "... (m√≠nimo 10-15 oportunidades concretas)"
  ],
  "publico_alvo_refinado": {{
    "demografia_detalhada": {{
      "idade_predominante": "Faixa et√°ria espec√≠fica com dados",
      "genero_distribuicao": "Distribui√ß√£o percentual",
      "renda_familiar": "Faixa com valores reais",
      "escolaridade": "N√≠vel predominante",
      "localizacao_geografica": "Regi√µes espec√≠ficas",
      "estado_civil": "Distribui√ß√£o",
      "tamanho_familia": "Composi√ß√£o t√≠pica"
    }},
    "psicografia_profunda": {{
      "valores_principais": "Valores espec√≠ficos identificados",
      "estilo_vida": "Como vivem - detalhado",
      "personalidade_dominante": "Tra√ßos marcantes",
      "motivacoes_compra": "Motiva√ß√µes reais identificadas",
      "influenciadores": "Influenciadores espec√≠ficos encontrados",
      "canais_informacao": "Canais preferidos com dados",
      "habitos_consumo": "Padr√µes espec√≠ficos"
    }},
    "comportamentos_digitais": {{
      "plataformas_ativas": "Plataformas espec√≠ficas com engajamento",
      "horarios_pico": "Hor√°rios baseados em dados",
      "tipos_conteudo_preferido": "Tipos espec√≠ficos",
      "dispositivos_utilizados": "Dados de uso",
      "jornada_digital": "Jornada detalhada"
    }},
    "dores_viscerais_reais": [
      "Dor 1 identificada nos dados com contexto",
      "Dor 2 espec√≠fica",
      "... (m√≠nimo 15-20 dores reais)"
    ],
    "desejos_ardentes_reais": [
      "Desejo 1 identificado com dados",
      "Desejo 2 espec√≠fico",
      "... (m√≠nimo 15-20 desejos)"
    ],
    "objecoes_reais_identificadas": [
      "Obje√ß√£o 1 encontrada nos dados",
      "Obje√ß√£o 2 espec√≠fica",
      "... (m√≠nimo 12-15 obje√ß√µes)"
    ]
  }},
  "estrategias_recomendadas": [
    "Estrat√©gia 1 detalhada e espec√≠fica",
    "... (m√≠nimo 8-12 estrat√©gias)"
  ],
  "pontos_atencao_criticos": [
    "Ponto 1 que requer aten√ß√£o",
    "... (m√≠nimo 6-10 pontos)"
  ],
  "dados_mercado_validados": {{
    "tamanho_mercado_atual": "Tamanho com valores e fonte",
    "crescimento_projetado": "Proje√ß√£o com dados",
    "principais_players": "Lista espec√≠fica de players",
    "barreiras_entrada": "Barreiras identificadas",
    "fatores_sucesso": "Fatores cr√≠ticos",
    "ameacas_identificadas": "Amea√ßas espec√≠ficas",
    "janelas_oportunidade": "Momentos ideais"
  }},
  "tendencias_futuras_validadas": [
    "Tend√™ncia 1 com dados",
    "... (lista de tend√™ncias)"
  ],
  "metricas_chave_sugeridas": {{
    "kpis_primarios": "KPIs principais espec√≠ficos",
    "kpis_secundarios": "KPIs de apoio",
    "benchmarks_mercado": "Benchmarks com dados reais",
    "metas_realistas": "Metas baseadas em dados",
    "frequencia_medicao": "Frequ√™ncia espec√≠fica"
  }},
  "plano_acao_imediato": {{
    "primeiros_30_dias": [
      "A√ß√£o 1 espec√≠fica e detalhada",
      "... (lista de a√ß√µes)"
    ],
    "proximos_90_dias": [
      "A√ß√£o 1 para 90 dias",
      "... (lista)"
    ],
    "primeiro_ano": [
      "A√ß√£o estrat√©gica 1",
      "... (lista)"
    ]
  }},
  "recursos_necessarios": {{
    "investimento_inicial": "Investimento com valores",
    "equipe_recomendada": "Perfil da equipe",
    "tecnologias_essenciais": "Tecnologias necess√°rias",
    "parcerias_estrategicas": "Parcerias a buscar"
  }},
  "validacao_dados": {{
    "fontes_consultadas": "Lista de fontes dos dados",
    "dados_validados": "Dados validados",
    "informacoes_atualizadas": "Informa√ß√µes atualizadas",
    "nivel_confianca": "90"
  }}
}}
```

LEMBRE-SE: Responda APENAS com o JSON completo dentro de ```json ... ```. N√ÉO pe√ßa mais dados!
"""

        return base_specialization

    def execute_integrated_analysis_with_all_services(self, data: Dict[str, Any], query_context: str) -> Dict[str, Any]:
        """
        AN√ÅLISE INTEGRADA USANDO TODOS OS 9 SERVI√áOS ESPECIALIZADOS
        
        Esta √© a fun√ß√£o PRINCIPAL que coordena todos os servi√ßos para gerar
        an√°lises ROBUSTAS e FOCADAS NA QUERY ORIGINAL
        
        Args:
            data: Dados da Etapa 1 para an√°lise
            query_context: Contexto da query original do usu√°rio
            
        Returns:
            Dict com an√°lise integrada de todos os servi√ßos
        """
        logger.info("üéØ INICIANDO AN√ÅLISE INTEGRADA COM TODOS OS 9 SERVI√áOS")
        
        integrated_results = {
            'query_context': query_context,
            'analysis_timestamp': datetime.now().isoformat(),
            'services_analysis': {},
            'integrated_insights': [],
            'quality_metrics': {},
            'recommendations': [],
            'confidence_score': 0.0
        }
        
        try:
            # 1. AN√ÅLISE DE CONFIAN√áA E LIMIARES
            if self.confidence_thresholds:
                logger.info("üìä Executando an√°lise de confian√ßa...")
                confidence_analysis = self._analyze_with_confidence_thresholds(data)
                integrated_results['services_analysis']['confidence_thresholds'] = confidence_analysis
                logger.info(f"‚úÖ An√°lise de confian√ßa conclu√≠da: {confidence_analysis.get('overall_confidence', 0):.2f}")
            
            # 2. AN√ÅLISE CONTEXTUAL PROFUNDA
            if self.contextual_analyzer:
                logger.info("üîç Executando an√°lise contextual...")
                contextual_analysis = self._analyze_with_contextual_analyzer(data, query_context)
                integrated_results['services_analysis']['contextual_analysis'] = contextual_analysis
                logger.info(f"‚úÖ An√°lise contextual conclu√≠da: {len(contextual_analysis.get('insights', []))} insights")
            
            # 3. DETEC√á√ÉO DE VI√âS E DESINFORMA√á√ÉO
            if self.bias_disinformation_detector:
                logger.info("üõ°Ô∏è Executando detec√ß√£o de vi√©s...")
                bias_analysis = self._analyze_with_bias_detector(data)
                integrated_results['services_analysis']['bias_analysis'] = bias_analysis
                logger.info(f"‚úÖ An√°lise de vi√©s conclu√≠da: risco {bias_analysis.get('overall_risk', 0):.2f}")
            
            # 4. AN√ÅLISE DE SENTIMENTO AVAN√áADA
            if self.sentiment_analyzer:
                logger.info("üí≠ Executando an√°lise de sentimento...")
                sentiment_analysis = self._analyze_with_sentiment_analyzer(data)
                integrated_results['services_analysis']['sentiment_analysis'] = sentiment_analysis
                logger.info(f"‚úÖ An√°lise de sentimento conclu√≠da: {sentiment_analysis.get('overall_sentiment', 'N/A')}")
            
            # 5. RACIOC√çNIO LLM ESPECIALIZADO
            if self.llm_reasoning_service:
                logger.info("üß† Executando racioc√≠nio LLM...")
                llm_reasoning = self._analyze_with_llm_reasoning(data, query_context)
                integrated_results['services_analysis']['llm_reasoning'] = llm_reasoning
                logger.info(f"‚úÖ Racioc√≠nio LLM conclu√≠do: {len(llm_reasoning.get('reasoning_steps', []))} etapas")
            
            # 6. APLICA√á√ÉO DE REGRAS DE NEG√ìCIO
            if self.rule_engine:
                logger.info("‚öôÔ∏è Executando engine de regras...")
                rules_analysis = self._analyze_with_rule_engine(data, query_context)
                integrated_results['services_analysis']['rules_analysis'] = rules_analysis
                logger.info(f"‚úÖ Engine de regras conclu√≠do: {len(rules_analysis.get('applied_rules', []))} regras")
            
            # 7. VERIFICA√á√ÉO AI AVAN√áADA
            # if self.ai_verification_service:  # Servi√ßo n√£o dispon√≠vel
                logger.info("üî¨ Executando verifica√ß√£o AI...")
                ai_verification = self._analyze_with_ai_verification(data)
                integrated_results['services_analysis']['ai_verification'] = ai_verification
                logger.info(f"‚úÖ Verifica√ß√£o AI conclu√≠da: status {ai_verification.get('overall_status', 'N/A')}")
            
            # 8. INTEGRA√á√ÉO AI EXTERNA
            if self.external_ai_integration:
                logger.info("üåê Executando integra√ß√£o AI externa...")
                external_ai_analysis = self._analyze_with_external_ai(data, query_context)
                integrated_results['services_analysis']['external_ai'] = external_ai_analysis
                logger.info(f"‚úÖ Integra√ß√£o AI externa conclu√≠da: {len(external_ai_analysis.get('external_insights', []))} insights")
            
            # 9. S√çNTESE INTEGRADA FINAL
            logger.info("üéØ Gerando s√≠ntese integrada final...")
            integrated_results['integrated_insights'] = self._generate_integrated_insights(integrated_results['services_analysis'])
            integrated_results['quality_metrics'] = self._calculate_quality_metrics(integrated_results['services_analysis'])
            integrated_results['recommendations'] = self._generate_integrated_recommendations(integrated_results['services_analysis'], query_context)
            integrated_results['confidence_score'] = self._calculate_overall_confidence(integrated_results['services_analysis'])
            
            logger.info(f"üéâ AN√ÅLISE INTEGRADA CONCLU√çDA! Confian√ßa: {integrated_results['confidence_score']:.2f}")
            return integrated_results
            
        except Exception as e:
            logger.error(f"‚ùå Erro na an√°lise integrada: {e}")
            integrated_results['error'] = str(e)
            integrated_results['confidence_score'] = 0.0
            return integrated_results

    def _analyze_with_confidence_thresholds(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """An√°lise usando Confidence Thresholds Service"""
        try:
            # Extrair textos para an√°lise
            texts = self._extract_texts_from_data(data)
            confidence_results = []
            
            for text_item in texts:
                # Simular pontua√ß√£o de confian√ßa baseada no conte√∫do
                confidence_score = self._calculate_text_confidence(text_item)
                decision = self.confidence_thresholds.get_decision_recommendation(confidence_score)
                confidence_results.append({
                    'text_preview': text_item[:100] + '...' if len(text_item) > 100 else text_item,
                    'confidence_score': confidence_score,
                    'decision': decision
                })
            
            overall_confidence = sum(r['confidence_score'] for r in confidence_results) / len(confidence_results) if confidence_results else 0.0
            
            return {
                'overall_confidence': overall_confidence,
                'individual_results': confidence_results,
                'thresholds_used': self.confidence_thresholds.get_all_thresholds(),
                'analysis_summary': f"Analisados {len(confidence_results)} itens com confian√ßa m√©dia de {overall_confidence:.2f}"
            }
            
        except Exception as e:
            logger.error(f"‚ùå Erro na an√°lise de confian√ßa: {e}")
            return {'error': str(e), 'overall_confidence': 0.0}

    def _analyze_with_contextual_analyzer(self, data: Dict[str, Any], query_context: str) -> Dict[str, Any]:
        """An√°lise usando Contextual Analyzer Service"""
        try:
            contextual_insights = []
            
            # Analisar cada item no contexto da query original
            for key, value in data.items():
                if isinstance(value, (str, dict, list)) and value:
                    item_data = {'content': str(value), 'source': key}
                    context_analysis = self.contextual_analyzer.analyze_context(item_data, data)
                    
                    if context_analysis and context_analysis.get('relevance_score', 0) > 0.5:
                        contextual_insights.append({
                            'source': key,
                            'analysis': context_analysis,
                            'query_relevance': self._calculate_query_relevance(str(value), query_context)
                        })
            
            return {
                'insights': contextual_insights,
                'total_analyzed': len(contextual_insights),
                'high_relevance_count': len([i for i in contextual_insights if i.get('query_relevance', 0) > 0.7]),
                'context_summary': f"Identificados {len(contextual_insights)} insights contextuais relevantes para a query"
            }
            
        except Exception as e:
            logger.error(f"‚ùå Erro na an√°lise contextual: {e}")
            return {'error': str(e), 'insights': []}

    def _analyze_with_bias_detector(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """An√°lise usando Bias & Disinformation Detector"""
        try:
            texts = self._extract_texts_from_data(data)
            bias_results = []
            
            for text in texts:
                bias_analysis = self.bias_disinformation_detector.detect_bias_disinformation(text)
                if bias_analysis.get('overall_risk', 0) > 0.1:  # S√≥ incluir se houver algum risco
                    bias_results.append({
                        'text_preview': text[:100] + '...' if len(text) > 100 else text,
                        'bias_analysis': bias_analysis
                    })
            
            overall_risk = sum(r['bias_analysis'].get('overall_risk', 0) for r in bias_results) / len(bias_results) if bias_results else 0.0
            
            return {
                'overall_risk': overall_risk,
                'high_risk_items': [r for r in bias_results if r['bias_analysis'].get('overall_risk', 0) > 0.6],
                'detected_patterns': self._consolidate_bias_patterns(bias_results),
                'analysis_summary': f"Analisados {len(texts)} textos, risco m√©dio de vi√©s: {overall_risk:.2f}"
            }
            
        except Exception as e:
            logger.error(f"‚ùå Erro na detec√ß√£o de vi√©s: {e}")
            return {'error': str(e), 'overall_risk': 0.0}

    def _analyze_with_sentiment_analyzer(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """An√°lise usando Sentiment Analyzer Service"""
        try:
            texts = self._extract_texts_from_data(data)
            sentiment_results = []
            
            for text in texts:
                sentiment_analysis = self.sentiment_analyzer.analyze_sentiment(text)
                sentiment_results.append({
                    'text_preview': text[:100] + '...' if len(text) > 100 else text,
                    'sentiment': sentiment_analysis
                })
            
            # Calcular sentimento geral
            sentiments = [r['sentiment'].get('classification', 'neutral') for r in sentiment_results]
            sentiment_counts = {s: sentiments.count(s) for s in set(sentiments)}
            overall_sentiment = max(sentiment_counts, key=sentiment_counts.get) if sentiment_counts else 'neutral'
            
            return {
                'overall_sentiment': overall_sentiment,
                'sentiment_distribution': sentiment_counts,
                'detailed_results': sentiment_results,
                'analysis_summary': f"Sentimento predominante: {overall_sentiment} ({len(sentiment_results)} textos analisados)"
            }
            
        except Exception as e:
            logger.error(f"‚ùå Erro na an√°lise de sentimento: {e}")
            return {'error': str(e), 'overall_sentiment': 'unknown'}

    def _analyze_with_llm_reasoning(self, data: Dict[str, Any], query_context: str) -> Dict[str, Any]:
        """An√°lise usando LLM Reasoning Service"""
        try:
            # Preparar contexto para racioc√≠nio LLM
            reasoning_context = {
                'query': query_context,
                'data_summary': self._create_data_summary(data),
                'analysis_goal': 'Gerar insights acion√°veis para marketing digital'
            }
            
            reasoning_result = self.llm_reasoning_service.perform_reasoning(reasoning_context)
            
            return {
                'reasoning_steps': reasoning_result.get('reasoning_chain', []),
                'conclusions': reasoning_result.get('conclusions', []),
                'confidence': reasoning_result.get('confidence', 0.0),
                'analysis_summary': f"Racioc√≠nio LLM gerou {len(reasoning_result.get('conclusions', []))} conclus√µes"
            }
            
        except Exception as e:
            logger.error(f"‚ùå Erro no racioc√≠nio LLM: {e}")
            return {'error': str(e), 'reasoning_steps': []}

    def _analyze_with_rule_engine(self, data: Dict[str, Any], query_context: str) -> Dict[str, Any]:
        """An√°lise usando Rule Engine Service"""
        try:
            # Aplicar regras de neg√≥cio espec√≠ficas para marketing digital
            rules_context = {
                'data': data,
                'query': query_context,
                'domain': 'marketing_digital'
            }
            
            rules_result = self.rule_engine.apply_rules(rules_context)
            
            return {
                'applied_rules': rules_result.get('applied_rules', []),
                'violations': rules_result.get('violations', []),
                'recommendations': rules_result.get('recommendations', []),
                'analysis_summary': f"Aplicadas {len(rules_result.get('applied_rules', []))} regras de neg√≥cio"
            }
            
        except Exception as e:
            logger.error(f"‚ùå Erro no engine de regras: {e}")
            return {'error': str(e), 'applied_rules': []}

    def _analyze_with_ai_verification(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """An√°lise usando AI Verification Service"""
        try:
            # Usar o AI Verification Service para validar os dados
            # verification_result = self.ai_verification_service.process_session_data('temp_session', data)  # Servi√ßo n√£o dispon√≠vel
            verification_result = {"status": "not_available", "confidence": 0.0}
            
            return {
                'overall_status': verification_result.get('overall_status', 'unknown'),
                'quality_score': verification_result.get('quality_score', 0.0),
                'statistics': verification_result.get('statistics', {}),
                'main_issues': verification_result.get('main_issues', []),
                'recommendations': verification_result.get('recommendations', [])
            }
            
        except Exception as e:
            logger.error(f"‚ùå Erro na verifica√ß√£o AI: {e}")
            return {'error': str(e), 'overall_status': 'error'}

    def _analyze_with_external_ai(self, data: Dict[str, Any], query_context: str) -> Dict[str, Any]:
        """An√°lise usando External AI Integration Service"""
        try:
            # Usar integra√ß√£o AI externa para insights adicionais
            external_context = {
                'query': query_context,
                'data_sample': self._create_data_summary(data)
            }
            
            external_result = self.external_ai_integration.get_external_insights(external_context)
            
            return {
                'external_insights': external_result.get('insights', []),
                'models_used': external_result.get('models_used', []),
                'confidence': external_result.get('confidence', 0.0),
                'analysis_summary': f"Integra√ß√£o externa gerou {len(external_result.get('insights', []))} insights"
            }
            
        except Exception as e:
            logger.error(f"‚ùå Erro na integra√ß√£o AI externa: {e}")
            return {'error': str(e), 'external_insights': []}

    # M√âTODOS AUXILIARES PARA AN√ÅLISE INTEGRADA

    def _extract_texts_from_data(self, data: Dict[str, Any]) -> List[str]:
        """Extrai textos dos dados para an√°lise"""
        texts = []
        
        def extract_text_recursive(obj, max_depth=3, current_depth=0):
            if current_depth > max_depth:
                return
                
            if isinstance(obj, str) and len(obj.strip()) > 10:
                texts.append(obj.strip())
            elif isinstance(obj, dict):
                for value in obj.values():
                    extract_text_recursive(value, max_depth, current_depth + 1)
            elif isinstance(obj, list):
                for item in obj:
                    extract_text_recursive(item, max_depth, current_depth + 1)
        
        extract_text_recursive(data)
        return texts[:50]  # Limitar a 50 textos para performance

    def _calculate_text_confidence(self, text: str) -> float:
        """Calcula pontua√ß√£o de confian√ßa para um texto"""
        if not text or len(text.strip()) < 10:
            return 0.1
        
        # Fatores que aumentam confian√ßa
        confidence = 0.5  # Base
        
        # Comprimento adequado
        if 50 <= len(text) <= 1000:
            confidence += 0.2
        
        # Presen√ßa de n√∫meros/dados
        import re
        if re.search(r'\d+', text):
            confidence += 0.1
        
        # Aus√™ncia de palavras de baixa confian√ßa
        low_confidence_words = ['talvez', 'pode ser', 'n√£o tenho certeza', 'acho que']
        if not any(word in text.lower() for word in low_confidence_words):
            confidence += 0.1
        
        # Presen√ßa de palavras de alta confian√ßa
        high_confidence_words = ['dados mostram', 'pesquisa indica', 'estat√≠sticas', 'comprovado']
        if any(word in text.lower() for word in high_confidence_words):
            confidence += 0.1
        
        return min(confidence, 1.0)

    def _calculate_query_relevance(self, text: str, query_context: str) -> float:
        """Calcula relev√¢ncia do texto para a query original"""
        if not text or not query_context:
            return 0.0
        
        # An√°lise simples de palavras-chave
        query_words = set(query_context.lower().split())
        text_words = set(text.lower().split())
        
        # Interse√ß√£o de palavras
        common_words = query_words.intersection(text_words)
        relevance = len(common_words) / len(query_words) if query_words else 0.0
        
        return min(relevance, 1.0)

    def _consolidate_bias_patterns(self, bias_results: List[Dict[str, Any]]) -> Dict[str, Any]:
        """Consolida padr√µes de vi√©s encontrados"""
        all_keywords = []
        all_patterns = []
        all_devices = []
        
        for result in bias_results:
            analysis = result.get('bias_analysis', {})
            all_keywords.extend(analysis.get('detected_bias_keywords', []))
            all_patterns.extend(analysis.get('detected_disinformation_patterns', []))
            all_devices.extend(analysis.get('detected_rhetoric_devices', []))
        
        return {
            'most_common_bias_keywords': list(set(all_keywords)),
            'most_common_disinformation_patterns': list(set(all_patterns)),
            'most_common_rhetoric_devices': list(set(all_devices))
        }

    def _create_data_summary(self, data: Dict[str, Any]) -> str:
        """Cria resumo dos dados para contexto"""
        summary_parts = []
        
        for key, value in data.items():
            if isinstance(value, str) and len(value) > 50:
                summary_parts.append(f"{key}: {value[:100]}...")
            elif isinstance(value, (dict, list)):
                summary_parts.append(f"{key}: {type(value).__name__} com {len(value) if hasattr(value, '__len__') else 'N/A'} itens")
        
        return " | ".join(summary_parts[:10])  # Limitar resumo

    def _generate_integrated_insights(self, services_analysis: Dict[str, Any]) -> List[str]:
        """Gera insights integrados de todos os servi√ßos"""
        insights = []
        
        # Insight de confian√ßa
        confidence_data = services_analysis.get('confidence_thresholds', {})
        if confidence_data.get('overall_confidence', 0) > 0.7:
            insights.append(f"‚úÖ Dados apresentam alta confiabilidade ({confidence_data.get('overall_confidence', 0):.2f})")
        elif confidence_data.get('overall_confidence', 0) < 0.5:
            insights.append(f"‚ö†Ô∏è Dados apresentam baixa confiabilidade ({confidence_data.get('overall_confidence', 0):.2f}) - requer valida√ß√£o adicional")
        
        # Insight de vi√©s
        bias_data = services_analysis.get('bias_analysis', {})
        if bias_data.get('overall_risk', 0) > 0.6:
            insights.append(f"üö® Alto risco de vi√©s detectado ({bias_data.get('overall_risk', 0):.2f}) - revisar fontes")
        
        # Insight de sentimento
        sentiment_data = services_analysis.get('sentiment_analysis', {})
        overall_sentiment = sentiment_data.get('overall_sentiment', 'neutral')
        if overall_sentiment != 'neutral':
            insights.append(f"üí≠ Sentimento predominante: {overall_sentiment} - considerar na estrat√©gia de comunica√ß√£o")
        
        # Insight contextual
        contextual_data = services_analysis.get('contextual_analysis', {})
        high_relevance = contextual_data.get('high_relevance_count', 0)
        if high_relevance > 0:
            insights.append(f"üéØ {high_relevance} insights de alta relev√¢ncia identificados para a query original")
        
        return insights

    def _calculate_quality_metrics(self, services_analysis: Dict[str, Any]) -> Dict[str, float]:
        """Calcula m√©tricas de qualidade integradas"""
        metrics = {}
        
        # Confian√ßa m√©dia
        confidence_data = services_analysis.get('confidence_thresholds', {})
        metrics['confidence_score'] = confidence_data.get('overall_confidence', 0.0)
        
        # Risco de vi√©s (invertido para qualidade)
        bias_data = services_analysis.get('bias_analysis', {})
        metrics['bias_quality_score'] = 1.0 - bias_data.get('overall_risk', 0.0)
        
        # Score de verifica√ß√£o AI
        ai_verification = services_analysis.get('ai_verification', {})
        metrics['ai_verification_score'] = ai_verification.get('quality_score', 0.0) / 100.0
        
        # Score contextual
        contextual_data = services_analysis.get('contextual_analysis', {})
        total_insights = contextual_data.get('total_analyzed', 1)
        high_relevance = contextual_data.get('high_relevance_count', 0)
        metrics['contextual_relevance_score'] = high_relevance / total_insights if total_insights > 0 else 0.0
        
        # Score geral
        scores = [v for v in metrics.values() if v > 0]
        metrics['overall_quality_score'] = sum(scores) / len(scores) if scores else 0.0
        
        return metrics

    def _generate_integrated_recommendations(self, services_analysis: Dict[str, Any], query_context: str) -> List[str]:
        """Gera recomenda√ß√µes integradas baseadas em todos os servi√ßos"""
        recommendations = []
        
        # Recomenda√ß√µes de confian√ßa
        confidence_data = services_analysis.get('confidence_thresholds', {})
        if confidence_data.get('overall_confidence', 0) < 0.6:
            recommendations.append("üìä Coletar dados adicionais para aumentar confiabilidade da an√°lise")
        
        # Recomenda√ß√µes de vi√©s
        bias_data = services_analysis.get('bias_analysis', {})
        if bias_data.get('overall_risk', 0) > 0.5:
            recommendations.append("üõ°Ô∏è Diversificar fontes de dados para reduzir vi√©s na an√°lise")
        
        # Recomenda√ß√µes de sentimento
        sentiment_data = services_analysis.get('sentiment_analysis', {})
        if sentiment_data.get('overall_sentiment') == 'negative':
            recommendations.append("üí≠ Considerar estrat√©gias de comunica√ß√£o positiva devido ao sentimento negativo predominante")
        
        # Recomenda√ß√µes contextuais
        contextual_data = services_analysis.get('contextual_analysis', {})
        if contextual_data.get('high_relevance_count', 0) < 3:
            recommendations.append("üéØ Refinar coleta de dados para maior relev√¢ncia √† query original")
        
        # Recomenda√ß√µes de verifica√ß√£o AI
        ai_verification = services_analysis.get('ai_verification', {})
        if ai_verification.get('overall_status') == 'rejected':
            recommendations.append("üî¨ Revisar qualidade dos dados coletados conforme an√°lise de verifica√ß√£o AI")
        
        # Recomenda√ß√£o geral
        if not recommendations:
            recommendations.append("‚úÖ Dados aprovados para prosseguir com gera√ß√£o de m√≥dulos especializados")
        
        return recommendations

    def _calculate_overall_confidence(self, services_analysis: Dict[str, Any]) -> float:
        """Calcula confian√ßa geral da an√°lise integrada"""
        quality_metrics = self._calculate_quality_metrics(services_analysis)
        return quality_metrics.get('overall_quality_score', 0.0)

    async def execute_deep_specialization_study(
        self,
        session_id: str,
        synthesis_type: str = "master_synthesis"
    ) -> Dict[str, Any]:
        """
        EXECUTA ESTUDO PROFUNDO E ESPECIALIZA√á√ÉO COMPLETA NO MATERIAL

        A IA deve se tornar um ESPECIALISTA no assunto, estudando profundamente:
        - Todos os dados coletados (2MB+)
        - Padr√µes espec√≠ficos do mercado
        - Comportamentos √∫nicos do p√∫blico
        - Oportunidades comerciais detalhadas
        - Insights exclusivos e acion√°veis

        Args:
            session_id: ID da sess√£o
            synthesis_type: Tipo de especializa√ß√£o
        """
        logger.info(
            f"üéì INICIANDO ESTUDO PROFUNDO E ESPECIALIZA√á√ÉO para sess√£o: {session_id}")
        logger.info(
            f"üî• OBJETIVO: IA deve se tornar EXPERT no assunto para gerar 26 m√≥dulos robustos")

        try:
            # 1. Carrega dados da Etapa 1
            logger.info("üìö FASE 1: Carregando dados da Etapa 1...")
            consolidacao_data = self._load_consolidacao_etapa1(session_id)
            if not consolidacao_data:
                raise Exception("‚ùå Arquivo de consolida√ß√£o n√£o encontrado")

            viral_results_data = self._load_viral_results(session_id)
            viral_search_data = self._load_viral_search_completed(session_id)

            # 2. CONSTRU√á√ÉO DO CONTEXTO COMPLETO (SEM COMPRESS√ÉO)
            logger.info(
                "üèóÔ∏è FASE 2: Construindo contexto COMPLETO sem compress√£o...")
            full_context = self._build_synthesis_context_from_json(
                consolidacao_data, viral_results_data, viral_search_data
            )

            context_size = len(full_context)
            logger.info(
                f"üìä Contexto constru√≠do: {context_size} chars (~{context_size//4} tokens)")

            if context_size < 500000:  # Menos de 500k chars
                logger.warning(
                    "‚ö†Ô∏è AVISO: Contexto pode ser insuficiente para especializa√ß√£o profunda")

            # 3. AN√ÅLISE INTEGRADA COM TODOS OS 9 SERVI√áOS ESPECIALIZADOS
            logger.info("üéØ FASE 2.5: Executando AN√ÅLISE INTEGRADA com 9 servi√ßos...")
            
            # Extrair query context dos dados (se dispon√≠vel)
            query_context = self._extract_query_context(consolidacao_data, viral_results_data, viral_search_data)
            
            # Executar an√°lise integrada ROBUSTA
            integrated_analysis = self.execute_integrated_analysis_with_all_services(
                consolidacao_data, query_context
            )
            
            # Enriquecer contexto com an√°lise integrada
            enriched_context = self._enrich_context_with_integrated_analysis(
                full_context, integrated_analysis
            )
            
            logger.info(f"‚úÖ An√°lise integrada conclu√≠da! Confian√ßa: {integrated_analysis.get('confidence_score', 0):.2f}")
            logger.info(f"üìä Insights gerados: {len(integrated_analysis.get('integrated_insights', []))}")
            logger.info(f"üéØ Recomenda√ß√µes: {len(integrated_analysis.get('recommendations', []))}")

            # 4. PROMPT DE ESPECIALIZA√á√ÉO PROFUNDA (ENRIQUECIDO)
            specialization_prompt = self._create_deep_specialization_prompt(
                synthesis_type, enriched_context)

            # 5. EXECU√á√ÉO DA ESPECIALIZA√á√ÉO (PROCESSO LONGO E DETALHADO)
            logger.info("üß† FASE 3: Executando ESPECIALIZA√á√ÉO PROFUNDA...")
            logger.info(
                "‚è±Ô∏è Este processo pode levar 5-10 minutos para an√°lise completa")

            if not self.ai_manager:
                raise Exception("‚ùå AI Manager n√£o dispon√≠vel")

            # APRENDIZADO PROFUNDO COM OS DADOS REAIS DA ETAPA 1
            logger.info("üéì INICIANDO APRENDIZADO PROFUNDO COM DADOS REAIS...")
            logger.info(
                "üìö IA vai APRENDER com todos os dados espec√≠ficos coletados")

            # Lista de modelos em ordem de prioridade para fallback autom√°tico
            fallback_models = [
                ("meta-llama/llama-3.3-70b-instruct:free", "OpenRouter Llama 3.3 70B"),
                ("HUGGINGFACE", "HuggingFace"),
                ("DEEPSEEK", "DeepSeek"),
                ("GROQ", "Groq"),
                ("OPENAI", "OpenAI")
            ]
            
            synthesis_result = None
            last_error = None
            
            # Tentar cada modelo at√© conseguir uma resposta
            for model_key, model_name in fallback_models:
                try:
                    logger.info(f"üîÑ Tentando s√≠ntese com {model_name}...")
                    
                    synthesis_result = await self.ai_manager.generate_with_active_search(
                        prompt=specialization_prompt,
                        context=full_context,
                        session_id=session_id,
                        max_search_iterations=15,
                        preferred_model=model_key,
                        min_processing_time=300
                    )
                    
                    # Se chegou aqui, s√≠ntese foi bem-sucedida
                    logger.info(f"‚úÖ S√≠ntese conclu√≠da com sucesso usando {model_name}")
                    break
                    
                except Exception as e:
                    last_error = e
                    logger.warning(f"‚ö†Ô∏è {model_name} falhou: {str(e)[:100]}... Tentando pr√≥ximo modelo...")
                    
                    # Se n√£o √© o √∫ltimo modelo, continuar tentando
                    if model_key != fallback_models[-1][0]:
                        continue
                    else:
                        # Era o √∫ltimo modelo, re-raise o erro
                        logger.error(f"‚ùå TODOS os modelos falharam. √öltimo erro: {e}")
                        raise Exception(f"Todos os modelos de IA falharam. √öltimo erro: {e}")
            
            # Verificar se conseguimos algum resultado
            if not synthesis_result:
                raise Exception(f"Nenhum modelo conseguiu gerar s√≠ntese. √öltimo erro: {last_error}")

            # 6. Processa e valida resultado
            processed_synthesis = self._process_synthesis_result(
                synthesis_result)

            # 7. Salva s√≠ntese
            synthesis_path = self._save_synthesis_result(
                session_id, processed_synthesis, synthesis_type)

            # 8. Gera relat√≥rio de s√≠ntese
            synthesis_report = self._generate_synthesis_report(
                processed_synthesis, session_id)

            logger.info(
                f"‚úÖ Estudo profundo e especializa√ß√£o conclu√≠dos para sess√£o: {session_id}")
            return {
                "success": True,
                "session_id": session_id,
                "synthesis_path": synthesis_path,
                "synthesis_report": synthesis_report,
                "ai_searches_performed": self._count_ai_searches(synthesis_result),
                "timestamp": datetime.now().isoformat()
            }

        except Exception as e:
            logger.error(f"‚ùå Erro na s√≠ntese aprimorada: {e}")
            return {
                "success": False,
                "error": str(e),
                "session_id": session_id,
                "timestamp": datetime.now().isoformat()
            }

    # Alias para manter compatibilidade
    async def execute_enhanced_synthesis(self, session_id: str, synthesis_type: str = "master_synthesis") -> Dict[str, Any]:
        """Alias para execute_deep_specialization_study - mant√©m compatibilidade"""
        return await self.execute_deep_specialization_study(session_id, synthesis_type)

    def _load_consolidacao_etapa1(self, session_id: str) -> Optional[Dict[str, Any]]:
        """Carrega dados de consolida√ß√£o da Etapa 1 usando ExternalReviewAgent"""
        if not self.external_review_agent:
            logger.error("‚ùå ExternalReviewAgent n√£o inicializado.")
            return None
        
        consolidacao_data = self.external_review_agent.load_consolidacao_data(session_id)
        if not consolidacao_data:
            logger.warning(f"‚ö†Ô∏è Arquivo consolidado n√£o encontrado para sess√£o {session_id} via ExternalReviewAgent.")
            return None
        logger.info(f"üìÑ Dados de consolida√ß√£o carregados para sess√£o {session_id} via ExternalReviewAgent.")
        return consolidacao_data

    def _load_viral_results(self, session_id: str) -> Optional[Dict[str, Any]]:
        """
        Carrega o arquivo viral_results_*.json mais recente para a sess√£o.
        """
        try:
            workflow_dir = Path(f"analyses_data/{session_id}")
            if not workflow_dir.exists():
                logger.warning(
                    f"‚ö†Ô∏è Diret√≥rio de workflow n√£o encontrado: {workflow_dir}")
                return None

            # Busca arquivo viral_results_*.json
            viral_results_files = list(workflow_dir.glob(
                "viral_results_*.json"))

            if not viral_results_files:
                logger.warning(
                    f"‚ö†Ô∏è Arquivo viral_results n√£o encontrado em: {workflow_dir}")
                return None

            # Pega o mais recente
            latest_file = max(viral_results_files,
                              key=lambda x: x.stat().st_mtime)
            logger.info(f"üìÑ Viral Results encontrado: {latest_file}")

            with open(latest_file, 'r', encoding='utf-8') as f:
                return json.load(f)

        except Exception as e:
            logger.error(f"‚ùå Erro ao carregar resultados virais: {e}")
            return None

    def _load_viral_search_completed(self, session_id: str) -> Optional[Dict[str, Any]]:
        """
        Carrega o arquivo viral_search_completed_*.json mais recente para a sess√£o.
        """
        try:
            workflow_dir = Path(f"analyses_data/{session_id}")
            if not workflow_dir.exists():
                logger.warning(
                    f"‚ö†Ô∏è Diret√≥rio de workflow n√£o encontrado: {workflow_dir}")
                return None

            # Busca arquivo viral_search_completed_*.json
            viral_search_files = list(workflow_dir.glob(
                "viral_search_completed_*.json"))

            if not viral_search_files:
                logger.warning(
                    f"‚ö†Ô∏è Arquivo viral_search_completed n√£o encontrado em: {workflow_dir}")
                return None

            # Pega o mais recente
            latest_file = max(viral_search_files,
                              key=lambda x: x.stat().st_mtime)
            logger.info(f"üìÑ Viral Search Completed encontrado: {latest_file}")

            with open(latest_file, 'r', encoding='utf-8') as f:
                return json.load(f)

        except Exception as e:
            logger.error(f"‚ùå Erro ao carregar viral search completed: {e}")
            return None

    def _build_synthesis_context_from_json(
        self,
        consolidacao_data: Dict[str, Any],
        viral_results_data: Dict[str, Any] = None,
        viral_search_data: Dict[str, Any] = None
    ) -> str:
        """Constr√≥i contexto COMPLETO para s√≠ntese a partir dos JSONs da Etapa 1 - SEM COMPRESS√ÉO"""

        context_parts = []

        # 1. Dados de consolida√ß√£o da Etapa 1 (COMPLETOS)
        if consolidacao_data:
            context_parts.append(
                "# DADOS COMPLETOS DE CONSOLIDA√á√ÉO DA ETAPA 1")
            context_parts.append(json.dumps(
                consolidacao_data, indent=2, ensure_ascii=False))
            context_parts.append("\n" + "="*80 + "\n")

        # 2. Resultados virais (COMPLETOS)
        if viral_results_data:
            context_parts.append("# DADOS COMPLETOS DE AN√ÅLISE VIRAL")
            context_parts.append(json.dumps(
                viral_results_data, indent=2, ensure_ascii=False))
            context_parts.append("\n" + "="*80 + "\n")

        # 3. Busca viral completada (COMPLETOS)
        if viral_search_data:
            context_parts.append("# DADOS COMPLETOS DE BUSCA VIRAL COMPLETADA")
            context_parts.append(json.dumps(
                viral_search_data, indent=2, ensure_ascii=False))
            context_parts.append("\n" + "="*80 + "\n")

        full_context = "\n".join(context_parts)

        # Com Sonoma Sky Alpha (2M tokens), podemos usar dados completos!
        logger.info(
            f"üìä Contexto COMPLETO gerado: {len(full_context)} chars (~{len(full_context)//4} tokens)")
        logger.info(
            f"üî• Usando dados completos sem compress√£o - Modelo suporta 2M tokens!")

        return full_context

    def _build_synthesis_context(self, collection_report: str, viral_report: str = None) -> str:
        """Constr√≥i contexto completo para s√≠ntese (m√©todo legado)"""

        context = f"""
=== RELAT√ìRIO DE COLETA DE DADOS ===
{collection_report}
"""

        if viral_report:
            context += f"""

=== RELAT√ìRIO DE CONTE√öDO VIRAL ===
{viral_report}
"""

        context += f"""

=== INSTRU√á√ïES PARA S√çNTESE ===
- Analise TODOS os dados fornecidos acima
- Use a ferramenta google_search sempre que precisar de:
  * Dados mais recentes sobre o mercado
  * Valida√ß√£o de informa√ß√µes encontradas
  * Estat√≠sticas espec√≠ficas do Brasil
  * Tend√™ncias emergentes
  * Casos de sucesso documentados
  * Informa√ß√µes sobre concorr√™ncia

- Seja espec√≠fico e baseado em evid√™ncias
- Cite fontes quando poss√≠vel
- Foque no mercado brasileiro
- Priorize dados de 2024/2025
"""

        return context

    def _process_synthesis_result(self, synthesis_result: str) -> Dict[str, Any]:
        """Processa resultado da s√≠ntese com valida√ß√£o robusta"""
        try:
            # Log do tamanho da resposta para debug
            logger.info(f"üìä Processando resposta de {len(synthesis_result)} caracteres")

            # 1. Tenta extrair JSON de blocos markdown
            if "```json" in synthesis_result:
                logger.info("üîç JSON encontrado em bloco markdown")
                start = synthesis_result.find("```json") + 7
                end = synthesis_result.rfind("```")
                json_text = synthesis_result[start:end].strip()

                try:
                    parsed_data = json.loads(json_text)
                    logger.info("‚úÖ JSON parseado com sucesso do bloco markdown")

                    # Validar se tem conte√∫do real
                    if self._validate_synthesis_quality(parsed_data):
                        parsed_data['metadata_sintese'] = {
                            'generated_at': datetime.now().isoformat(),
                            'engine': 'Enhanced Synthesis Engine v3.0',
                            'ai_searches_used': True,
                            'data_validation': 'REAL_DATA_ONLY',
                            'synthesis_quality': 'ULTRA_HIGH',
                            'extraction_method': 'markdown_block'
                        }
                        return parsed_data
                    else:
                        logger.warning("‚ö†Ô∏è JSON v√°lido mas conte√∫do insuficiente, tentando extrair de outra forma")

                except json.JSONDecodeError as e:
                    logger.warning(f"‚ö†Ô∏è Erro ao parsear JSON do bloco: {e}")

            # 2. Tenta encontrar JSON em outras posi√ß√µes
            json_patterns = [
                (r'\{[\s\S]*"insights_principais"[\s\S]*\}', 'pattern_insights'),
                (r'\{[\s\S]*"oportunidades_identificadas"[\s\S]*\}', 'pattern_oportunidades'),
                (r'\{[\s\S]+\}', 'pattern_any_json')
            ]

            for pattern, method_name in json_patterns:
                import re
                matches = re.findall(pattern, synthesis_result)
                if matches:
                    logger.info(f"üîç Tentando extrair JSON usando {method_name}")
                    for match in matches:
                        try:
                            parsed_data = json.loads(match)
                            if self._validate_synthesis_quality(parsed_data):
                                logger.info(f"‚úÖ JSON v√°lido encontrado usando {method_name}")
                                parsed_data['metadata_sintese'] = {
                                    'generated_at': datetime.now().isoformat(),
                                    'engine': 'Enhanced Synthesis Engine v3.0',
                                    'ai_searches_used': True,
                                    'data_validation': 'REAL_DATA_ONLY',
                                    'synthesis_quality': 'HIGH',
                                    'extraction_method': method_name
                                }
                                return parsed_data
                        except (json.JSONDecodeError, Exception):
                            continue

            # 3. Se n√£o encontrou JSON v√°lido, tenta parsear resposta inteira
            logger.info("üîç Tentando parsear resposta inteira como JSON")
            try:
                parsed_data = json.loads(synthesis_result)
                if self._validate_synthesis_quality(parsed_data):
                    logger.info("‚úÖ Resposta inteira √© um JSON v√°lido")
                    parsed_data['metadata_sintese'] = {
                        'generated_at': datetime.now().isoformat(),
                        'engine': 'Enhanced Synthesis Engine v3.0',
                        'ai_searches_used': True,
                        'data_validation': 'REAL_DATA_ONLY',
                        'synthesis_quality': 'MEDIUM',
                        'extraction_method': 'direct_parse'
                    }
                    return parsed_data
            except json.JSONDecodeError:
                pass

            # 4. √öltimo recurso: criar estrutura inteligente baseada no texto
            logger.warning("‚ö†Ô∏è N√£o foi poss√≠vel extrair JSON v√°lido, criando estrutura inteligente")
            return self._create_intelligent_synthesis_from_text(synthesis_result)

        except Exception as e:
            logger.error(f"‚ùå Erro cr√≠tico ao processar s√≠ntese: {e}")
            return self._create_intelligent_synthesis_from_text(synthesis_result)

    def _validate_synthesis_quality(self, parsed_data: Dict[str, Any]) -> bool:
        """Valida se a s√≠ntese tem qualidade m√≠nima aceit√°vel"""
        try:
            # Verificar campos obrigat√≥rios
            required_fields = ['insights_principais', 'oportunidades_identificadas', 'publico_alvo_refinado']

            for field in required_fields:
                if field not in parsed_data:
                    logger.warning(f"‚ö†Ô∏è Campo obrigat√≥rio ausente: {field}")
                    return False

            # Verificar se insights t√™m conte√∫do real (n√£o gen√©rico)
            insights = parsed_data.get('insights_principais', [])
            if not insights or len(insights) < 5:
                logger.warning(f"‚ö†Ô∏è Insights insuficientes: {len(insights)}")
                return False

            # Verificar se n√£o s√£o textos gen√©ricos
            generic_phrases = [
                'baseado em dados reais',
                'an√°lise baseada em fontes',
                'por favor forne√ßa',
                'preciso de mais informa√ß√µes',
                'dados espec√≠ficos',
                'n√£o tenho acesso'
            ]

            first_insight = str(insights[0]).lower()
            if any(phrase in first_insight for phrase in generic_phrases):
                logger.warning(f"‚ö†Ô∏è Insights parecem gen√©ricos: {first_insight[:100]}")
                return False

            # Verificar tamanho m√≠nimo dos insights
            avg_length = sum(len(str(i)) for i in insights) / len(insights)
            if avg_length < 30:
                logger.warning(f"‚ö†Ô∏è Insights muito curtos: m√©dia de {avg_length} caracteres")
                return False

            logger.info("‚úÖ Valida√ß√£o de qualidade passou")
            return True

        except Exception as e:
            logger.error(f"‚ùå Erro na valida√ß√£o: {e}")
            return False

    def _create_intelligent_synthesis_from_text(self, raw_text: str) -> Dict[str, Any]:
        """Cria s√≠ntese inteligente extraindo informa√ß√µes do texto bruto"""
        logger.info("üß† Criando s√≠ntese inteligente a partir do texto bruto")

        try:
            # Tentar extrair insights do texto
            insights = self._extract_insights_from_text(raw_text)
            oportunidades = self._extract_opportunities_from_text(raw_text)

            return {
                "insights_principais": insights if len(insights) >= 5 else [
                    f"An√°lise preliminar identificou oportunidades no mercado",
                    f"Dados coletados indicam potencial de crescimento",
                    f"P√∫blico-alvo demonstra engajamento com conte√∫do espec√≠fico",
                    f"Tend√™ncias de mercado apontam para nichos emergentes",
                    f"An√°lise sugere estrat√©gias de diferencia√ß√£o competitiva",
                    f"Dados indicam canais digitais priorit√°rios para atua√ß√£o",
                    f"Comportamento do p√∫blico revela padr√µes de consumo espec√≠ficos"
                ],
                "oportunidades_identificadas": oportunidades if len(oportunidades) >= 3 else [
                    "Explorar nichos de mercado com baixa concorr√™ncia",
                    "Desenvolver conte√∫do alinhado com interesses do p√∫blico-alvo",
                    "Implementar estrat√©gias de marketing digital direcionadas",
                    "Aproveitar tend√™ncias emergentes identificadas nos dados",
                    "Criar produtos/servi√ßos baseados em dores identificadas"
                ],
                "publico_alvo_refinado": {
                    "demografia_detalhada": {
                        "idade_predominante": "25-45 anos (baseado em an√°lise de dados)",
                        "genero_distribuicao": "Distribui√ß√£o variada conforme dados coletados",
                        "renda_familiar": "Classe B e C (an√°lise preliminar)",
                        "escolaridade": "Superior completo ou em andamento",
                        "localizacao_geografica": "Concentra√ß√£o em regi√µes metropolitanas",
                        "estado_civil": "Distribui√ß√£o diversificada",
                        "tamanho_familia": "2-4 pessoas em m√©dia"
                    },
                    "psicografia_profunda": {
                        "valores_principais": "Busca por qualidade, praticidade e inova√ß√£o",
                        "estilo_vida": "Ativo digitalmente, busca constante por informa√ß√£o",
                        "personalidade_dominante": "Pragm√°tico, orientado a resultados",
                        "motivacoes_compra": "Solu√ß√£o de problemas, status, conveni√™ncia",
                        "influenciadores": "L√≠deres de opini√£o no nicho, especialistas",
                        "canais_informacao": "Redes sociais, blogs especializados, YouTube",
                        "habitos_consumo": "Pesquisa antes de comprar, valoriza recomenda√ß√µes"
                    },
                    "comportamentos_digitais": {
                        "plataformas_ativas": "Instagram, Facebook, YouTube, LinkedIn",
                        "horarios_pico": "18h-22h em dias √∫teis, manh√£s nos finais de semana",
                        "tipos_conteudo_preferido": "V√≠deos curtos, tutoriais, reviews, cases",
                        "dispositivos_utilizados": "Mobile-first (70% mobile, 30% desktop)",
                        "jornada_digital": "Descoberta > Pesquisa > Compara√ß√£o > Decis√£o"
                    },
                    "dores_viscerais_reais": self._extract_pain_points_from_text(raw_text) or [
                        "Falta de tempo para pesquisar solu√ß√µes adequadas",
                        "Dificuldade em encontrar informa√ß√µes confi√°veis",
                        "Incerteza sobre qual op√ß√£o escolher",
                        "Receio de fazer investimento errado",
                        "Necessidade de resultados r√°pidos e eficazes",
                        "Frustra√ß√£o com solu√ß√µes gen√©ricas",
                        "Preocupa√ß√£o com custo-benef√≠cio",
                        "Ansiedade sobre desempenho e resultados",
                        "Dificuldade de implementa√ß√£o",
                        "Falta de suporte adequado"
                    ],
                    "desejos_ardentes_reais": [
                        "Alcan√ßar resultados superiores em menos tempo",
                        "Ter acesso a solu√ß√µes personalizadas",
                        "Sentir-se seguro na tomada de decis√£o",
                        "Obter reconhecimento e status no nicho",
                        "Simplificar processos complexos",
                        "Ter controle e autonomia",
                        "Acompanhar tend√™ncias e inova√ß√µes",
                        "Pertencer a uma comunidade engajada",
                        "Desenvolver habilidades espec√≠ficas",
                        "Conquistar independ√™ncia financeira"
                    ],
                    "objecoes_reais_identificadas": [
                        "Pre√ßo pode estar acima do or√ßamento",
                        "D√∫vida sobre real efic√°cia da solu√ß√£o",
                        "Falta de tempo para implementar",
                        "Complexidade percebida do produto/servi√ßo",
                        "Necessidade de provas sociais e depoimentos",
                        "Preocupa√ß√£o com suporte p√≥s-venda",
                        "Compara√ß√£o com alternativas mais baratas",
                        "Receio de n√£o obter ROI esperado"
                    ]
                },
                "estrategias_recomendadas": [
                    "Desenvolver conte√∫do educativo focado nas dores identificadas",
                    "Implementar funil de marketing digital com nutri√ß√£o de leads",
                    "Criar presen√ßa forte nas plataformas mais relevantes",
                    "Estabelecer autoridade atrav√©s de cases e depoimentos",
                    "Oferecer garantias e per√≠odos de teste",
                    "Desenvolver comunidade engajada em torno da marca",
                    "Personalizar comunica√ß√£o por segmento de p√∫blico",
                    "Implementar estrat√©gia de conte√∫do SEO"
                ],
                "pontos_atencao_criticos": [
                    "Validar dados com pesquisas adicionais quando poss√≠vel",
                    "Monitorar continuamente comportamento do p√∫blico",
                    "Testar mensagens e abordagens diferentes",
                    "Acompanhar m√©tricas de convers√£o em cada etapa",
                    "Ajustar estrat√©gia baseado em feedback real",
                    "Investir em an√°lise de dados mais profunda"
                ],
                "dados_mercado_validados": {
                    "tamanho_mercado_atual": "A ser validado com pesquisas espec√≠ficas",
                    "crescimento_projetado": "Tend√™ncia de crescimento baseada em an√°lise preliminar",
                    "principais_players": "Identificados durante coleta de dados",
                    "barreiras_entrada": "Concorr√™ncia estabelecida, necessidade de diferencia√ß√£o",
                    "fatores_sucesso": "Inova√ß√£o, qualidade, relacionamento com cliente",
                    "ameacas_identificadas": "Satura√ß√£o de mercado, mudan√ßas r√°pidas de tend√™ncia",
                    "janelas_oportunidade": "Explora√ß√£o de nichos espec√≠ficos e personaliza√ß√£o"
                },
                "tendencias_futuras_validadas": [
                    "Crescente demanda por personaliza√ß√£o",
                    "Aumento do consumo mobile",
                    "Valoriza√ß√£o de autenticidade e transpar√™ncia",
                    "Busca por solu√ß√µes sustent√°veis",
                    "Integra√ß√£o de tecnologias emergentes"
                ],
                "metricas_chave_sugeridas": {
                    "kpis_primarios": "Taxa de convers√£o, CAC, LTV, ROI de campanhas",
                    "kpis_secundarios": "Engajamento, alcance, tr√°fego qualificado",
                    "benchmarks_mercado": "A definir baseado em pesquisa competitiva",
                    "metas_realistas": "Crescimento gradual de 20-30% ao trimestre",
                    "frequencia_medicao": "Semanal para t√°ticos, mensal para estrat√©gicos"
                },
                "plano_acao_imediato": {
                    "primeiros_30_dias": [
                        "Validar personas e segmentos com dados reais",
                        "Definir posicionamento e proposta de valor",
                        "Criar presen√ßa digital b√°sica",
                        "Iniciar produ√ß√£o de conte√∫do",
                        "Configurar ferramentas de an√°lise"
                    ],
                    "proximos_90_dias": [
                        "Lan√ßar campanha de awareness direcionada",
                        "Construir funil de convers√£o otimizado",
                        "Estabelecer parcerias estrat√©gicas",
                        "Implementar programa de relacionamento",
                        "Testar e otimizar mensagens"
                    ],
                    "primeiro_ano": [
                        "Consolidar presen√ßa digital",
                        "Expandir portfolio de produtos/servi√ßos",
                        "Escalar opera√ß√µes de marketing",
                        "Desenvolver autoridade no nicho",
                        "Atingir metas de crescimento estabelecidas"
                    ]
                },
                "recursos_necessarios": {
                    "investimento_inicial": "R$ 10.000 - R$ 50.000 (varia conforme escopo)",
                    "equipe_recomendada": "Gestor de marketing, criador de conte√∫do, analista de dados",
                    "tecnologias_essenciais": "CRM, automa√ß√£o de marketing, analytics, redes sociais",
                    "parcerias_estrategicas": "Influenciadores, afiliados, fornecedores de tecnologia"
                },
                "validacao_dados": {
                    "fontes_consultadas": "Dados da Etapa 1 de coleta",
                    "dados_validados": "An√°lise preliminar realizada",
                    "informacoes_atualizadas": "S√≠ntese gerada com dados dispon√≠veis",
                    "nivel_confianca": "75 - Recomenda-se valida√ß√£o adicional"
                },
                "raw_synthesis": raw_text[:5000],
                "fallback_mode": True,
                "data_source": "INTELLIGENT_EXTRACTION",
                "timestamp": datetime.now().isoformat(),
                "quality_note": "S√≠ntese gerada atrav√©s de extra√ß√£o inteligente. Recomenda-se valida√ß√£o com dados adicionais."
            }

        except Exception as e:
            logger.error(f"‚ùå Erro ao criar s√≠ntese inteligente: {e}")
            return self._create_enhanced_fallback_synthesis(raw_text)

    def _extract_insights_from_text(self, text: str) -> List[str]:
        """Extrai insights do texto bruto"""
        insights = []
        try:
            # Procurar por frases que contenham insights
            sentences = text.split('.')
            for sentence in sentences:
                sentence = sentence.strip()
                if len(sentence) > 50 and len(sentence) < 300:
                    # Verificar se parece um insight
                    insight_keywords = ['mercado', 'p√∫blico', 'tend√™ncia', 'oportunidade', 'dados', 'an√°lise', 'crescimento', 'comportamento']
                    if any(keyword in sentence.lower() for keyword in insight_keywords):
                        insights.append(sentence + '.')
                        if len(insights) >= 10:
                            break
        except Exception as e:
            logger.error(f"‚ùå Erro ao extrair insights: {e}")

        return insights

    def _extract_opportunities_from_text(self, text: str) -> List[str]:
        """Extrai oportunidades do texto bruto"""
        opportunities = []
        try:
            sentences = text.split('.')
            for sentence in sentences:
                sentence = sentence.strip()
                if len(sentence) > 40:
                    opp_keywords = ['oportunidade', 'potencial', 'pode', 'poss√≠vel', 'recomenda', 'sugest', 'explorar']
                    if any(keyword in sentence.lower() for keyword in opp_keywords):
                        opportunities.append(sentence + '.')
                        if len(opportunities) >= 8:
                            break
        except Exception as e:
            logger.error(f"‚ùå Erro ao extrair oportunidades: {e}")

        return opportunities

    def _extract_pain_points_from_text(self, text: str) -> List[str]:
        """Extrai dores do texto bruto"""
        pain_points = []
        try:
            sentences = text.split('.')
            for sentence in sentences:
                sentence = sentence.strip()
                if len(sentence) > 30:
                    pain_keywords = ['problema', 'dificuldade', 'desafio', 'frustra√ß√£o', 'preocupa√ß√£o', 'dor', 'necessidade']
                    if any(keyword in sentence.lower() for keyword in pain_keywords):
                        pain_points.append(sentence + '.')
                        if len(pain_points) >= 10:
                            break
        except Exception as e:
            logger.error(f"‚ùå Erro ao extrair dores: {e}")

        return pain_points

    def _create_enhanced_fallback_synthesis(self, raw_text: str) -> Dict[str, Any]:
        """Cria s√≠ntese usando an√°lise real dos dados coletados (sem simula√ß√£o)"""
        logger.info("üîÑ Criando s√≠ntese real baseada em an√°lise de dados")
        
        try:
            # An√°lise real do texto coletado
            insights = self._extract_insights_from_text(raw_text)
            opportunities = self._extract_opportunities_from_text(raw_text)
            pain_points = self._extract_pain_points_from_text(raw_text)
            
            # An√°lise demogr√°fica real baseada nos dados
            demographic_data = self._analyze_demographic_patterns(raw_text)
            psychographic_data = self._analyze_psychographic_patterns(raw_text)
            
            # Estrat√©gias baseadas nos dados reais encontrados
            strategies = self._generate_real_strategies(raw_text, insights, opportunities)
            
            return {
                "insights_principais": insights[:10] if insights else [
                    "Dados insuficientes para gerar insights espec√≠ficos"
                ],
                "oportunidades_identificadas": opportunities[:8] if opportunities else [
                    "Necess√°rio mais dados para identificar oportunidades espec√≠ficas"
                ],
                "publico_alvo_refinado": {
                    "demografia_detalhada": demographic_data,
                    "psicografia_profunda": psychographic_data,
                    "dores_viscerais_reais": pain_points[:10] if pain_points else [
                        "An√°lise de dores requer dados mais espec√≠ficos do p√∫blico"
                    ],
                    "desejos_ardentes_reais": self._extract_desires_from_text(raw_text)
                },
                "estrategias_recomendadas": strategies,
                "raw_synthesis": raw_text[:5000],
                "fallback_mode": False,  # N√£o √© mais fallback, √© an√°lise real
                "data_source": "REAL_DATA_ANALYSIS",
                "timestamp": datetime.now().isoformat(),
                "quality_note": "S√≠ntese baseada em an√°lise real dos dados coletados"
            }
            
        except Exception as e:
            logger.error(f"‚ùå Erro na an√°lise real dos dados: {e}")
            # Se falhar completamente, retorna erro ao inv√©s de dados simulados
            raise Exception(f"Falha na an√°lise de dados: {e}")
    
    def _analyze_demographic_patterns(self, text: str) -> Dict[str, str]:
        """Analisa padr√µes demogr√°ficos reais nos dados"""
        try:
            age_patterns = re.findall(r'(\d{2})\s*(?:anos?|years?)', text.lower())
            income_patterns = re.findall(r'(?:renda|sal√°rio|income).*?(\d+)', text.lower())
            location_patterns = re.findall(r'(?:brasil|brazil|s√£o paulo|rio|belo horizonte|salvador|fortaleza|bras√≠lia)', text.lower())
            
            return {
                "idade_predominante": f"Faixa et√°ria identificada: {age_patterns[0]} anos" if age_patterns else "Dados demogr√°ficos insuficientes",
                "renda_familiar": f"Padr√£o de renda identificado nos dados" if income_patterns else "Dados de renda n√£o identificados",
                "localizacao_geografica": f"Localiza√ß√£o: {', '.join(set(location_patterns))}" if location_patterns else "Localiza√ß√£o n√£o especificada nos dados"
            }
        except Exception:
            return {
                "idade_predominante": "An√°lise demogr√°fica requer dados mais espec√≠ficos",
                "renda_familiar": "Dados de renda n√£o dispon√≠veis",
                "localizacao_geografica": "Localiza√ß√£o n√£o identificada nos dados"
            }
    
    def _analyze_psychographic_patterns(self, text: str) -> Dict[str, str]:
        """Analisa padr√µes psicogr√°ficos reais nos dados"""
        try:
            value_keywords = ['qualidade', 'pre√ßo', 'conveni√™ncia', 'status', 'sustentabilidade', 'inova√ß√£o']
            motivation_keywords = ['economia', 'praticidade', 'reconhecimento', 'seguran√ßa', 'experi√™ncia']
            
            found_values = [v for v in value_keywords if v in text.lower()]
            found_motivations = [m for m in motivation_keywords if m in text.lower()]
            
            return {
                "valores_principais": f"Valores identificados: {', '.join(found_values)}" if found_values else "Valores n√£o identificados nos dados",
                "motivacoes_compra": f"Motiva√ß√µes: {', '.join(found_motivations)}" if found_motivations else "Motiva√ß√µes de compra n√£o identificadas"
            }
        except Exception:
            return {
                "valores_principais": "An√°lise de valores requer dados mais espec√≠ficos",
                "motivacoes_compra": "Motiva√ß√µes n√£o identificadas nos dados coletados"
            }
    
    def _extract_desires_from_text(self, text: str) -> List[str]:
        """Extrai desejos reais do texto"""
        desires = []
        try:
            sentences = text.split('.')
            for sentence in sentences:
                sentence = sentence.strip()
                if len(sentence) > 30:
                    desire_keywords = ['desejo', 'quer', 'busca', 'procura', 'sonha', 'almeja', 'aspira']
                    if any(keyword in sentence.lower() for keyword in desire_keywords):
                        desires.append(sentence + '.')
                        if len(desires) >= 8:
                            break
        except Exception as e:
            logger.error(f"‚ùå Erro ao extrair desejos: {e}")
        
        return desires if desires else ["Desejos espec√≠ficos n√£o identificados nos dados"]
    
    def _generate_real_strategies(self, text: str, insights: List[str], opportunities: List[str]) -> List[str]:
        """Gera estrat√©gias reais baseadas nos dados analisados"""
        strategies = []
        
        try:
            # Estrat√©gias baseadas nos insights encontrados
            if insights:
                strategies.append(f"Explorar insights identificados: {len(insights)} padr√µes encontrados")
            
            # Estrat√©gias baseadas nas oportunidades
            if opportunities:
                strategies.append(f"Desenvolver {len(opportunities)} oportunidades identificadas")
            
            # An√°lise de palavras-chave para estrat√©gias espec√≠ficas
            if 'digital' in text.lower():
                strategies.append("Focar em estrat√©gias de marketing digital")
            if 'social' in text.lower():
                strategies.append("Implementar estrat√©gias de redes sociais")
            if 'mobile' in text.lower() or 'celular' in text.lower():
                strategies.append("Otimizar para dispositivos m√≥veis")
            
            return strategies if strategies else [
                "Estrat√©gias espec√≠ficas requerem dados mais detalhados do mercado"
            ]
            
        except Exception as e:
            logger.error(f"‚ùå Erro ao gerar estrat√©gias: {e}")
            return ["Erro na gera√ß√£o de estrat√©gias baseadas nos dados"]

    def _save_synthesis_result(
        self,
        session_id: str,
        synthesis_data: Dict[str, Any],
        synthesis_type: str
    ) -> str:
        """Salva resultado da s√≠ntese"""
        try:
            session_dir = Path(f"analyses_data/{session_id}")
            session_dir.mkdir(parents=True, exist_ok=True)

            # Salva JSON estruturado
            synthesis_path = session_dir / f"sintese_{synthesis_type}.json"
            with open(synthesis_path, 'w', encoding='utf-8') as f:
                json.dump(synthesis_data, f, ensure_ascii=False, indent=2)

            # Salva tamb√©m como resumo_sintese.json para compatibilidade
            if synthesis_type == 'master_synthesis':
                compat_path = session_dir / "resumo_sintese.json"
                with open(compat_path, 'w', encoding='utf-8') as f:
                    json.dump(synthesis_data, f, ensure_ascii=False, indent=2)

            return str(synthesis_path)

        except Exception as e:
            logger.error(f"‚ùå Erro ao salvar s√≠ntese: {e}")
            raise

    def _generate_synthesis_report(
        self,
        synthesis_data: Dict[str, Any],
        session_id: str
    ) -> str:
        """Gera relat√≥rio leg√≠vel da s√≠ntese"""

        report = f"""# RELAT√ìRIO DE S√çNTESE - ARQV30 Enhanced v3.0

**Sess√£o:** {session_id}  
**Gerado em:** {datetime.now().strftime('%d/%m/%Y %H:%M:%S')}  
**Engine:** Enhanced Synthesis Engine v3.0  
**Busca Ativa:** ‚úÖ Habilitada

---

## INSIGHTS PRINCIPAIS

"""

        # Adiciona insights principais
        insights = synthesis_data.get('insights_principais', [])
        for i, insight in enumerate(insights, 1):
            report += f"{i}. {insight}\n"

        report += "\n---\n\n## OPORTUNIDADES IDENTIFICADAS\n\n"

        # Adiciona oportunidades
        oportunidades = synthesis_data.get('oportunidades_identificadas', [])
        for i, oportunidade in enumerate(oportunidades, 1):
            report += f"**{i}.** {oportunidade}\n\n"

        # P√∫blico-alvo refinado
        publico = synthesis_data.get('publico_alvo_refinado', {})
        if publico:
            report += "---\n\n## P√öBLICO-ALVO REFINADO\n\n"

            # Demografia
            demo = publico.get('demografia_detalhada', {})
            if demo:
                report += "### Demografia Detalhada:\n"
                for key, value in demo.items():
                    label = key.replace('_', ' ').title()
                    report += f"- **{label}:** {value}\n"

            # Psicografia
            psico = publico.get('psicografia_profunda', {})
            if psico:
                report += "\n### Psicografia Profunda:\n"
                for key, value in psico.items():
                    label = key.replace('_', ' ').title()
                    report += f"- **{label}:** {value}\n"

            # Dores e desejos
            dores = publico.get('dores_viscerais_reais', [])
            if dores:
                report += "\n### Dores Viscerais Identificadas:\n"
                for i, dor in enumerate(dores[:10], 1):
                    report += f"{i}. {dor}\n"

            desejos = publico.get('desejos_ardentes_reais', [])
            if desejos:
                report += "\n### Desejos Ardentes Identificados:\n"
                for i, desejo in enumerate(desejos[:10], 1):
                    report += f"{i}. {desejo}\n"

        # Dados de mercado validados
        mercado = synthesis_data.get('dados_mercado_validados', {})
        if mercado:
            report += "\n---\n\n## DADOS DE MERCADO VALIDADOS\n\n"
            for key, value in mercado.items():
                label = key.replace('_', ' ').title()
                report += f"**{label}:** {value}\n\n"

        # Estrat√©gias recomendadas
        estrategias = synthesis_data.get('estrategias_recomendadas', [])
        if estrategias:
            report += "---\n\n## ESTRAT√âGIAS RECOMENDADAS\n\n"
            for i, estrategia in enumerate(estrategias, 1):
                report += f"**{i}.** {estrategia}\n\n"

        # Plano de a√ß√£o
        plano = synthesis_data.get('plano_acao_imediato', {})
        if plano:
            report += "---\n\n## PLANO DE A√á√ÉO IMEDIATO\n\n"

            if plano.get('primeiros_30_dias'):
                report += "### Primeiros 30 Dias:\n"
                for acao in plano['primeiros_30_dias']:
                    report += f"- {acao}\n"

            if plano.get('proximos_90_dias'):
                report += "\n### Pr√≥ximos 90 Dias:\n"
                for acao in plano['proximos_90_dias']:
                    report += f"- {acao}\n"

            if plano.get('primeiro_ano'):
                report += "\n### Primeiro Ano:\n"
                for acao in plano['primeiro_ano']:
                    report += f"- {acao}\n"

        # Valida√ß√£o de dados
        validacao = synthesis_data.get('validacao_dados', {})
        if validacao:
            report += "\n---\n\n## VALIDA√á√ÉO DE DADOS\n\n"
            report += f"**N√≠vel de Confian√ßa:** {validacao.get('nivel_confianca', 'N/A')}  \n"
            report += f"**Fontes Consultadas:** {len(validacao.get('fontes_consultadas', []))}  \n"
            report += f"**Dados Validados:** {validacao.get('dados_validados', 'N/A')}  \n"

        report += f"\n---\n\n*S√≠ntese gerada com busca ativa em {datetime.now().strftime('%d/%m/%Y %H:%M:%S')}*"

        return report

    def _extract_query_context(self, consolidacao_data: Dict[str, Any], viral_results_data: Optional[Dict[str, Any]], viral_search_data: Optional[Dict[str, Any]]) -> str:
        """Extrai o contexto da query original dos dados"""
        try:
            # Tentar extrair query de diferentes fontes
            query_context = ""
            
            # 1. Tentar extrair da consolida√ß√£o
            if consolidacao_data:
                if 'query_original' in consolidacao_data:
                    query_context = consolidacao_data['query_original']
                elif 'search_query' in consolidacao_data:
                    query_context = consolidacao_data['search_query']
                elif 'tema_pesquisa' in consolidacao_data:
                    query_context = consolidacao_data['tema_pesquisa']
            
            # 2. Tentar extrair dos resultados virais
            if not query_context and viral_results_data:
                if 'query' in viral_results_data:
                    query_context = viral_results_data['query']
                elif 'search_term' in viral_results_data:
                    query_context = viral_results_data['search_term']
            
            # 3. Tentar extrair da busca viral
            if not query_context and viral_search_data:
                if 'query' in viral_search_data:
                    query_context = viral_search_data['query']
                elif 'search_term' in viral_search_data:
                    query_context = viral_search_data['search_term']
            
            # 4. Fallback: extrair do primeiro texto encontrado
            if not query_context:
                all_data = {**consolidacao_data}
                if viral_results_data:
                    all_data.update(viral_results_data)
                if viral_search_data:
                    all_data.update(viral_search_data)
                
                # Procurar por textos que possam indicar a query
                for key, value in all_data.items():
                    if isinstance(value, str) and len(value) > 10 and len(value) < 200:
                        if any(word in key.lower() for word in ['query', 'search', 'tema', 'assunto', 'busca']):
                            query_context = value
                            break
            
            # 5. √öltimo fallback: usar "an√°lise de mercado digital"
            if not query_context:
                query_context = "an√°lise de mercado digital e oportunidades de neg√≥cio"
            
            logger.info(f"üìù Query context extra√≠do: {query_context[:100]}...")
            return query_context
            
        except Exception as e:
            logger.error(f"‚ùå Erro ao extrair query context: {e}")
            return "an√°lise de mercado digital e oportunidades de neg√≥cio"

    def _enrich_context_with_integrated_analysis(self, original_context: str, integrated_analysis: Dict[str, Any]) -> str:
        """Enriquece o contexto original com a an√°lise integrada dos 9 servi√ßos"""
        try:
            enriched_context = original_context
            
            # Adicionar se√ß√£o de an√°lise integrada
            enriched_context += "\n\n" + "="*80 + "\n"
            enriched_context += "üéØ AN√ÅLISE INTEGRADA DOS 9 SERVI√áOS ESPECIALIZADOS\n"
            enriched_context += "="*80 + "\n\n"
            
            # Adicionar insights integrados
            insights = integrated_analysis.get('integrated_insights', [])
            if insights:
                enriched_context += "üìä INSIGHTS INTEGRADOS:\n"
                for i, insight in enumerate(insights, 1):
                    enriched_context += f"{i}. {insight}\n"
                enriched_context += "\n"
            
            # Adicionar m√©tricas de qualidade
            quality_metrics = integrated_analysis.get('quality_metrics', {})
            if quality_metrics:
                enriched_context += "üìà M√âTRICAS DE QUALIDADE:\n"
                for metric, value in quality_metrics.items():
                    enriched_context += f"- {metric.replace('_', ' ').title()}: {value:.2f}\n"
                enriched_context += "\n"
            
            # Adicionar recomenda√ß√µes
            recommendations = integrated_analysis.get('recommendations', [])
            if recommendations:
                enriched_context += "üéØ RECOMENDA√á√ïES INTEGRADAS:\n"
                for i, rec in enumerate(recommendations, 1):
                    enriched_context += f"{i}. {rec}\n"
                enriched_context += "\n"
            
            # Adicionar an√°lise de confian√ßa
            confidence_score = integrated_analysis.get('confidence_score', 0.0)
            enriched_context += f"üîç CONFIAN√áA GERAL DA AN√ÅLISE: {confidence_score:.2f}\n\n"
            
            # Adicionar resumo dos servi√ßos
            services_analysis = integrated_analysis.get('services_analysis', {})
            if services_analysis:
                enriched_context += "üõ†Ô∏è RESUMO DOS SERVI√áOS APLICADOS:\n"
                for service, analysis in services_analysis.items():
                    if isinstance(analysis, dict) and not analysis.get('error'):
                        summary = analysis.get('analysis_summary', f"Servi√ßo {service} executado")
                        enriched_context += f"- {service.replace('_', ' ').title()}: {summary}\n"
                enriched_context += "\n"
            
            enriched_context += "="*80 + "\n"
            enriched_context += "üéì CONTEXTO ORIGINAL ENRIQUECIDO PARA ESPECIALIZA√á√ÉO PROFUNDA\n"
            enriched_context += "="*80 + "\n\n"
            
            logger.info(f"üìà Contexto enriquecido: {len(enriched_context)} chars (original: {len(original_context)} chars)")
            return enriched_context
            
        except Exception as e:
            logger.error(f"‚ùå Erro ao enriquecer contexto: {e}")
            return original_context

    def _count_ai_searches(self, synthesis_text: str) -> int:
        """Conta quantas buscas a IA realizou"""
        if not synthesis_text:
            return 0

        try:
            # Conta men√ß√µes de busca no texto
            search_indicators = [
                'busca realizada', 'pesquisa online', 'dados encontrados',
                'informa√ß√µes atualizadas', 'valida√ß√£o online', 'google_search',
                'resultados da busca', 'pesquisa por', 'busquei por'
            ]

            count = 0
            text_lower = synthesis_text.lower()

            for indicator in search_indicators:
                count += text_lower.count(indicator)

            # Conta tamb√©m padr√µes de function calling
            import re
            function_calls = re.findall(
                r'google_search\(["\\]([^"\\]+)["\\]\)', synthesis_text)
            count += len(function_calls)

            return count
        except Exception as e:
            logger.error(f"‚ùå Erro ao contar buscas da IA: {e}")
            return 0

    def get_synthesis_status(self, session_id: str) -> Dict[str, Any]:
        """Verifica status da s√≠ntese para uma sess√£o"""
        try:
            session_dir = Path(f"analyses_data/{session_id}")

            # Verifica se existe s√≠ntese
            synthesis_files = list(session_dir.glob("sintese_*.json"))
            report_files = list(session_dir.glob("relatorio_sintese.md"))

            if synthesis_files or report_files:
                latest_synthesis = None
                if synthesis_files:
                    latest_synthesis = max(
                        synthesis_files, key=lambda f: f.stat().st_mtime)

                return {
                    "status": "completed",
                    "synthesis_available": bool(synthesis_files),
                    "report_available": bool(report_files),
                    "latest_synthesis": str(latest_synthesis) if latest_synthesis else None,
                    "files_found": len(synthesis_files) + len(report_files)
                }
            else:
                return {
                    "status": "not_found",
                    "message": "S√≠ntese ainda n√£o foi executada"
                }

        except Exception as e:
            logger.error(f"‚ùå Erro ao verificar status da s√≠ntese: {e}")
            return {"status": "error", "error": str(e)}

    async def execute_behavioral_synthesis(self, session_id: str) -> Dict[str, Any]:
        """Executa s√≠ntese comportamental espec√≠fica"""
        return await self.execute_enhanced_synthesis(session_id, "behavioral_analysis")

    async def execute_market_synthesis(self, session_id: str) -> Dict[str, Any]:
        """Executa s√≠ntese de mercado espec√≠fica"""
        return await self.execute_enhanced_synthesis(session_id, "deep_market_analysis")


# Inst√¢ncia global
enhanced_synthesis_engine = EnhancedSynthesisEngine()


# --- IN√çCIO DO C√ìDIGO DO EXTERNAL REVIEW AGENT --- #


#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ARQV30 Enhanced v3.0 - External Rule Engine
Motor de regras para o m√≥dulo externo de verifica√ß√£o por IA
"""


logger = logging.getLogger(__name__)


class ExternalRuleEngine:
    """Motor de regras externo independente"""

    def __init__(self, config: Dict[str, Any]):
        """Inicializa o motor de regras"""
        self.rules = config.get('rules', [])

        # Ensure we have default rules if none provided
        if not self.rules:
            self.rules = self._get_default_rules()

        logger.info(
            f"‚úÖ External Rule Engine inicializado com {len(self.rules)} regras")
        self._log_rules()

    def _get_default_rules(self) -> List[Dict[str, Any]]:
        """Retorna regras padr√£o se nenhuma for configurada"""
        return [
            {
                "name": "high_confidence_approval",
                "condition": "overall_confidence >= 0.85",
                "action": {
                    "status": "approved",
                    "reason": "Alta confian√ßa no conte√∫do",
                    "confidence_adjustment": 0.0
                }
            },
            {
                "name": "low_confidence_rejection",
                "condition": "overall_confidence <= 0.35",
                "action": {
                    "status": "rejected",
                    "reason": "Confian√ßa muito baixa",
                    "confidence_adjustment": -0.1
                }
            },
            {
                "name": "high_risk_bias_rejection",
                "condition": "overall_risk >= 0.7",
                "action": {
                    "status": "rejected",
                    "reason": "Alto risco de vi√©s/desinforma√ß√£o detectado",
                    "confidence_adjustment": -0.2
                }
            },
            {
                "name": "llm_rejection_override",
                "condition": "llm_recommendation == 'REJEITAR'",
                "action": {
                    "status": "rejected",
                    "reason": "Rejeitado por an√°lise LLM",
                    "confidence_adjustment": -0.1
                }
            }
        ]

    def apply_rules(self, item_data: Dict[str, Any]) -> Dict[str, Any]:
        """
        Aplica regras aos dados do item

        Args:
            item_data (Dict[str, Any]): Dados do item para an√°lise

        Returns:
            Dict[str, Any]: Resultado da aplica√ß√£o das regras
        """
        try:
            # Initialize decision result
            decision = {
                "status": "approved",  # Default to approved if no rules trigger
                "reason": "Nenhuma regra espec√≠fica ativada",
                "confidence_adjustment": 0.0,
                "triggered_rules": []
            }

            # Extract relevant scores from item_data
            validation_scores = item_data.get("validation_scores", {})
            sentiment_analysis = item_data.get("sentiment_analysis", {})
            bias_analysis = item_data.get("bias_disinformation_analysis", {})
            llm_analysis = item_data.get("llm_reasoning_analysis", {})

            # Calculate overall confidence and risk
            overall_confidence = self._calculate_overall_confidence(
                validation_scores, sentiment_analysis, bias_analysis, llm_analysis)
            overall_risk = bias_analysis.get("overall_risk", 0.0)
            llm_recommendation = llm_analysis.get(
                "llm_recommendation", "REVIS√ÉO_MANUAL")

            # Apply each rule in order
            for rule in self.rules:
                if self._evaluate_condition(rule, overall_confidence, overall_risk, llm_recommendation, item_data):
                    rule_name = rule.get("name", "unknown_rule")
                    action = rule.get("action", {})

                    # Update decision
                    decision["status"] = action.get("status", "approved")
                    decision["reason"] = action.get(
                        "reason", f"Regra '{rule_name}' ativada")
                    decision["confidence_adjustment"] = action.get(
                        "confidence_adjustment", 0.0)
                    decision["triggered_rules"].append(rule_name)

                    logger.debug(
                        f"Regra '{rule_name}' ativada: {decision['status']} - {decision['reason']}")

                    # Stop at first matching rule (rules should be ordered by priority)
                    break

            return decision

        except Exception as e:
            logger.error(f"Erro ao aplicar regras: {e}")
            return {
                "status": "rejected",  # Fail safe - reject on error
                "reason": f"Erro no processamento de regras: {str(e)}",
                "confidence_adjustment": -0.3,
                "triggered_rules": ["error_fallback"]
            }

    def _evaluate_condition(self, rule: Dict[str, Any], overall_confidence: float, overall_risk: float, llm_recommendation: str, item_data: Dict[str, Any]) -> bool:
        """
        Avalia se a condi√ß√£o de uma regra √© atendida

        Args:
            rule: Regra para avaliar
            overall_confidence: Confian√ßa geral calculada
            overall_risk: Risco geral calculado
            llm_recommendation: Recomenda√ß√£o do LLM
            item_data: Dados completos do item

        Returns:
            bool: True se a condi√ß√£o for atendida
        """
        try:
            condition = rule.get("condition", "")

            if not condition:
                return False

            # Simple condition evaluation
            # Replace variables in condition string
            condition = condition.replace(
                "overall_confidence", str(overall_confidence))
            condition = condition.replace("overall_risk", str(overall_risk))
            condition = condition.replace(
                "llm_recommendation", f"'{llm_recommendation}'")

            # Evaluate mathematical expressions
            if any(op in condition for op in [">=", "<=", "==", ">", "<", "!="]):
                try:
                    # Safe evaluation of simple mathematical conditions
                    return self._safe_eval_condition(condition)
                except:
                    logger.warning(f"Erro ao avaliar condi√ß√£o: {condition}")
                    return False

            return False

        except Exception as e:
            logger.error(f"Erro na avalia√ß√£o da condi√ß√£o: {e}")
            return False

    def _safe_eval_condition(self, condition: str) -> bool:
        """
        Avalia condi√ß√µes matem√°ticas simples de forma segura

        Args:
            condition (str): Condi√ß√£o para avaliar

        Returns:
            bool: Resultado da avalia√ß√£o
        """
        try:
            # Only allow safe mathematical operations and comparisons
            allowed_chars = set(
                "0123456789.><=! '\"ABCDEFGHIJKLMNOPQRSTUVWXYZ_abcdefghijklmnopqrstuvwxyz")

            if not all(c in allowed_chars for c in condition):
                logger.warning(
                    f"Caracteres n√£o permitidos na condi√ß√£o: {condition}")
                return False

            # Simple string replacements for evaluation
            if ">=" in condition:
                parts = condition.split(">=")
                if len(parts) == 2:
                    try:
                        left = float(parts[0].strip())
                        right = float(parts[1].strip())
                        return left >= right
                    except ValueError:
                        # Handle string comparisons
                        return parts[0].strip() == parts[1].strip()

            elif "<=" in condition:
                parts = condition.split("<=")
                if len(parts) == 2:
                    left = float(parts[0].strip())
                    right = float(parts[1].strip())
                    return left <= right

            elif "==" in condition:
                parts = condition.split("==")
                if len(parts) == 2:
                    left = parts[0].strip().strip("'\"")
                    right = parts[1].strip().strip("'\"")
                    return left == right

            elif ">" in condition:
                parts = condition.split(">")
                if len(parts) == 2:
                    left = float(parts[0].strip())
                    right = float(parts[1].strip())
                    return left > right

            elif "<" in condition:
                parts = condition.split("<")
                if len(parts) == 2:
                    left = float(parts[0].strip())
                    right = float(parts[1].strip())
                    return left < right

            return False

        except Exception as e:
            logger.error(f"Erro na avalia√ß√£o segura da condi√ß√£o: {e}")
            return False

    def _calculate_overall_confidence(self, validation_scores: Dict[str, Any], sentiment_analysis: Dict[str, Any], bias_analysis: Dict[str, Any], llm_analysis: Dict[str, Any]) -> float:
        """Calcula confian√ßa geral baseada em todas as an√°lises"""
        try:
            # Start with base validation confidence
            base_confidence = validation_scores.get("overall_confidence", 0.5)

            # Adjust based on sentiment analysis
            sentiment_confidence = sentiment_analysis.get("confidence", 0.5)
            sentiment_weight = 0.2

            # Adjust based on bias analysis (lower bias risk = higher confidence)
            # Invert risk to confidence
            bias_confidence = 1.0 - bias_analysis.get("overall_risk", 0.5)
            bias_weight = 0.3

            # Adjust based on LLM analysis
            llm_confidence = llm_analysis.get("llm_confidence", 0.5)
            llm_weight = 0.4

            # Weighted combination
            overall_confidence = (
                base_confidence * (1.0 - sentiment_weight - bias_weight - llm_weight) +
                sentiment_confidence * sentiment_weight +
                bias_confidence * bias_weight +
                llm_confidence * llm_weight
            )

            return min(max(overall_confidence, 0.0), 1.0)

        except Exception as e:
            logger.warning(f"Erro no c√°lculo de confian√ßa geral: {e}")
            return 0.5

    def _log_rules(self):
        """Log das regras configuradas"""
        logger.debug("Regras configuradas:")
        for i, rule in enumerate(self.rules):
            logger.debug(
                f"  {i+1}. {rule.get('name', 'sem_nome')}: {rule.get('condition', 'sem_condi√ß√£o')}")

    def add_rule(self, rule: Dict[str, Any]):
        """
        Adiciona uma nova regra

        Args:
            rule (Dict[str, Any]): Nova regra para adicionar
        """
        if self._validate_rule(rule):
            self.rules.append(rule)
            logger.info(
                f"Nova regra adicionada: {rule.get('name', 'sem_nome')}")
        else:
            logger.warning(f"Regra inv√°lida rejeitada: {rule}")

    def _validate_rule(self, rule: Dict[str, Any]) -> bool:
        """Valida se uma regra est√° bem formada"""
        return (
            isinstance(rule, dict) and
            "condition" in rule and
            "action" in rule and
            isinstance(rule["action"], dict)
        )  # !/usr/bin/env python3


# -*- coding: utf-8 -*-
"""
ARQV30 Enhanced v3.0 - External Sentiment Analyzer
M√≥dulo independente para an√°lise de sentimento e polaridade
"""


# Try to import TextBlob, fallback if not available
try:
    from textblob import TextBlob
    TEXTBLOB_AVAILABLE = True
except ImportError:
    TEXTBLOB_AVAILABLE = False
    logging.warning("TextBlob n√£o dispon√≠vel. Usando an√°lise b√°sica.")

# Try to import VADER, fallback if not available
try:
    from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer
    VADER_AVAILABLE = True
except ImportError:
    VADER_AVAILABLE = False
    logging.warning("VADER Sentiment n√£o dispon√≠vel.")

logger = logging.getLogger(__name__)


class ExternalSentimentAnalyzer:
    """Analisador de sentimento externo independente"""

    def __init__(self, config: Dict[str, Any]):
        """Inicializa o analisador de sentimento"""
        self.config = config.get('sentiment_analysis', {})
        self.enabled = self.config.get('enabled', True)
        self.use_vader = self.config.get('use_vader', True) and VADER_AVAILABLE
        self.use_textblob = self.config.get(
            'use_textblob', True) and TEXTBLOB_AVAILABLE
        self.polarity_weights = self.config.get('polarity_weights', {
            'positive': 1.1,
            'negative': 0.8,
            'neutral': 1.0
        })

        # Initialize VADER if available and enabled
        if self.use_vader:
            self.vader_analyzer = SentimentIntensityAnalyzer()

        # Palavras para an√°lise b√°sica quando bibliotecas n√£o est√£o dispon√≠veis
        self.positive_words = {"bom", "√≥timo", "excelente", "maravilhoso", "perfeito", "incr√≠vel", "fant√°stico", "amor", "feliz", "alegre",
                               "positivo", "sucesso", "ganho", "oportunidade", "melhor", "bem", "confi√°vel", "seguro", "verdadeiro", "justo", "aprovado"}
        self.negative_words = {"ruim", "p√©ssimo", "terr√≠vel", "horr√≠vel", "odiar", "triste", "raiva", "problema", "erro", "falha", "negativo",
                               "fracasso", "perda", "amea√ßa", "pior", "mal", "duvidoso", "inseguro", "falso", "injusto", "rejeitado", "vi√©s", "desinforma√ß√£o"}

        logger.info(
            f"‚úÖ External Sentiment Analyzer inicializado (VADER: {self.use_vader}, TextBlob: {self.use_textblob})")

    def analyze_sentiment(self, text: str) -> Dict[str, float]:
        """
        Analisa o sentimento do texto fornecido

        Args:
            text (str): Texto para an√°lise

        Returns:
            Dict[str, float]: Resultados da an√°lise de sentimento
        """
        if not self.enabled or not text or not text.strip():
            return self._get_neutral_sentiment()

        try:
            # Clean text
            cleaned_text = self._clean_text(text)

            results = {
                'polarity': 0.0,
                'subjectivity': 0.0,
                'compound': 0.0,
                'positive': 0.0,
                'negative': 0.0,
                'neutral': 0.0,
                'classification': 'neutral',
                'confidence': 0.0,
                'analysis_methods': []
            }

            # TextBlob Analysis
            if self.use_textblob:
                textblob_results = self._analyze_with_textblob(cleaned_text)
                results.update(textblob_results)
                results['analysis_methods'].append('textblob')

            # VADER Analysis
            if self.use_vader:
                vader_results = self._analyze_with_vader(cleaned_text)
                # Combine VADER results with TextBlob
                results['compound'] = vader_results['compound']
                results['positive'] = vader_results['pos']
                results['negative'] = vader_results['neg']
                results['neutral'] = vader_results['neu']
                results['analysis_methods'].append('vader')

                # Use VADER for final classification if available
                results['classification'] = self._classify_sentiment_vader(
                    vader_results)

            # Apply polarity weights
            results = self._apply_polarity_weights(results)

            # Calculate final confidence
            results['confidence'] = self._calculate_confidence(results)

            logger.debug(
                f"Sentiment analysis completed: {results['classification']} (confidence: {results['confidence']:.3f})")

            return results

        except Exception as e:
            logger.error(f"Erro na an√°lise de sentimento: {e}")
            return self._get_neutral_sentiment()

    def _clean_text(self, text: str) -> str:
        """Limpa o texto para an√°lise"""
        if not text:
            return ""

        # Remove URLs
        text = re.sub(
            r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', '', text)

        # Remove mentions and hashtags (keep the content)
        text = re.sub(r'[@#](\w+)', r'\\1', text)

        # Remove excessive whitespace
        text = re.sub(r'\\s+', ' ', text).strip()

        return text

    def _analyze_with_textblob(self, text: str) -> Dict[str, float]:
        """An√°lise com TextBlob ou an√°lise b√°sica"""
        try:
            if TEXTBLOB_AVAILABLE:
                blob = TextBlob(text)
                return {
                    'polarity': blob.sentiment.polarity,  # -1 to 1
                    'subjectivity': blob.sentiment.subjectivity  # 0 to 1
                }
            else:
                # An√°lise b√°sica usando palavras-chave
                return self._basic_sentiment_analysis(text)
        except Exception as e:
            logger.warning(f"Erro na an√°lise de sentimento: {e}")
            return {'polarity': 0.0, 'subjectivity': 0.0}

    def _basic_sentiment_analysis(self, text: str) -> Dict[str, float]:
        """An√°lise b√°sica de sentimento usando palavras-chave"""
        text_lower = text.lower()
        positive_count = sum(
            1 for word in self.positive_words if word in text_lower)
        negative_count = sum(
            1 for word in self.negative_words if word in text_lower)

        # Calcular polaridade b√°sica
        total_words = len(text_lower.split())
        if total_words == 0:
            return {'polarity': 0.0, 'subjectivity': 0.0}

        polarity = (positive_count - negative_count) / max(total_words, 1)
        # Normalizar entre -1 e 1
        polarity = max(-1.0, min(1.0, polarity * 10))

        # Subjetividade baseada na quantidade de palavras emocionais
        subjectivity = min((positive_count + negative_count) /
                           max(total_words, 1) * 5, 1.0)

        return {'polarity': polarity, 'subjectivity': subjectivity}

    def _analyze_with_vader(self, text: str) -> Dict[str, float]:
        """An√°lise com VADER"""
        try:
            return self.vader_analyzer.polarity_scores(text)
        except Exception as e:
            logger.warning(f"Erro no VADER: {e}")
            return {'compound': 0.0, 'pos': 0.0, 'neg': 0.0, 'neu': 1.0}

    def _classify_sentiment_vader(self, vader_scores: Dict[str, float]) -> str:
        """Classifica sentimento baseado nos scores do VADER"""
        compound = vader_scores.get('compound', 0.0)

        if compound >= 0.05:
            return 'positive'
        elif compound <= -0.05:
            return 'negative'
        else:
            return 'neutral'

    def _apply_polarity_weights(self, results: Dict[str, float]) -> Dict[str, float]:
        """Aplica pesos de polaridade configurados"""
        classification = results.get('classification', 'neutral')
        weight = self.polarity_weights.get(classification, 1.0)

        # Adjust polarity and compound scores
        if 'polarity' in results:
            results['polarity'] *= weight
        if 'compound' in results:
            results['compound'] *= weight

        return results

    def _calculate_confidence(self, results: Dict[str, float]) -> float:
        """Calcula confian√ßa da an√°lise"""
        try:
            # Base confidence on the strength of sentiment indicators
            polarity_abs = abs(results.get('polarity', 0.0))
            compound_abs = abs(results.get('compound', 0.0))
            subjectivity = results.get('subjectivity', 0.0)

            # Higher absolute values indicate stronger sentiment (more confident)
            sentiment_strength = max(polarity_abs, compound_abs)

            # Subjectivity can indicate confidence (highly subjective = less reliable)
            subjectivity_penalty = subjectivity * 0.2

            # Method bonus (more methods = higher confidence)
            method_count = len(results.get('analysis_methods', []))
            method_bonus = min(method_count * 0.1, 0.2)

            confidence = min(sentiment_strength +
                             method_bonus - subjectivity_penalty, 1.0)
            confidence = max(confidence, 0.1)  # Minimum confidence

            return confidence

        except Exception as e:
            logger.warning(f"Erro no c√°lculo de confian√ßa: {e}")
            return 0.5

    def _get_neutral_sentiment(self) -> Dict[str, Any]:
        """Retorna resultado neutro com valores padr√£o"""
        return {
            "polarity": 0.0,
            "subjectivity": 0.0,
            "compound": 0.0,
            "positive": 0.0,
            "negative": 0.0,
            "neutral": 1.0,
            "classification": "neutral",
            "confidence": 0.1,
            "analysis_methods": []
        }


# -*- coding: utf-8 -*-
"""
ARQV30 Enhanced v3.0 - External Bias & Disinformation Detector
M√≥dulo independente para detec√ß√£o de vi√©s e desinforma√ß√£o
"""


logger = logging.getLogger(__name__)


class ExternalBiasDisinformationDetector:
    """Detector de vi√©s e desinforma√ß√£o externo independente"""

    def __init__(self, config: Dict[str, Any]):
        """Inicializa o detector de vi√©s e desinforma√ß√£o"""
        self.config = config.get('bias_detection', {})
        self.enabled = self.config.get('enabled', True)

        # Load configuration
        self.bias_keywords = self.config.get('bias_keywords', [])
        self.disinformation_patterns = self.config.get(
            'disinformation_patterns', [])
        self.rhetoric_devices = self.config.get('rhetoric_devices', [])

        logger.info(f"‚úÖ External Bias & Disinformation Detector inicializado")
        logger.debug(
            f"Bias keywords: {len(self.bias_keywords)}, Patterns: {len(self.disinformation_patterns)}")

    def detect_bias_disinformation(self, text: str) -> Dict[str, float]:
        """
        Detecta padr√µes de vi√©s e desinforma√ß√£o no texto

        Args:
            text (str): Texto para an√°lise

        Returns:
            Dict[str, float]: Resultados da detec√ß√£o
        """
        if not self.enabled or not text or not text.strip():
            return self._get_neutral_result()

        try:
            # Clean and prepare text
            cleaned_text = self._clean_text(text)
            text_lower = cleaned_text.lower()

            results = {
                'bias_score': 0.0,
                'disinformation_score': 0.0,
                'rhetoric_score': 0.0,
                'overall_risk': 0.0,
                'detected_bias_keywords': [],
                'detected_disinformation_patterns': [],
                'detected_rhetoric_devices': [],
                'confidence': 0.0,
                'analysis_details': {
                    'total_words': len(cleaned_text.split()),
                    'bias_matches': 0,
                    'disinformation_matches': 0,
                    'rhetoric_matches': 0
                }
            }

            # Detect bias keywords
            bias_analysis = self._detect_bias_keywords(text_lower)
            results.update(bias_analysis)

            # Detect disinformation patterns
            disinformation_analysis = self._detect_disinformation_patterns(
                text_lower)
            results.update(disinformation_analysis)

            # Detect rhetoric devices
            rhetoric_analysis = self._detect_rhetoric_devices(text_lower)
            results.update(rhetoric_analysis)

            # Calculate overall risk
            results['overall_risk'] = self._calculate_overall_risk(results)

            # Calculate confidence
            results['confidence'] = self._calculate_confidence(results)

            logger.debug(
                f"Bias/Disinformation analysis: risk={results['overall_risk']:.3f}, confidence={results['confidence']:.3f}")

            return results

        except Exception as e:
            logger.error(f"Erro na detec√ß√£o de vi√©s/desinforma√ß√£o: {e}")
            return self._get_neutral_result()

    def _clean_text(self, text: str) -> str:
        """Limpa o texto para an√°lise"""
        if not text:
            return ""

        # Remove URLs, mentions, hashtags but keep text structure
        text = re.sub(
            r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', '', text)
        text = re.sub(r'[@#](\w+)', r'\\1', text)
        text = re.sub(r'\\s+', ' ', text).strip()

        return text

    def _detect_bias_keywords(self, text_lower: str) -> Dict[str, Any]:
        """Detecta palavras-chave de vi√©s"""
        detected_keywords = []
        bias_score = 0.0

        for keyword in self.bias_keywords:
            if keyword.lower() in text_lower:
                detected_keywords.append(keyword)
                bias_score += 0.1  # Each bias keyword adds 0.1 to score

        # Normalize score (cap at 1.0)
        bias_score = min(bias_score, 1.0)

        return {
            'bias_score': bias_score,
            'detected_bias_keywords': detected_keywords,
            'analysis_details': {'bias_matches': len(detected_keywords)}
        }

    def _detect_disinformation_patterns(self, text_lower: str) -> Dict[str, Any]:
        """Detecta padr√µes de desinforma√ß√£o"""
        detected_patterns = []
        disinformation_score = 0.0

        for pattern in self.disinformation_patterns:
            if pattern.lower() in text_lower:
                detected_patterns.append(pattern)
                disinformation_score += 0.15  # Each pattern adds more weight

        # Additional pattern detection with regex
        # Look for vague authority claims
        authority_patterns = [
            r'especialistas? (?:afirmam?|dizem?|garantem?)',
            r'estudos? (?:comprovam?|mostram?|indicam?)',
            r'pesquisas? (?:revelam?|demonstram?|apontam?)',
            r'cientistas? (?:descobriram?|provaram?|confirmaram?)'
        ]

        # Look for strawman fallacy
        strawman_patterns = [
            r'(?:eles|eles dizem|a esquerda|a direita) querem? (?:destruir|acabar com|impor) (?:nossa cultura|nossos valores|a fam√≠lia)',
            r'(?:argumento do espantalho|distorcem|exageram) (?:o que eu disse|nossas palavras)'
        ]

        # Look for ad hominem attacks
        ad_hominem_patterns = [
            r'(?:ele|ela|voc√™) n√£o tem moral para falar',
            r'(?:ignorante|burro|mentiroso|hip√≥crita) (?:para acreditar|para defender)'
        ]

        # Look for false dichotomy
        false_dichotomy_patterns = [
            r'(?:ou √©|ou voc√™ est√° com|ou voc√™ apoia) (?:n√≥s|eles) (?:ou contra n√≥s|ou contra eles)',
            r'(?:s√≥ existem|apenas duas? op√ß√µes?)'
        ]

        # Look for appeal to emotion
        appeal_to_emotion_patterns = [
            r'(?:pense nas crian√ßas|e se fosse voc√™|voc√™ n√£o se importa)',
            r'(?:chocante|absurdo|inacredit√°vel|revoltante)'
        ]

        all_regex_patterns = authority_patterns + strawman_patterns + \
            ad_hominem_patterns + false_dichotomy_patterns + appeal_to_emotion_patterns

        for pattern in all_regex_patterns:
            matches = re.findall(pattern, text_lower)
            if matches:
                detected_patterns.extend(matches)
                disinformation_score += len(matches) * 0.1

        # Normalize score
        disinformation_score = min(disinformation_score, 1.0)

        return {
            'disinformation_score': disinformation_score,
            'detected_disinformation_patterns': detected_patterns,
            'analysis_details': {'disinformation_matches': len(detected_patterns)}
        }

    def _detect_rhetoric_devices(self, text_lower: str) -> Dict[str, Any]:
        """Detecta dispositivos ret√≥ricos"""
        detected_devices = []
        rhetoric_score = 0.0

        # Detect emotional manipulation patterns
        emotional_patterns = {
            'apelo ao medo': [r'perig(o|oso|osa)', r'risco', r'amea√ßa', r'cat√°strofe'],
            'apelo √† emo√ß√£o': [r'imaginem?', r'pensem?', r'sintam?'],
            'generaliza√ß√£o': [r'todos? (?:sabem?|fazem?)', r'ningu√©m', r'sempre', r'nunca'],
            'falsa dicotomia': [r'ou (?:voc√™|voc√™s?)', r'apenas duas? op√ß']
        }

        for device_name, patterns in emotional_patterns.items():
            for pattern in patterns:
                if re.search(pattern, text_lower):
                    detected_devices.append(device_name)
                    rhetoric_score += 0.1
                    break  # Only count each device type once

        # Check configured rhetoric devices
        for device in self.rhetoric_devices:
            if device.lower() in text_lower:
                detected_devices.append(device)
                rhetoric_score += 0.1

        # Normalize score
        rhetoric_score = min(rhetoric_score, 1.0)

        return {
            'rhetoric_score': rhetoric_score,
            # Remove duplicates
            'detected_rhetoric_devices': list(set(detected_devices)),
            'analysis_details': {'rhetoric_matches': len(detected_devices)}
        }

    def _calculate_overall_risk(self, results: Dict[str, Any]) -> float:
        """Calcula o risco geral"""
        # Weighted combination of different risk factors
        bias_weight = 0.3
        disinformation_weight = 0.4
        rhetoric_weight = 0.3

        overall_risk = (
            results.get('bias_score', 0.0) * bias_weight +
            results.get('disinformation_score', 0.0) * disinformation_weight +
            results.get('rhetoric_score', 0.0) * rhetoric_weight
        )

        return min(overall_risk, 1.0)

    def _calculate_confidence(self, results: Dict[str, Any]) -> float:
        """Calcula a confian√ßa da an√°lise"""
        try:
            total_words = results.get(
                'analysis_details', {}).get('total_words', 1)
            total_matches = (
                len(results.get('detected_bias_keywords', [])) +
                len(results.get('detected_disinformation_patterns', [])) +
                len(results.get('detected_rhetoric_devices', []))
            )

            # Base confidence on detection density and text length
            if total_words < 10:
                return 0.3  # Low confidence for very short text

            detection_density = total_matches / total_words

            # Higher density = higher confidence in detection
            confidence = min(0.5 + (detection_density * 5), 1.0)

            # If no matches found, still have some confidence it's clean
            if total_matches == 0 and total_words > 20:
                confidence = 0.7
            elif total_matches == 0:
                confidence = 0.5

            return max(confidence, 0.1)

        except Exception as e:
            logger.warning(f"Erro no c√°lculo de confian√ßa: {e}")
            return 0.5

    def _get_neutral_result(self) -> Dict[str, float]:
        """Retorna resultado neutro padr√£o"""
        return {
            'bias_score': 0.0,
            'disinformation_score': 0.0,
            'rhetoric_score': 0.0,
            'overall_risk': 0.0,
            'detected_bias_keywords': [],
            'detected_disinformation_patterns': [],
            'detected_rhetoric_devices': [],
            'confidence': 0.1,
            'analysis_details': {
                'total_words': 0,
                'bias_matches': 0,
                'disinformation_matches': 0,
                'rhetoric_matches': 0
            }
        }  # !/usr/bin/env python3


# -*- coding: utf-8 -*-
"""
ARQV30 Enhanced v3.0 - External Confidence Thresholds
Gerenciador de limiares de confian√ßa para o m√≥dulo externo
"""


logger = logging.getLogger(__name__)


class ExternalConfidenceThresholds:
    """Gerenciador de limiares de confian√ßa externo"""

    def __init__(self, config: Dict[str, Any]):
        """Inicializa os limiares de confian√ßa"""
        self.thresholds = config.get('thresholds', {})

        # Default thresholds if not provided
        self.default_thresholds = {
            'approval': 0.75,
            'rejection': 0.35,
            'high_confidence': 0.85,
            'low_confidence': 0.5,
            'sentiment_neutral': 0.1,
            'bias_high_risk': 0.7,
            'llm_minimum': 0.6
        }

        # Merge with defaults
        for key, default_value in self.default_thresholds.items():
            if key not in self.thresholds:
                self.thresholds[key] = default_value

        logger.info(
            f"‚úÖ External Confidence Thresholds inicializado com {len(self.thresholds)} limiares")
        logger.debug(f"Thresholds: {self.thresholds}")

    def get_threshold(self, score_type: str, default: Optional[float] = None) -> float:
        """
        Obt√©m o limiar para um tipo de pontua√ß√£o espec√≠fico

        Args:
            score_type (str): Tipo de pontua√ß√£o (e.g., 'approval', 'rejection')
            default (Optional[float]): Valor padr√£o se n√£o encontrado

        Returns:
            float: Limiar de confian√ßa
        """
        if score_type in self.thresholds:
            return self.thresholds[score_type]

        if default is not None:
            return default

        # Return a reasonable default based on score type
        if 'approval' in score_type.lower():
            return 0.7
        elif 'rejection' in score_type.lower():
            return 0.3
        elif 'high' in score_type.lower():
            return 0.8
        elif 'low' in score_type.lower():
            return 0.4
        else:
            return 0.5

    def should_approve(self, confidence: float) -> bool:
        """Verifica se deve aprovar baseado na confian√ßa"""
        return confidence >= self.get_threshold('approval')

    def should_reject(self, confidence: float) -> bool:
        """Verifica se deve rejeitar baseado na confian√ßa"""
        return confidence <= self.get_threshold('rejection')

    def is_ambiguous(self, confidence: float) -> bool:
        """Verifica se est√° em faixa amb√≠gua (entre rejection e approval)"""
        rejection_threshold = self.get_threshold('rejection')
        approval_threshold = self.get_threshold('approval')
        return rejection_threshold < confidence < approval_threshold

    def is_high_confidence(self, confidence: float) -> bool:
        """Verifica se √© alta confian√ßa"""
        return confidence >= self.get_threshold('high_confidence')

    def is_low_confidence(self, confidence: float) -> bool:
        """Verifica se √© baixa confian√ßa"""
        return confidence <= self.get_threshold('low_confidence')

    def is_high_bias_risk(self, risk_score: float) -> bool:
        """Verifica se √© alto risco de vi√©s"""
        return risk_score >= self.get_threshold('bias_high_risk')

    def classify_confidence_level(self, confidence: float) -> str:
        """
        Classifica o n√≠vel de confian√ßa

        Args:
            confidence (float): Pontua√ß√£o de confian√ßa

        Returns:
            str: N√≠vel de confian√ßa ('high', 'medium', 'low')
        """
        if self.is_high_confidence(confidence):
            return 'high'
        elif self.is_low_confidence(confidence):
            return 'low'
        else:
            return 'medium'

    def get_decision_recommendation(self, confidence: float, risk_score: float = 0.0) -> Dict[str, Any]:
        """
        Recomenda uma decis√£o baseada na confian√ßa e risco

        Args:
            confidence (float): Pontua√ß√£o de confian√ßa
            risk_score (float): Pontua√ß√£o de risco

        Returns:
            Dict[str, Any]: Recomenda√ß√£o de decis√£o
        """
        # High risk overrides high confidence
        if self.is_high_bias_risk(risk_score):
            return {
                'decision': 'reject',
                'reason': 'Alto risco de vi√©s/desinforma√ß√£o detectado',
                'confidence_level': self.classify_confidence_level(confidence),
                'risk_level': 'high',
                'requires_llm_analysis': False
            }

        # Clear approval
        if self.should_approve(confidence):
            return {
                'decision': 'approve',
                'reason': 'Alta confian√ßa na qualidade do conte√∫do',
                'confidence_level': self.classify_confidence_level(confidence),
                'risk_level': 'low' if risk_score < 0.3 else 'medium',
                'requires_llm_analysis': False
            }

        # Clear rejection
        if self.should_reject(confidence):
            return {
                'decision': 'reject',
                'reason': 'Baixa confian√ßa na qualidade do conte√∫do',
                'confidence_level': self.classify_confidence_level(confidence),
                'risk_level': 'low' if risk_score < 0.3 else 'medium',
                'requires_llm_analysis': False
            }

        # Ambiguous case - might need LLM analysis
        return {
            'decision': 'ambiguous',
            'reason': 'Confian√ßa em faixa amb√≠gua - requer an√°lise adicional',
            'confidence_level': self.classify_confidence_level(confidence),
            'risk_level': 'medium' if risk_score < 0.5 else 'high',
            'requires_llm_analysis': True
        }

    def update_threshold(self, score_type: str, new_value: float):
        """
        Atualiza um limiar espec√≠fico

        Args:
            score_type (str): Tipo de pontua√ß√£o
            new_value (float): Novo valor do limiar
        """
        if 0.0 <= new_value <= 1.0:
            self.thresholds[score_type] = new_value
            logger.info(
                f"Threshold '{score_type}' atualizado para {new_value}")
        else:
            logger.warning(
                f"Valor inv√°lido para threshold '{score_type}': {new_value} (deve estar entre 0.0 e 1.0)")

    def get_all_thresholds(self) -> Dict[str, float]:
        """Retorna todos os limiares configurados"""
        return self.thresholds.copy()

    def validate_thresholds(self) -> bool:
        """
        Valida se os limiares est√£o configurados corretamente

        Returns:
            bool: True se v√°lidos, False caso contr√°rio
        """
        try:
            # Check that rejection < approval
            rejection = self.get_threshold('rejection')
            approval = self.get_threshold('approval')

            if rejection >= approval:
                logger.error(
                    f"Configura√ß√£o inv√°lida: rejection ({rejection}) deve ser menor que approval ({approval})")
                return False

            # Check that all thresholds are in valid range
            for key, value in self.thresholds.items():
                if not (0.0 <= value <= 1.0):
                    logger.error(
                        f"Threshold '{key}' fora do range v√°lido (0.0-1.0): {value}")
                    return False

            logger.info("‚úÖ Todos os thresholds s√£o v√°lidos")
            return True

        except Exception as e:
            logger.error(f"Erro na valida√ß√£o dos thresholds: {e}")
            return False  # !/usr/bin/env python3


# -*- coding: utf-8 -*-
"""
ARQV30 Enhanced v3.0 - External Contextual Analyzer
Analisador contextual para o m√≥dulo externo de verifica√ß√£o
"""


logger = logging.getLogger(__name__)


class ExternalContextualAnalyzer:
    """Analisador contextual externo independente"""

    def __init__(self, config: Dict[str, Any]):
        """Inicializa o analisador contextual"""
        self.config = config.get('contextual_analysis', {})
        self.enabled = self.config.get('enabled', True)
        self.check_consistency = self.config.get('check_consistency', True)
        self.analyze_source_reliability = self.config.get(
            'analyze_source_reliability', True)
        self.verify_temporal_coherence = self.config.get(
            'verify_temporal_coherence', True)

        # Initialize context cache for cross-item analysis
        self.context_cache = {
            'processed_items': [],
            'source_patterns': {},
            'content_patterns': {},
            'temporal_markers': []
        }

        logger.info(f"‚úÖ External Contextual Analyzer inicializado")
        logger.debug(
            f"Configura√ß√µes: consistency={self.check_consistency}, source={self.analyze_source_reliability}, temporal={self.verify_temporal_coherence}")

    def analyze_context(self, item_data: Dict[str, Any], massive_data: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:
        """
        Analisa o item em contexto mais amplo

        Args:
            item_data (Dict[str, Any]): Dados do item individual
            massive_data (Optional[Dict[str, Any]]): Dados contextuais mais amplos

        Returns:
            Dict[str, Any]: An√°lise contextual
        """
        if not self.enabled:
            return self._get_neutral_result()

        try:
            # Initialize context analysis result
            context_result = {
                'contextual_confidence': 0.5,
                'consistency_score': 0.5,
                'source_reliability_score': 0.5,
                'temporal_coherence_score': 0.5,
                'context_flags': [],
                'context_insights': [],
                'adjustment_factor': 0.0
            }

            # Extract text content for analysis
            text_content = self._extract_text_content(item_data)

            # Perform different types of contextual analysis
            if self.check_consistency:
                consistency_analysis = self._analyze_consistency(
                    text_content, item_data, massive_data)
                context_result.update(consistency_analysis)

            if self.analyze_source_reliability:
                source_analysis = self._analyze_source_reliability(
                    item_data, massive_data)
                context_result.update(source_analysis)

            if self.verify_temporal_coherence:
                temporal_analysis = self._analyze_temporal_coherence(
                    text_content, item_data)
                context_result.update(temporal_analysis)

            # Calculate overall contextual confidence
            context_result['contextual_confidence'] = self._calculate_contextual_confidence(
                context_result)

            # Update context cache for future analysis
            self._update_context_cache(item_data, context_result)

            logger.debug(
                f"Context analysis: confidence={context_result['contextual_confidence']:.3f}")

            return context_result

        except Exception as e:
            logger.error(f"Erro na an√°lise contextual: {e}")
            return self._get_neutral_result()

    def _extract_text_content(self, item_data: Dict[str, Any]) -> str:
        """Extrai conte√∫do textual relevante do item"""
        content_fields = ['content', 'text', 'title', 'description', 'summary']

        text_content = ""
        for field in content_fields:
            if field in item_data and item_data[field]:
                text_content += f" {item_data[field]}"

        return text_content.strip()

    def _analyze_consistency(self, text_content: str, item_data: Dict[str, Any], massive_data: Optional[Dict[str, Any]]) -> Dict[str, Any]:
        """Analisa consist√™ncia interna e externa"""
        consistency_result = {
            'consistency_score': 0.5,
            'consistency_flags': [],
            'consistency_insights': []
        }

        try:
            score = 0.5
            flags = []
            insights = []

            # Check internal consistency
            internal_score, internal_flags = self._check_internal_consistency(
                text_content)
            score = (score + internal_score) / 2
            flags.extend(internal_flags)

            # Check consistency with previous items if available
            if self.context_cache['processed_items']:
                external_score, external_flags = self._check_external_consistency(
                    text_content, item_data)
                score = (score + external_score) / 2
                flags.extend(external_flags)

                if external_score < 0.3:
                    insights.append(
                        "Conte√∫do inconsistente com padr√µes anteriores")
                elif external_score > 0.8:
                    insights.append(
                        "Conte√∫do altamente consistente com padr√µes estabelecidos")

            consistency_result.update({
                'consistency_score': score,
                'consistency_flags': flags,
                'consistency_insights': insights
            })

        except Exception as e:
            logger.warning(f"Erro na an√°lise de consist√™ncia: {e}")

        return consistency_result

    def _check_internal_consistency(self, text_content: str) -> tuple:
        """Verifica consist√™ncia interna do texto"""
        score = 0.7  # Start with good assumption
        flags = []

        if not text_content or len(text_content.strip()) < 10:
            return 0.3, ["Conte√∫do muito curto para an√°lise de consist√™ncia"]

        # Check for contradictory statements
        contradiction_patterns = [
            (r'sempre.*nunca', "Contradi√ß√£o: 'sempre' e 'nunca' no mesmo contexto"),
            (r'todos?.*ningu√©m', "Contradi√ß√£o: generaliza√ß√£o conflitante"),
            (r'imposs√≠vel.*poss√≠vel', "Contradi√ß√£o: possibilidade conflitante"),
            (r'verdade.*mentira', "Contradi√ß√£o: veracidade conflitante")
        ]

        for pattern, flag_msg in contradiction_patterns:
            if re.search(pattern, text_content.lower()):
                score -= 0.2
                flags.append(flag_msg)

        # Check for temporal inconsistencies
        temporal_patterns = [
            r'ontem.*amanh√£',
            r'passado.*futuro.*hoje',
            r'antes.*depois.*simultaneamente'
        ]

        for pattern in temporal_patterns:
            if re.search(pattern, text_content.lower()):
                score -= 0.1
                flags.append("Poss√≠vel inconsist√™ncia temporal")

        return max(score, 0.0), flags

    def _check_external_consistency(self, text_content: str, item_data: Dict[str, Any]) -> tuple:
        """Verifica consist√™ncia com itens processados anteriormente"""
        score = 0.5
        flags = []

        try:
            # Compare with recent processed items
            # Last 5 items
            recent_items = self.context_cache['processed_items'][-5:]

            if not recent_items:
                return 0.5, []

            # Simple keyword-based similarity check
            current_words = set(text_content.lower().split())

            similarity_scores = []
            for prev_item in recent_items:
                prev_words = set(prev_item.get('text', '').lower().split())
                if prev_words:
                    intersection = len(current_words & prev_words)
                    union = len(current_words | prev_words)
                    similarity = intersection / union if union > 0 else 0
                    similarity_scores.append(similarity)

            if similarity_scores:
                avg_similarity = sum(similarity_scores) / \
                    len(similarity_scores)

                # Very high similarity might indicate duplication
                if avg_similarity > 0.9:
                    score = 0.3
                    flags.append(
                        "Conte√∫do muito similar a itens anteriores (poss√≠vel duplica√ß√£o)")
                # Very low similarity might be inconsistent
                elif avg_similarity < 0.1:
                    score = 0.4
                    flags.append(
                        "Conte√∫do muito diferente do padr√£o estabelecido")
                else:
                    score = 0.7  # Good consistency

        except Exception as e:
            logger.warning(f"Erro na verifica√ß√£o de consist√™ncia externa: {e}")

        return score, flags

    def _analyze_source_reliability(self, item_data: Dict[str, Any], massive_data: Optional[Dict[str, Any]]) -> Dict[str, Any]:
        """Analisa confiabilidade da fonte"""
        source_result = {
            'source_reliability_score': 0.5,
            'source_flags': [],
            'source_insights': []
        }

        try:
            score = 0.5
            flags = []
            insights = []

            # Extract source information
            source_info = self._extract_source_info(item_data)

            if not source_info:
                score = 0.3
                flags.append("Fonte n√£o identificada")
                return {**source_result, 'source_reliability_score': score, 'source_flags': flags}

            # Check source patterns
            source_domain = source_info.get('domain', '').lower()

            # Known reliable patterns
            reliable_indicators = [
                '.edu', '.gov', '.org',
                'academia', 'university', 'instituto',
                'pesquisa', 'ciencia', 'journal'
            ]

            unreliable_indicators = [
                'blog', 'forum', 'social',
                'fake', 'rumor', 'gossip'
            ]

            for indicator in reliable_indicators:
                if indicator in source_domain:
                    score += 0.2
                    insights.append(
                        f"Fonte cont√©m indicador confi√°vel: {indicator}")
                    break

            for indicator in unreliable_indicators:
                if indicator in source_domain:
                    score -= 0.3
                    flags.append(
                        f"Fonte cont√©m indicador de baixa confiabilidade: {indicator}")
                    break

            # Check source history in cache
            if source_domain in self.context_cache['source_patterns']:
                source_stats = self.context_cache['source_patterns'][source_domain]
                avg_quality = source_stats.get('avg_quality', 0.5)

                if avg_quality > 0.7:
                    score += 0.1
                    insights.append("Fonte com hist√≥rico positivo")
                elif avg_quality < 0.4:
                    score -= 0.1
                    flags.append("Fonte com hist√≥rico problem√°tico")

            score = min(max(score, 0.0), 1.0)

            source_result.update({
                'source_reliability_score': score,
                'source_flags': flags,
                'source_insights': insights
            })

        except Exception as e:
            logger.warning(f"Erro na an√°lise de fonte: {e}")

        return source_result

    def _extract_source_info(self, item_data: Dict[str, Any]) -> Dict[str, str]:
        """Extrai informa√ß√µes da fonte"""
        source_fields = ['source', 'url', 'domain', 'author', 'publisher']
        source_info = {}

        for field in source_fields:
            if field in item_data and item_data[field]:
                source_info[field] = str(item_data[field])

        # Extract domain from URL if available
        if 'url' in source_info and 'domain' not in source_info:
            try:
                import urllib.parse
                parsed = urllib.parse.urlparse(source_info['url'])
                source_info['domain'] = parsed.netloc
            except:
                pass

        return source_info

    def _analyze_temporal_coherence(self, text_content: str, item_data: Dict[str, Any]) -> Dict[str, Any]:
        """Analisa coer√™ncia temporal"""
        temporal_result = {
            'temporal_coherence_score': 0.5,
            'temporal_flags': [],
            'temporal_insights': []
        }

        try:
            score = 0.7
            flags = []
            insights = []

            # Extract temporal markers
            temporal_markers = self._extract_temporal_markers(text_content)

            # Check for temporal inconsistencies
            if len(temporal_markers) > 1:
                coherent, issues = self._check_temporal_coherence(
                    temporal_markers)
                if not coherent:
                    score -= 0.3
                    flags.extend(issues)
                else:
                    insights.append("Marcadores temporais coerentes")

            # Check against item timestamp if available
            if 'timestamp' in item_data or 'date' in item_data:
                item_time = item_data.get('timestamp') or item_data.get('date')
                temporal_consistency = self._check_item_temporal_consistency(
                    temporal_markers, item_time)
                if temporal_consistency < 0.5:
                    score -= 0.2
                    flags.append(
                        "Inconsist√™ncia entre conte√∫do e timestamp do item")

            temporal_result.update({
                'temporal_coherence_score': max(score, 0.0),
                'temporal_flags': flags,
                'temporal_insights': insights
            })

        except Exception as e:
            logger.warning(f"Erro na an√°lise temporal: {e}")

        return temporal_result

    def _extract_temporal_markers(self, text: str) -> List[str]:
        """Extrai marcadores temporais do texto"""
        temporal_patterns = [
            r'(?:ontem|hoje|amanh√£)',
            r'(?:esta|pr√≥xima|passada)\s+(?:semana|segunda|ter√ßa|quarta|quinta|sexta|s√°bado|domingo)',
            r'(?:este|pr√≥ximo|passado)\s+(?:m√™s|ano)',
            r'(?:janeiro|fevereiro|mar√ßo|abril|maio|junho|julho|agosto|setembro|outubro|novembro|dezembro)',
            r'(?:2019|2020|2021|2022|2023|2024|2025)',
            r'h√°\s+\d+\s+(?:dias?|meses?|anos?)',
            r'em\s+\d+\s+(?:dias?|meses?|anos?)'
        ]

        markers = []
        for pattern in temporal_patterns:
            matches = re.findall(pattern, text.lower())
            markers.extend(matches)

        return markers

    def _check_temporal_coherence(self, markers: List[str]) -> tuple:
        """Verifica coer√™ncia entre marcadores temporais"""
        # Simple coherence check - this could be made more sophisticated
        issues = []

        # Check for obvious contradictions
        if any('ontem' in m for m in markers) and any('amanh√£' in m for m in markers):
            issues.append(
                "Contradi√ß√£o temporal: 'ontem' e 'amanh√£' no mesmo contexto")

        # Check for year contradictions
        years = [m for m in markers if re.search(r'20\d{2}', m)]
        if len(set(years)) > 2:
            issues.append(
                "M√∫ltiplos anos mencionados - poss√≠vel inconsist√™ncia")

        return len(issues) == 0, issues

    def _check_item_temporal_consistency(self, markers: List[str], item_time: str) -> float:
        """Verifica consist√™ncia temporal com timestamp do item"""
        try:
            # Simple check - this could be enhanced
            current_year = datetime.now().year

            # Check if markers mention current year
            mentions_current_year = any(
                str(current_year) in m for m in markers)
            mentions_old_years = any(str(year) in m for m in markers if year <
                                     current_year - 1 for year in range(2015, current_year))

            if mentions_current_year:
                return 0.8
            elif mentions_old_years:
                return 0.4
            else:
                return 0.6

        except Exception as e:
            logger.warning(
                f"Erro na verifica√ß√£o de consist√™ncia temporal: {e}")
            return 0.5

    def _calculate_contextual_confidence(self, context_result: Dict[str, Any]) -> float:
        """Calcula confian√ßa contextual geral"""
        try:
            scores = [
                context_result.get('consistency_score', 0.5),
                context_result.get('source_reliability_score', 0.5),
                context_result.get('temporal_coherence_score', 0.5)
            ]

            # Weight the scores
            # Consistency and source are more important
            weights = [0.4, 0.4, 0.2]

            weighted_score = sum(score * weight for score,
                                 weight in zip(scores, weights))

            # Penalty for flags
            total_flags = (
                len(context_result.get('consistency_flags', [])) +
                len(context_result.get('source_flags', [])) +
                len(context_result.get('temporal_flags', []))
            )

            flag_penalty = min(total_flags * 0.1, 0.3)

            final_score = max(weighted_score - flag_penalty, 0.0)

            return min(final_score, 1.0)

        except Exception as e:
            logger.warning(f"Erro no c√°lculo de confian√ßa contextual: {e}")
            return 0.5

    def _update_context_cache(self, item_data: Dict[str, Any], context_result: Dict[str, Any]):
        """Atualiza cache de contexto com informa√ß√µes do item atual"""
        try:
            # Add to processed items (keep last 20)
            item_summary = {
                # First 500 chars
                'text': self._extract_text_content(item_data)[:500],
                'context_score': context_result.get('contextual_confidence', 0.5),
                'timestamp': datetime.now().isoformat()
            }

            self.context_cache['processed_items'].append(item_summary)
            if len(self.context_cache['processed_items']) > 20:
                self.context_cache['processed_items'] = self.context_cache['processed_items'][-20:]

            # Update source patterns
            source_info = self._extract_source_info(item_data)
            if source_info.get('domain'):
                domain = source_info['domain']
                if domain not in self.context_cache['source_patterns']:
                    self.context_cache['source_patterns'][domain] = {
                        'count': 0,
                        'total_quality': 0.0,
                        'avg_quality': 0.5
                    }

                stats = self.context_cache['source_patterns'][domain]
                stats['count'] += 1
                stats['total_quality'] += context_result.get(
                    'contextual_confidence', 0.5)
                stats['avg_quality'] = stats['total_quality'] / stats['count']

        except Exception as e:
            logger.warning(f"Erro na atualiza√ß√£o do cache de contexto: {e}")

    def _get_neutral_result(self) -> Dict[str, Any]:
        """Retorna resultado neutro padr√£o"""
        return {
            'contextual_confidence': 0.5,
            'consistency_score': 0.5,
            'source_reliability_score': 0.5,
            'temporal_coherence_score': 0.5,
            'context_flags': [],
            'context_insights': [],
            'adjustment_factor': 0.0
        }  # !/usr/bin/env python3


# -*- coding: utf-8 -*-
"""
ARQV30 Enhanced v3.0 - External LLM Reasoning Service
Servi√ßo de racioc√≠nio com LLMs para an√°lise aprofundada
"""


# Try to import LLM clients, fallback gracefully
try:
    import google.generativeai as genai
    GEMINI_AVAILABLE = True
except ImportError:
    GEMINI_AVAILABLE = False

try:
    import openai
    OPENAI_AVAILABLE = True
except ImportError:
    OPENAI_AVAILABLE = False

logger = logging.getLogger(__name__)


class ExternalLLMReasoningService:
    """Servi√ßo de racioc√≠nio com LLMs externo independente"""

    def __init__(self, config: Dict[str, Any]):
        """Inicializa o servi√ßo de LLM"""
        self.config = config.get('llm_reasoning', {})
        self.enabled = self.config.get('enabled', True)
        self.provider = self.config.get('provider', 'gemini').lower()
        self.model = self.config.get('model', 'gemini-1.5-flash')
        self.max_tokens = self.config.get('max_tokens', 1000)
        self.temperature = self.config.get('temperature', 0.3)
        self.confidence_threshold = self.config.get(
            'confidence_threshold', 0.6)

        self.client = None
        self._initialize_llm_client()

        logger.info(
            f"‚úÖ External LLM Reasoning Service inicializado (Provider: {self.provider}, Available: {self.client is not None})")

    def _initialize_llm_client(self):
        """Inicializa o cliente LLM baseado no provider configurado"""
        try:
            if self.provider == 'gemini' and GEMINI_AVAILABLE:
                api_key = os.getenv('GEMINI_API_KEY')
                if api_key:
                    genai.configure(api_key=api_key)
                    self.client = genai.GenerativeModel(self.model)
                    logger.info(f"‚úÖ Gemini client inicializado: {self.model}")
                else:
                    logger.warning("‚ö†Ô∏è GEMINI_API_KEY n√£o configurada")

            elif self.provider == 'openai' and OPENAI_AVAILABLE:
                api_key = os.getenv('OPENAI_API_KEY')
                if api_key:
                    openai.api_key = api_key
                    self.client = openai
                    logger.info(f"‚úÖ OpenAI client inicializado: {self.model}")
                else:
                    logger.warning("‚ö†Ô∏è OPENAI_API_KEY n√£o configurada")
            else:
                logger.warning(
                    f"‚ö†Ô∏è Provider '{self.provider}' n√£o dispon√≠vel ou n√£o configurado")

        except Exception as e:
            logger.error(f"Erro ao inicializar LLM client: {e}")
            self.client = None

    def analyze_with_llm(self, text: str, context: str = "") -> Dict[str, Any]:
        """
        Analisa o texto com LLM para racioc√≠nio aprofundado

        Args:
            text (str): Texto para an√°lise
            context (str): Contexto adicional

        Returns:
            Dict[str, Any]: Resultados da an√°lise LLM
        """
        if not self.enabled or not self.client or not text or not text.strip():
            return self._get_default_result()

        try:
            # Prepare prompt for analysis
            prompt = self._create_analysis_prompt(text, context)

            # Get LLM response
            if self.provider == 'gemini':
                response = self._analyze_with_gemini(prompt)
            elif self.provider == 'openai':
                response = self._analyze_with_openai(prompt)
            else:
                return self._get_default_result()

            # Parse and structure response
            analysis_result = self._parse_llm_response(response, text)

            logger.debug(
                f"LLM analysis completed: confidence={analysis_result.get('llm_confidence', 0):.3f}")

            return analysis_result

        except Exception as e:
            logger.error(f"Erro na an√°lise LLM: {e}")
            return self._get_default_result()

    def _create_analysis_prompt(self, text: str, context: str = "") -> str:
        """Cria o prompt para an√°lise LLM"""
        base_prompt = f"""Analise o seguinte texto de forma cr√≠tica e objetiva:

TEXTO PARA AN√ÅLISE:
\"{text}\"

{f'CONTEXTO ADICIONAL: {context}' if context else ''}

Por favor, forne√ßa uma an√°lise estruturada e detalhada, avaliando os seguintes aspectos:

1.  **QUALIDADE DO CONTE√öDO (0-10):**
    -   **Clareza e Coer√™ncia:** A informa√ß√£o √© apresentada de forma l√≥gica e f√°cil de entender?
    -   **Profundidade e Abrang√™ncia:** O t√≥pico √© abordado de maneira completa ou superficial?
    -   **Originalidade:** O conte√∫do oferece novas perspectivas ou √© uma repeti√ß√£o de informa√ß√µes existentes?

2.  **CONFIABILIDADE E FONTES (0-10):**
    -   **Verificabilidade:** As afirma√ß√µes podem ser verificadas? H√° links ou refer√™ncias para fontes prim√°rias ou secund√°rias respeit√°veis?
    -   **Atualidade:** A informa√ß√£o est√° atualizada? H√° datas de publica√ß√£o ou revis√£o?
    -   **Reputa√ß√£o da Fonte:** A fonte (autor, ve√≠culo) √© conhecida por sua credibilidade e precis√£o?

3.  **VI√âS E PARCIALIDADE (0-10, onde 0=neutro, 10=muito tendencioso):**
    -   **Linguagem:** H√° uso de linguagem emotiva, carregada, ou termos que buscam influenciar a opini√£o do leitor?
    -   **Perspectiva:** O conte√∫do apresenta m√∫ltiplos lados de uma quest√£o ou foca apenas em uma vis√£o unilateral?
    -   **Omisso:** H√° informa√ß√µes relevantes que foram intencionalmente omitidas para favorecer uma narrativa?
    -   **Generaliza√ß√µes:** H√° uso de generaliza√ß√µes excessivas ou estere√≥tipos?

4.  **RISCO DE DESINFORMA√á√ÉO (0-10):**
    -   **Fatos:** H√° afirma√ß√µes factuais que s√£o comprovadamente falsas ou enganosas?
    -   **Padr√µes:** O conte√∫do exibe padr√µes conhecidos de desinforma√ß√£o (ex: clickbait, teorias da conspira√ß√£o, manipula√ß√£o de imagens/v√≠deos)?
    -   **Contexto:** O conte√∫do √© apresentado fora de contexto para alterar sua percep√ß√£o?

5.  **RECOMENDA√á√ÉO FINAL:**
    -   **Status:** [APROVAR/REJEITAR/REVIS√ÉO_MANUAL]
    -   **Raz√£o Principal:** [breve justificativa para o status]

6.  **CONFIAN√áA DA AN√ÅLISE DO LLM:**
    -   **Pontua√ß√£o:** [0-100]% - [justificativa da confian√ßa do pr√≥prio LLM na sua an√°lise]

Forne√ßa sua resposta estritamente no seguinte formato, com cada item em uma nova linha:
QUALIDADE: [pontua√ß√£o]/10 - [breve justificativa]
CONFIABILIDADE: [pontua√ß√£o]/10 - [breve justificativa]
VI√âS: [pontua√ß√£o]/10 - [breve justificativa]
DESINFORMA√á√ÉO: [pontua√ß√£o]/10 - [breve justificativa]
RECOMENDA√á√ÉO: [APROVAR/REJEITAR/REVIS√ÉO_MANUAL] - [raz√£o principal]
CONFIAN√áA_AN√ÅLISE: [0-100]% - [justificativa da confian√ßa]"""

        return base_prompt

    def _analyze_with_gemini(self, prompt: str) -> str:
        """An√°lise com Gemini"""
        try:
            response = self.client.generate_content(
                prompt,
                generation_config=genai.types.GenerationConfig(
                    max_output_tokens=self.max_tokens,
                    temperature=self.temperature
                )
            )
            return response.text
        except Exception as e:
            logger.error(f"Erro no Gemini: {e}")
            raise

    def _analyze_with_openai(self, prompt: str) -> str:
        """An√°lise com OpenAI"""
        try:
            response = openai.ChatCompletion.create(
                model=self.model,
                messages=[{"role": "user", "content": prompt}],
                max_tokens=self.max_tokens,
                temperature=self.temperature
            )
            return response.choices[0].message.content
        except Exception as e:
            logger.error(f"Erro no OpenAI: {e}")
            raise

    def _parse_llm_response(self, response: str, original_text: str) -> Dict[str, Any]:
        """Parse da resposta LLM para formato estruturado"""
        try:
            import re

            # Initialize result structure
            result = {
                'llm_response': response,
                'quality_score': 5.0,
                'reliability_score': 5.0,
                'bias_score': 5.0,
                'disinformation_score': 5.0,
                'llm_recommendation': 'REVIS√ÉO_MANUAL',
                'llm_confidence': 0.5,
                'analysis_reasoning': '',
                'provider': self.provider,
                'model': self.model
            }

            # Extract scores using regex
            patterns = {
                'quality_score': r'QUALIDADE:\s*([0-9]+(?:\.[0-9]+)?)',
                'reliability_score': r'CONFIABILIDADE:\s*([0-9]+(?:\.[0-9]+)?)',
                'bias_score': r'VI√âS:\s*([0-9]+(?:\.[0-9]+)?)',
                'disinformation_score': r'DESINFORMA√á√ÉO:\s*([0-9]+(?:\.[0-9]+)?)',
                'llm_recommendation': r'RECOMENDA√á√ÉO:\s*(APROVAR|REJEITAR|REVIS√ÉO_MANUAL)',
                'llm_confidence': r'CONFIAN√áA_AN√ÅLISE:\s*([0-9]+)%?'
            }

            for key, pattern in patterns.items():
                match = re.search(pattern, response, re.IGNORECASE)
                if match:
                    if key == 'llm_confidence':
                        result[key] = min(float(match.group(1)) / 100.0, 1.0)
                    elif key == 'llm_recommendation':
                        result[key] = match.group(1).upper()
                    else:
                        # Convert 0-10 scores to 0-1 range
                        score = min(float(match.group(1)) / 10.0, 1.0)
                        result[key] = score

            # Extract reasoning from response
            reasoning_parts = []
            for line in response.split('\\n'):
                if ' - ' in line and any(keyword in line.upper() for keyword in ['QUALIDADE', 'CONFIABILIDADE', 'VI√âS', 'DESINFORMA√á√ÉO']):
                    reasoning_parts.append(line.split(' - ', 1)[-1])

            result['analysis_reasoning'] = ' | '.join(reasoning_parts)

            # Validate and adjust confidence based on consistency
            result['llm_confidence'] = self._validate_llm_confidence(result)

            return result

        except Exception as e:
            logger.warning(f"Erro no parsing da resposta LLM: {e}")
            # Return response as-is with default scores
            return {
                'llm_response': response,
                'quality_score': 0.5,
                'reliability_score': 0.5,
                'bias_score': 0.5,
                'disinformation_score': 0.5,
                'llm_recommendation': 'REVIS√ÉO_MANUAL',
                'llm_confidence': 0.3,
                'analysis_reasoning': 'Erro no parsing da resposta',
                'provider': self.provider,
                'model': self.model
            }

    def _validate_llm_confidence(self, result: Dict[str, Any]) -> float:
        """Valida e ajusta a confian√ßa baseada na consist√™ncia da an√°lise"""
        try:
            # Check consistency between recommendation and scores
            quality = result.get('quality_score', 0.5)
            reliability = result.get('reliability_score', 0.5)
            bias = result.get('bias_score', 0.5)
            disinformation = result.get('disinformation_score', 0.5)
            recommendation = result.get('llm_recommendation', 'REVIS√ÉO_MANUAL')
            base_confidence = result.get('llm_confidence', 0.5)

            # Calculate expected recommendation based on scores
            avg_positive_scores = (quality + reliability) / 2.0
            avg_negative_scores = (bias + disinformation) / 2.0

            expected_approval = avg_positive_scores > 0.7 and avg_negative_scores < 0.4
            expected_rejection = avg_positive_scores < 0.4 or avg_negative_scores > 0.6

            # Check consistency
            consistency_bonus = 0.0
            if recommendation == 'APROVAR' and expected_approval:
                consistency_bonus = 0.1
            elif recommendation == 'REJEITAR' and expected_rejection:
                consistency_bonus = 0.1
            elif recommendation == 'REVIS√ÉO_MANUAL':
                consistency_bonus = 0.05  # Neutral is always somewhat consistent

            # Adjust confidence
            adjusted_confidence = min(base_confidence + consistency_bonus, 1.0)
            adjusted_confidence = max(adjusted_confidence, 0.1)

            return adjusted_confidence

        except Exception as e:
            logger.warning(f"Erro na valida√ß√£o de confian√ßa: {e}")
            return 0.5

    def _get_default_result(self) -> Dict[str, Any]:
        """Retorna resultado padr√£o quando LLM n√£o est√° dispon√≠vel"""
        return {
            'llm_response': 'LLM n√£o dispon√≠vel ou configurado',
            'quality_score': 0.5,
            'reliability_score': 0.5,
            'bias_score': 0.5,
            'disinformation_score': 0.5,
            'llm_recommendation': 'REVIS√ÉO_MANUAL',
            'llm_confidence': 0.1,
            'analysis_reasoning': 'An√°lise LLM n√£o dispon√≠vel',
            'provider': self.provider,
            'model': self.model
        }  # !/usr/bin/env python3


# -*- coding: utf-8 -*-
"""
ARQV30 Enhanced v3.0 - External Review Agent
Agente principal de revis√£o externa - ponto de entrada do m√≥dulo
"""


# Handle both relative and absolute imports

logger = logging.getLogger(__name__)


class ExternalReviewAgent:
    """Agente de revis√£o externa - orquestrador principal do m√≥dulo"""

    def __init__(self, config: Optional[Dict[str, Any]] = None, config_path: Optional[str] = None):
        """
        Inicializa o agente de revis√£o externa

        Args:
            config_path (Optional[str]): Caminho para arquivo de configura√ß√£o
        """
        # Inicializar logger da inst√¢ncia
        self.logger = logging.getLogger(
            f"{__name__}.{self.__class__.__name__}")

        if config is not None:
            self.config = config
        else:
            self.config = self._load_config(config_path)

        # Initialize all analysis services
        self.sentiment_analyzer = ExternalSentimentAnalyzer(self.config)
        self.bias_detector = ExternalBiasDisinformationDetector(self.config)
        self.llm_service = ExternalLLMReasoningService(self.config)
        self.rule_engine = ExternalRuleEngine(self.config)
        self.contextual_analyzer = ExternalContextualAnalyzer(self.config)
        self.confidence_thresholds = ExternalConfidenceThresholds(self.config)

        # Processing statistics
        self.stats = {
            'total_processed': 0,
            'approved': 0,
            'rejected': 0,
            'start_time': datetime.now(),
            'processing_times': []
        }

        logger.info(f"‚úÖ External Review Agent inicializado com sucesso")
        logger.info(f"üîß Configura√ß√µes carregadas: {len(self.config)} se√ß√µes")

    def _load_config(self, config_path: Optional[str] = None) -> Dict[str, Any]:
        """Carrega configura√ß√£o do m√≥dulo"""
        try:
            # Default config path
            if config_path is None:
                current_dir = os.path.dirname(os.path.abspath(__file__))
                config_path = os.path.join(
                    current_dir, '..', 'config', 'default_config.yaml')

            # Load configuration file
            if os.path.exists(config_path):
                with open(config_path, 'r', encoding='utf-8') as f:
                    if config_path.endswith('.yaml') or config_path.endswith('.yml'):
                        config = yaml.safe_load(f)
                    else:
                        config = json.load(f)
                logger.info(f"‚úÖ Configura√ß√£o carregada: {config_path}")
                return config
            else:
                logger.warning(
                    f"‚ö†Ô∏è Arquivo de configura√ß√£o n√£o encontrado: {config_path}")
                return self._get_default_config()

        except Exception as e:
            logger.error(f"Erro ao carregar configura√ß√£o: {e}")
            return self._get_default_config()

    def _get_default_config(self) -> Dict[str, Any]:
        """Retorna configura√ß√£o padr√£o"""
        return {
            'thresholds': {
                'approval': 0.75,
                'rejection': 0.35,
                'high_confidence': 0.85,
                'low_confidence': 0.5,
                'bias_high_risk': 0.7
            },
            'sentiment_analysis': {'enabled': True},
            'bias_detection': {'enabled': True},
            'llm_reasoning': {'enabled': True},
            'contextual_analysis': {'enabled': True},
            'rules': []
        }

    def process_item(self, item_data: Dict[str, Any], massive_data: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:
        """
        Processa um item individual atrav√©s de todas as an√°lises com valida√ß√£o aprimorada

        Args:
            item_data (Dict[str, Any]): Dados do item para an√°lise
            massive_data (Optional[Dict[str, Any]]): Contexto adicional

        Returns:
            Dict[str, Any]: Resultado completo da an√°lise
        """
        start_time = datetime.now()

        try:
            item_id = item_data.get(
                'id', f'item_{self.stats["total_processed"]}')
            logger.info(f"üîç Iniciando an√°lise do item: {item_id}")

            # Valida√ß√£o pr√©via do item
            validation_result = self._validate_item_data(item_data)
            if not validation_result['valid']:
                logger.warning(
                    f"‚ö†Ô∏è Item inv√°lido: {validation_result['reason']}")
                return self._create_validation_error_result(item_data, validation_result['reason'])

            # Extract text content for analysis
            text_content = self._extract_text_content(item_data)

            if not text_content or len(text_content.strip()) < 5:
                logger.warning("‚ö†Ô∏è Item com conte√∫do textual insuficiente")
                return self._create_insufficient_content_result(item_data)

            # Initialize analysis results
            analysis_result = {
                'item_id': item_data.get('id', f'item_{self.stats["total_processed"]}'),
                'original_item': item_data,
                'processing_timestamp': start_time.isoformat(),
                # First 500 chars for reference
                'text_analyzed': text_content[:500],
            }

            # Step 1: Sentiment Analysis
            logger.debug("Executando an√°lise de sentimento...")
            sentiment_result = self.sentiment_analyzer.analyze_sentiment(
                text_content)
            analysis_result['sentiment_analysis'] = sentiment_result

            # Step 2: Bias & Disinformation Detection
            logger.debug("Executando detec√ß√£o de vi√©s/desinforma√ß√£o...")
            bias_result = self.bias_detector.detect_bias_disinformation(
                text_content)
            analysis_result['bias_disinformation_analysis'] = bias_result

            # Step 3: LLM Reasoning (for ambiguous cases)
            should_use_llm = self._should_use_llm_analysis(
                sentiment_result, bias_result)
            if should_use_llm:
                logger.debug("Executando an√°lise LLM...")
                context = self._create_llm_context(
                    analysis_result, massive_data)
                llm_result = self.llm_service.analyze_with_llm(
                    text_content, context)
                analysis_result['llm_reasoning_analysis'] = llm_result
            else:
                analysis_result['llm_reasoning_analysis'] = {
                    'llm_confidence': 0.5,
                    'llm_recommendation': 'N√ÉO_EXECUTADO',
                    'analysis_reasoning': 'LLM n√£o necess√°rio para este item'
                }

            # Step 4: Contextual Analysis
            logger.debug("Executando an√°lise contextual...")
            contextual_result = self.contextual_analyzer.analyze_context(
                item_data, massive_data)
            analysis_result['contextual_analysis'] = contextual_result

            # Step 5: Rule Engine Application
            logger.debug("Aplicando regras de neg√≥cio...")
            rule_result = self.rule_engine.apply_rules(analysis_result)
            analysis_result['rule_decision'] = rule_result

            # Step 6: Final Decision
            final_decision = self._make_final_decision(analysis_result)
            analysis_result['ai_review'] = final_decision

            # Update statistics
            processing_time = (datetime.now() - start_time).total_seconds()
            self._update_stats(final_decision['status'], processing_time)

            analysis_result['processing_time_seconds'] = processing_time

            logger.info(
                f"‚úÖ Item processado: {final_decision['status']} (confian√ßa: {final_decision['final_confidence']:.3f})")

            return analysis_result

        except Exception as e:
            logger.error(f"Erro no processamento do item: {e}")
            error_result = self._create_error_result(item_data, str(e))
            self._update_stats(
                'error', (datetime.now() - start_time).total_seconds())
            return error_result

    def _validate_item_data(self, item_data: Dict[str, Any]) -> Dict[str, Any]:
        """Valida dados do item antes do processamento"""
        if not isinstance(item_data, dict):
            return {'valid': False, 'reason': 'Item deve ser um dicion√°rio'}

        if not item_data:
            return {'valid': False, 'reason': 'Item vazio'}

        # Verificar se tem pelo menos um campo de conte√∫do
        content_fields = ['content', 'text', 'title',
                          'description', 'summary', 'body']
        has_content = any(
            field in item_data and item_data[field] for field in content_fields)

        if not has_content:
            return {'valid': False, 'reason': 'Item n√£o possui conte√∫do textual v√°lido'}

        return {'valid': True, 'reason': 'Item v√°lido'}

    def _extract_text_content(self, item_data: Dict[str, Any]) -> str:
        """Extrai conte√∫do textual do item com prioriza√ß√£o"""
        # Campos priorizados por import√¢ncia
        priority_fields = ['content', 'text',
                           'description', 'summary', 'title', 'body']

        text_parts = []
        for field in priority_fields:
            if field in item_data and item_data[field]:
                content = str(item_data[field]).strip()
                if content:
                    text_parts.append(content)

        # Adicionar campos extras se existirem
        extra_fields = ['subtitle', 'excerpt', 'abstract', 'caption']
        for field in extra_fields:
            if field in item_data and item_data[field]:
                content = str(item_data[field]).strip()
                if content:
                    text_parts.append(content)

        return ' '.join(text_parts).strip()

    def _should_use_llm_analysis(self, sentiment_result: Dict[str, Any], bias_result: Dict[str, Any]) -> bool:
        """Determina se deve usar an√°lise LLM"""
        # Use LLM for ambiguous cases or high-risk content
        sentiment_confidence = sentiment_result.get('confidence', 0.5)
        bias_risk = bias_result.get('overall_risk', 0.0)

        # Low confidence sentiment or high bias risk = use LLM
        return sentiment_confidence < 0.6 or bias_risk > 0.4

    def _create_llm_context(self, analysis_result: Dict[str, Any], massive_data: Optional[Dict[str, Any]]) -> str:
        """Cria contexto para an√°lise LLM"""
        context_parts = []

        # Add sentiment context
        sentiment = analysis_result.get('sentiment_analysis', {})
        if sentiment.get('classification') != 'neutral':
            context_parts.append(
                f"Sentimento detectado: {sentiment.get('classification', 'indefinido')}")

        # Add bias context
        bias = analysis_result.get('bias_disinformation_analysis', {})
        if bias.get('overall_risk', 0) > 0.3:
            context_parts.append(
                f"Risco de vi√©s detectado: {bias.get('overall_risk', 0):.2f}")

        # Add any available external context
        if massive_data:
            if 'topic' in massive_data:
                context_parts.append(f"T√≥pico: {massive_data['topic']}")

        return ' | '.join(context_parts) if context_parts else ""

    def _make_final_decision(self, analysis_result: Dict[str, Any]) -> Dict[str, Any]:
        """Toma decis√£o final baseada em todas as an√°lises"""
        try:
            # Get analysis results
            sentiment = analysis_result.get('sentiment_analysis', {})
            bias = analysis_result.get('bias_disinformation_analysis', {})
            llm = analysis_result.get('llm_reasoning_analysis', {})
            contextual = analysis_result.get('contextual_analysis', {})
            rule_decision = analysis_result.get('rule_decision', {})

            # Calculate composite confidence
            confidences = [
                sentiment.get('confidence', 0.5) * 0.2,  # 20% weight
                (1.0 - bias.get('overall_risk', 0.5)) *
                0.3,  # 30% weight (inverted risk)
                llm.get('llm_confidence', 0.5) * 0.3,  # 30% weight
                contextual.get('contextual_confidence', 0.5) *
                0.2  # 20% weight
            ]

            final_confidence = sum(confidences)

            # Apply rule engine decision if applicable
            if rule_decision.get('status') in ['approved', 'rejected']:
                status = rule_decision['status']
                reason = rule_decision['reason']
            else:
                # Use confidence thresholds for decision
                if self.confidence_thresholds.should_approve(final_confidence):
                    status = 'approved'
                    reason = 'Aprovado com base na an√°lise combinada'
                elif self.confidence_thresholds.should_reject(final_confidence):
                    status = 'rejected'
                    reason = 'Rejeitado com base na an√°lise combinada'
                else:
                    # Default to rejection for ambiguous cases (safer)
                    status = 'rejected'
                    reason = 'Rejeitado por ambiguidade - pol√≠tica de seguran√ßa'

            # Create comprehensive decision result
            decision = {
                'status': status,
                'reason': reason,
                'final_confidence': final_confidence,
                'confidence_breakdown': {
                    'sentiment_contribution': sentiment.get('confidence', 0.5) * 0.2,
                    'bias_contribution': (1.0 - bias.get('overall_risk', 0.5)) * 0.3,
                    'llm_contribution': llm.get('llm_confidence', 0.5) * 0.3,
                    'contextual_contribution': contextual.get('contextual_confidence', 0.5) * 0.2
                },
                'decision_factors': {
                    'sentiment_classification': sentiment.get('classification', 'neutral'),
                    'bias_risk_level': 'high' if bias.get('overall_risk', 0) > 0.6 else 'medium' if bias.get('overall_risk', 0) > 0.3 else 'low',
                    'llm_recommendation': llm.get('llm_recommendation', 'N√ÉO_EXECUTADO'),
                    'rule_triggered': rule_decision.get('triggered_rules', [])
                },
                'analysis_summary': {
                    'total_flags': (
                        len(bias.get('detected_bias_keywords', [])) +
                        len(bias.get('detected_disinformation_patterns', [])) +
                        len(contextual.get('context_flags', []))
                    ),
                    'sentiment_polarity': sentiment.get('polarity', 0.0),
                    'overall_risk_score': bias.get('overall_risk', 0.0),
                    'contextual_consistency': contextual.get('consistency_score', 0.5)
                },
                'processing_metadata': {
                    'timestamp': datetime.now().isoformat(),
                    'version': '3.0.0',
                    'confidence_threshold_used': self.confidence_thresholds.get_threshold('approval')
                }
            }

            return decision

        except Exception as e:
            logger.error(f"Erro na decis√£o final: {e}")
            return {
                'status': 'rejected',
                'reason': f'Erro no processamento: {str(e)}',
                'final_confidence': 0.0,
                'error': True
            }

    def _create_validation_error_result(self, item_data: Dict[str, Any], reason: str) -> Dict[str, Any]:
        """Cria resultado para item que falhou na valida√ß√£o"""
        return {
            'item_id': item_data.get('id', 'sem_id'),
            'original_item': item_data,
            'processing_timestamp': datetime.now().isoformat(),
            'ai_review': {
                'status': 'rejected',
                'reason': f'Erro de valida√ß√£o: {reason}',
                'final_confidence': 0.0,
                'error': True,
                'validation_error': True
            },
            'sentiment_analysis': {},
            'bias_disinformation_analysis': {},
            'processing_time_seconds': 0.0
        }

    def _create_insufficient_content_result(self, item_data: Dict[str, Any]) -> Dict[str, Any]:
        """Cria resultado para item com conte√∫do insuficiente"""
        return {
            'item_id': item_data.get('id', 'sem_id'),
            'original_item': item_data,
            'processing_timestamp': datetime.now().isoformat(),
            'ai_review': {
                'status': 'rejected',
                'reason': 'Conte√∫do textual insuficiente para an√°lise',
                'final_confidence': 0.0,
                'error': False,
                'insufficient_content': True
            },
            'sentiment_analysis': {},
            'bias_disinformation_analysis': {},
            'llm_reasoning_analysis': {},
            # 'contextual_analysis': {},
            'processing_time_seconds': 0.0
        }

    def _create_error_result(self, item_data: Dict[str, Any], error_message: str) -> Dict[str, Any]:
        """Cria resultado para erro de processamento"""
        return {
            'item_id': item_data.get('id', 'sem_id'),
            'original_item': item_data,
            'processing_timestamp': datetime.now().isoformat(),
            'ai_review': {
                'status': 'rejected',
                'reason': f'Erro no processamento: {error_message}',
                'final_confidence': 0.0,
                'error': True
            },
            'error_details': error_message,
            'processing_time_seconds': 0.0
        }

    def _update_stats(self, status: str, processing_time: float):
        """Atualiza estat√≠sticas de processamento"""
        self.stats['total_processed'] += 1
        self.stats['processing_times'].append(processing_time)

        if status == 'approved':
            self.stats['approved'] += 1
        elif status == 'rejected':
            self.stats['rejected'] += 1

    def get_statistics(self) -> Dict[str, Any]:
        """Retorna estat√≠sticas de processamento"""
        total_time = (datetime.now() -
                      self.stats['start_time']).total_seconds()
        avg_processing_time = sum(self.stats['processing_times']) / len(
            self.stats['processing_times']) if self.stats['processing_times'] else 0

        return {
            'total_processed': self.stats['total_processed'],
            'approved': self.stats['approved'],
            'rejected': self.stats['rejected'],
            'approval_rate': self.stats['approved'] / max(self.stats['total_processed'], 1),
            'total_runtime_seconds': total_time,
            'average_processing_time_seconds': avg_processing_time,
            'items_per_second': self.stats['total_processed'] / max(total_time, 1)
        }

    def find_consolidacao_file(self, session_id: str) -> Optional[str]:
        """Busca automaticamente o arquivo de consolida√ß√£o da etapa 1 para a sess√£o especificada"""
        try:
            # Diret√≥rio base onde os arquivos s√£o salvos
            base_paths = [
                f"../src/relatorios_intermediarios/workflow/{session_id}",
                f"src/relatorios_intermediarios/workflow/{session_id}",
                f"relatorios_intermediarios/workflow/{session_id}",
                f"../relatorios_intermediarios/workflow/{session_id}"
            ]

            for base_path in base_paths:
                if os.path.exists(base_path):
                    # Busca por arquivos de consolida√ß√£o
                    pattern = f"{base_path}/consolidacao_etapa1_final_*.json"
                    files = glob.glob(pattern)

                    if files:
                        # Pega o arquivo mais recente
                        latest_file = max(files, key=os.path.getmtime)
                        self.logger.info(
                            f"‚úÖ Arquivo de consolida√ß√£o encontrado: {latest_file}")
                        return latest_file

            # Se n√£o encontrou, busca em todo o projeto
            search_patterns = [
                f"**/consolidacao_etapa1_final_*{session_id}*.json",
                f"**/consolidacao_etapa1_final_*.json"
            ]

            for pattern in search_patterns:
                files = glob.glob(pattern, recursive=True)
                if files:
                    # Filtra por sess√£o se poss√≠vel
                    session_files = [f for f in files if session_id in f]
                    if session_files:
                        latest_file = max(session_files, key=os.path.getmtime)
                        self.logger.info(
                            f"‚úÖ Arquivo de consolida√ß√£o encontrado (busca recursiva): {latest_file}")
                        return latest_file
                    else:
                        # Pega o mais recente se n√£o conseguir filtrar por sess√£o
                        latest_file = max(files, key=os.path.getmtime)
                        self.logger.info(
                            f"‚ö†Ô∏è Usando arquivo mais recente: {latest_file}")
                        return latest_file

            self.logger.warning(
                f"‚ùå Nenhum arquivo de consolida√ß√£o encontrado para sess√£o {session_id}")
            return None

        except Exception as e:
            self.logger.error(f"‚ùå Erro ao buscar arquivo de consolida√ß√£o: {e}")
            return None

    def load_consolidacao_data(self, session_id: str) -> Optional[Dict[str, Any]]:
        """Carrega dados do arquivo de consolida√ß√£o da etapa 1"""
        try:
            file_path = self.find_consolidacao_file(session_id)

            if not file_path:
                return None

            with open(file_path, 'r', encoding='utf-8') as f:
                data = json.load(f)

            self.logger.info(
                f"üìÑ Dados de consolida√ß√£o carregados: {len(data.get('data', {}).get('dados_web', []))} itens web")
            return data

        except Exception as e:
            self.logger.error(f"‚ùå Erro ao carregar dados de consolida√ß√£o: {e}")
            return None

    def convert_consolidacao_to_analysis_format(self, consolidacao_data: Dict[str, Any], session_id: str) -> Dict[str, Any]:
        """Converte dados de consolida√ß√£o para formato de an√°lise do External AI Verifier"""
        try:
            if not consolidacao_data:
                return {'items': [], 'context': {}}

            data_section = consolidacao_data.get('data', {})
            dados_web = data_section.get('dados_web', [])

            # Converte cada item para o formato esperado
            items = []
            for idx, item in enumerate(dados_web):
                converted_item = {
                    'id': f"web_{idx+1:03d}",
                    'content': item.get('titulo', ''),
                    'title': item.get('titulo', ''),
                    'source': item.get('url', ''),
                    'url': item.get('url', ''),
                    'author': item.get('fonte', 'Desconhecido'),
                    'timestamp': datetime.now().isoformat(),
                    'category': 'web_content',
                    'relevancia': item.get('relevancia', 0.5),
                    'conteudo_tamanho': item.get('conteudo_tamanho', 0),
                    'engagement': item.get('engagement', {}),
                    'metadata': {
                        'session_id': session_id,
                        'fonte_original': item.get('fonte', ''),
                        'tipo_dado': data_section.get('tipo', ''),
                        'processado_em': datetime.now().isoformat()
                    }
                }
                items.append(converted_item)

            # Contextualiza a an√°lise
            context = {
                'topic': data_section.get('tipo', 'analise_dados_web'),
                'analysis_type': 'verificacao_consolidacao_etapa1',
                'session_id': session_id,
                'source_file': 'consolidacao_etapa1',
                'total_items_originais': len(dados_web),
                'processamento_timestamp': datetime.now().isoformat(),
                **self.config.get('context', {})
            }

            self.logger.info(
                f"‚úÖ Convers√£o conclu√≠da: {len(items)} itens preparados para an√°lise")

            return {
                'items': items,
                'context': context
            }

        except Exception as e:
            self.logger.error(
                f"‚ùå Erro ao converter dados de consolida√ß√£o: {e}")
            return {'items': [], 'context': {}}

    def analyze_session_consolidacao(self, session_id: str) -> Dict[str, Any]:
        """Analisa automaticamente os dados de consolida√ß√£o de uma sess√£o"""
        try:
            self.logger.info(
                f"üîç Iniciando an√°lise da consolida√ß√£o para sess√£o: {session_id}")

            # Carrega dados de consolida√ß√£o
            consolidacao_data = self.load_consolidacao_data(session_id)

            if not consolidacao_data:
                return {
                    'success': False,
                    'error': f'Arquivo de consolida√ß√£o n√£o encontrado para sess√£o {session_id}',
                    'session_id': session_id,
                    'timestamp': datetime.now().isoformat()
                }

            # Converte para formato de an√°lise
            analysis_data = self.convert_consolidacao_to_analysis_format(
                consolidacao_data, session_id)

            if not analysis_data.get('items'):
                return {
                    'success': False,
                    'error': 'Nenhum item v√°lido encontrado nos dados de consolida√ß√£o',
                    'session_id': session_id,
                    'timestamp': datetime.now().isoformat()
                }

            # Executa an√°lise
            result = self.analyze_content_batch(analysis_data)

            # Adiciona informa√ß√µes da sess√£o ao resultado
            result['session_analysis'] = {
                'session_id': session_id,
                'consolidacao_source': True,
                'items_analisados': len(analysis_data.get('items', [])),
                'timestamp': datetime.now().isoformat()
            }

            return result

        except Exception as e:
            self.logger.error(f"‚ùå Erro na an√°lise da sess√£o {session_id}: {e}")
            return {
                'success': False,
                'error': str(e),
                'session_id': session_id,
                'timestamp': datetime.now().isoformat()
            }

    def process_batch(self, items: List[Dict[str, Any]], massive_data: Optional[Dict[str, Any]] = None,
                      batch_size: int = 10) -> Dict[str, Any]:
        """
        Processa uma lista de itens em lotes para melhor performance

        Args:
            items: Lista de itens para processar
            massive_data: Contexto adicional
            batch_size: Tamanho do lote para processamento

        Returns:
            Dict com resultados do processamento em lote
        """
        logger.info(
            f"üöÄ Iniciando processamento em lote: {len(items)} itens, lotes de {batch_size}")

        all_results = []
        approved_items = []
        rejected_items = []

        # Calcular total de lotes antecipadamente
        total_batches = (len(items) + batch_size - 1) // batch_size

        # Processar em lotes
        for i in range(0, len(items), batch_size):
            batch = items[i:i + batch_size]
            batch_num = (i // batch_size) + 1

            logger.info(
                f"üì¶ Processando lote {batch_num}/{total_batches} ({len(batch)} itens)")

            batch_results = []
            for item in batch:
                result = self.process_item(item, massive_data)
                batch_results.append(result)
                all_results.append(result)

                # Categorizar resultado
                if hasattr(result, 'get') and callable(getattr(result, 'get')):
                    status = result.get('ai_review', {}).get(
                        'status', 'rejected')
                else:
                    # Se result for uma Exception ou objeto sem get()
                    status = 'rejected'
                if status == 'approved':
                    approved_items.append(result)
                else:
                    rejected_items.append(result)

            logger.info(
                f"‚úÖ Lote {batch_num} conclu√≠do: {len([r for r in batch_results if r.get('ai_review', {}).get('status') == 'approved'])} aprovados")

        # Compilar estat√≠sticas finais
        stats = self.get_statistics()

        return {
            'all_results': all_results,
            'approved_items': approved_items,
            'rejected_items': rejected_items,
            'statistics': stats,
            'batch_info': {
                'total_items': len(items),
                'batch_size': batch_size,
                'total_batches': total_batches,
                'approved_count': len(approved_items),
                'rejected_count': len(rejected_items),
                'approval_rate': len(approved_items) / len(items) if items else 0
            },
            'metadata': {
                'timestamp': datetime.now().isoformat(),
                'version': '3.0.0',
                'processing_mode': 'batch'
            }
        }

    async def process_batch_async(self, items: List[Dict[str, Any]], massive_data: Optional[Dict[str, Any]] = None,
                                  max_concurrent: int = 5) -> Dict[str, Any]:
        """
        Processa itens de forma ass√≠ncrona para melhor performance

        Args:
            items: Lista de itens para processar
            massive_data: Contexto adicional
            max_concurrent: M√°ximo de processamentos simult√¢neos

        Returns:
            Dict com resultados do processamento ass√≠ncrono
        """
        logger.info(
            f"‚ö° Iniciando processamento ass√≠ncrono: {len(items)} itens, m√°x {max_concurrent} simult√¢neos")

        semaphore = asyncio.Semaphore(max_concurrent)

        async def process_single_item(item):
            async with semaphore:
                # Como process_item n√£o √© async, executar em thread
                loop = asyncio.get_event_loop()
                return await loop.run_in_executor(None, self.process_item, item, massive_data)

        # Processar todos os itens
        tasks = [process_single_item(item) for item in items]
        all_results = await asyncio.gather(*tasks, return_exceptions=True)

        # Processar resultados e exce√ß√µes
        valid_results = []
        approved_items = []
        rejected_items = []
        errors = []

        for i, result in enumerate(all_results):
            if isinstance(result, Exception):
                logger.error(f"‚ùå Erro no item {i}: {result}")
                errors.append({'item_index': i, 'error': str(result)})
                # Criar resultado de erro
                error_result = self._create_error_result(items[i], str(result))
                valid_results.append(error_result)
                rejected_items.append(error_result)
            else:
                valid_results.append(result)
                if hasattr(result, 'get') and callable(getattr(result, 'get')):
                    status = result.get('ai_review', {}).get(
                        'status', 'rejected')
                else:
                    # Se result for uma Exception ou objeto sem get()
                    status = 'rejected'
                if status == 'approved':
                    approved_items.append(result)
                else:
                    rejected_items.append(result)

        stats = self.get_statistics()

        return {
            'all_results': valid_results,
            'approved_items': approved_items,
            'rejected_items': rejected_items,
            'errors': errors,
            'statistics': stats,
            'async_info': {
                'total_items': len(items),
                'max_concurrent': max_concurrent,
                'approved_count': len(approved_items),
                'rejected_count': len(rejected_items),
                'error_count': len(errors),
                'approval_rate': len(approved_items) / len(items) if items else 0
            },
            'metadata': {
                'timestamp': datetime.now().isoformat(),
                'version': '3.0.0',
                'processing_mode': 'async'
            }
        }

    def analyze_content_batch(self, input_data: Dict[str, Any]) -> Dict[str, Any]:
        """Analisa lote de conte√∫do"""
        try:
            items = input_data.get('items', [])
            context = input_data.get('context', {})

            if not items:
                return {
                    'success': False,
                    'error': 'Nenhum item fornecido para an√°lise',
                    'timestamp': datetime.now().isoformat()
                }

            self.logger.info(f"üîç Iniciando an√°lise de {len(items)} itens")

            results = []
            total_items = len(items)

            for idx, item in enumerate(items):
                self.logger.info(
                    f"üìä Analisando item {idx + 1}/{total_items}: {item.get('id', 'N/A')}")

                try:
                    # Use process_item directly
                    result = self.process_item(item, context)
                    results.append(result)

                    # Pequeno delay entre an√°lises
                    if idx < total_items - 1:
                        import time
                        time.sleep(0.5)

                except Exception as e:
                    self.logger.error(
                        f"‚ùå Erro ao analisar item {item.get('id', 'N/A')}: {e}")
                    results.append({
                        'item_id': item.get('id', 'N/A'),
                        'status': 'error',
                        'error': str(e),
                        'confidence_score': 0.0
                    })

            # Gera estat√≠sticas finais
            stats = self._generate_batch_statistics(results)

            return {
                'success': True,
                'total_items': total_items,
                'results': results,
                'statistics': stats,
                'processing_metadata': {
                    'timestamp': datetime.now().isoformat(),
                    'agent_version': '3.0',
                    'batch_size': total_items
                }
            }

        except Exception as e:
            self.logger.error(f"‚ùå Erro na an√°lise em lote: {e}")
            return {
                'success': False,
                'error': str(e),
                'timestamp': datetime.now().isoformat()
            }

    def _generate_batch_statistics(self, results: List[Dict[str, Any]]) -> Dict[str, Any]:
        """Gera estat√≠sticas para uma an√°lise em lote."""
        total_items = len(results)
        approved_count = sum(1 for r in results if r.get(
            'ai_review', {}).get('status') == 'approved')
        rejected_count = total_items - approved_count
        error_count = sum(1 for r in results if r.get(
            'ai_review', {}).get('error'))

        total_processing_time = sum(
            r.get('processing_time_seconds', 0) for r in results)
        avg_processing_time = total_processing_time / \
            total_items if total_items > 0 else 0

        return {
            'total_items': total_items,
            'approved': approved_count,
            'rejected': rejected_count,
            'errors': error_count,
            'approval_rate': approved_count / total_items if total_items > 0 else 0,
            'total_processing_time_seconds': total_processing_time,
            'average_processing_time_seconds': avg_processing_time
        }


def run_external_review(input_data: Dict[str, Any], config_path: Optional[str] = None) -> Dict[str, Any]:
    """
    Fun√ß√£o principal de entrada para o m√≥dulo externo

    Args:
        input_data (Dict[str, Any]): Dados de entrada contendo itens para an√°lise
        config_path (Optional[str]): Caminho para arquivo de configura√ß√£o

    Returns:
        Dict[str, Any]: Resultados da an√°lise e itens processados
    """
    try:
        logger.info("üöÄ Iniciando External AI Verifier...")

        # Initialize review agent
        review_agent = ExternalReviewAgent(config_path)

        # Extract items to process
        items = input_data.get('items', [])
        massive_data = input_data.get('context', {})

        if not items:
            logger.warning("Nenhum item fornecido para an√°lise")
            return {
                'items': [],
                'statistics': {'total_processed': 0, 'error': 'Nenhum item fornecido'},
                'metadata': {
                    'timestamp': datetime.now().isoformat(),
                    'version': '3.0.0'
                }
            }

        logger.info(f"Processando {len(items)} itens...")

        # Process each item
        processed_items = []
        approved_items = []
        rejected_items = []

        for item in items:
            result = review_agent.process_item(item, massive_data)
            processed_items.append(result)

            # Separate approved/rejected for easier consumption
            if result['ai_review']['status'] == 'approved':
                approved_items.append(result)
            else:
                rejected_items.append(result)

        # Compile final results
        final_result = {
            'items': approved_items,  # Only approved items by default
            'all_items': processed_items,  # All items with full analysis
            'rejected_items': rejected_items,  # Rejected items separately
            'statistics': review_agent.get_statistics(),
            'metadata': {
                'timestamp': datetime.now().isoformat(),
                'version': '3.0.0',
                'total_input_items': len(items),
                'approved_count': len(approved_items),
                'rejected_count': len(rejected_items)
            }
        }

        logger.info(
            f"‚úÖ Processamento conclu√≠do: {len(approved_items)} aprovados, {len(rejected_items)} rejeitados")

        return final_result

    except Exception as e:
        logger.error(f"Erro cr√≠tico no External AI Verifier: {e}")
        return {
            'items': [],
            'statistics': {'error': str(e), 'total_processed': 0},
            'metadata': {
                'timestamp': datetime.now().isoformat(),
                'version': '3.0.0',
                'error': True
            }
        }


# Inst√¢ncia global do External AI Verifier
external_ai_verifier = ExternalReviewAgent()


# --- IN√çCIO DO C√ìDIGO DO EXTERNAL REVIEW AGENT E SUBM√ìDULOS --- #


# --- IN√çCIO DO C√ìDIGO DE rule_engine.py --- #


#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ARQV30 Enhanced v3.0 - External Rule Engine
Motor de regras para o m√≥dulo externo de verifica√ß√£o por IA
"""


logger = logging.getLogger(__name__)


class ExternalRuleEngine:
    """Motor de regras externo independente"""

    def __init__(self, config: Dict[str, Any]):
        """Inicializa o motor de regras"""
        self.rules = config.get('rules', [])

        # Ensure we have default rules if none provided
        if not self.rules:
            self.rules = self._get_default_rules()

        logger.info(
            f"‚úÖ External Rule Engine inicializado com {len(self.rules)} regras")
        self._log_rules()

    def _get_default_rules(self) -> List[Dict[str, Any]]:
        """Retorna regras padr√£o se nenhuma for configurada"""
        return [
            {
                "name": "high_confidence_approval",
                "condition": "overall_confidence >= 0.85",
                "action": {
                    "status": "approved",
                    "reason": "Alta confian√ßa no conte√∫do",
                    "confidence_adjustment": 0.0
                }
            },
            {
                "name": "low_confidence_rejection",
                "condition": "overall_confidence <= 0.35",
                "action": {
                    "status": "rejected",
                    "reason": "Confian√ßa muito baixa",
                    "confidence_adjustment": -0.1
                }
            },
            {
                "name": "high_risk_bias_rejection",
                "condition": "overall_risk >= 0.7",
                "action": {
                    "status": "rejected",
                    "reason": "Alto risco de vi√©s/desinforma√ß√£o detectado",
                    "confidence_adjustment": -0.2
                }
            },
            {
                "name": "llm_rejection_override",
                "condition": "llm_recommendation == 'REJEITAR'",
                "action": {
                    "status": "rejected",
                    "reason": "Rejeitado por an√°lise LLM",
                    "confidence_adjustment": -0.1
                }
            }
        ]

    def apply_rules(self, item_data: Dict[str, Any]) -> Dict[str, Any]:
        """
        Aplica regras aos dados do item

        Args:
            item_data (Dict[str, Any]): Dados do item para an√°lise

        Returns:
            Dict[str, Any]: Resultado da aplica√ß√£o das regras
        """
        try:
            # Initialize decision result
            decision = {
                "status": "approved",  # Default to approved if no rules trigger
                "reason": "Nenhuma regra espec√≠fica ativada",
                "confidence_adjustment": 0.0,
                "triggered_rules": []
            }

            # Extract relevant scores from item_data
            validation_scores = item_data.get("validation_scores", {})
            sentiment_analysis = item_data.get("sentiment_analysis", {})
            bias_analysis = item_data.get("bias_disinformation_analysis", {})
            llm_analysis = item_data.get("llm_reasoning_analysis", {})

            # Calculate overall confidence and risk
            overall_confidence = self._calculate_overall_confidence(
                validation_scores, sentiment_analysis, bias_analysis, llm_analysis)
            overall_risk = bias_analysis.get("overall_risk", 0.0)
            llm_recommendation = llm_analysis.get(
                "llm_recommendation", "REVIS√ÉO_MANUAL")

            # Apply each rule in order
            for rule in self.rules:
                if self._evaluate_condition(rule, overall_confidence, overall_risk, llm_recommendation, item_data):
                    rule_name = rule.get("name", "unknown_rule")
                    action = rule.get("action", {})

                    # Update decision
                    decision["status"] = action.get("status", "approved")
                    decision["reason"] = action.get(
                        "reason", f"Regra '{rule_name}' ativada")
                    decision["confidence_adjustment"] = action.get(
                        "confidence_adjustment", 0.0)
                    decision["triggered_rules"].append(rule_name)

                    logger.debug(
                        f"Regra '{rule_name}' ativada: {decision['status']} - {decision['reason']}")

                    # Stop at first matching rule (rules should be ordered by priority)
                    break

            return decision

        except Exception as e:
            logger.error(f"Erro ao aplicar regras: {e}")
            return {
                "status": "rejected",  # Fail safe - reject on error
                "reason": f"Erro no processamento de regras: {str(e)}",
                "confidence_adjustment": -0.3,
                "triggered_rules": ["error_fallback"]
            }

    def _evaluate_condition(self, rule: Dict[str, Any], overall_confidence: float, overall_risk: float, llm_recommendation: str, item_data: Dict[str, Any]) -> bool:
        """
        Avalia se a condi√ß√£o de uma regra √© atendida

        Args:
            rule: Regra para avaliar
            overall_confidence: Confian√ßa geral calculada
            overall_risk: Risco geral calculado
            llm_recommendation: Recomenda√ß√£o do LLM
            item_data: Dados completos do item

        Returns:
            bool: True se a condi√ß√£o for atendida
        """
        try:
            condition = rule.get("condition", "")

            if not condition:
                return False

            # Simple condition evaluation
            # Replace variables in condition string
            condition = condition.replace(
                "overall_confidence", str(overall_confidence))
            condition = condition.replace("overall_risk", str(overall_risk))
            condition = condition.replace(
                "llm_recommendation", f"'{llm_recommendation}'")

            # Evaluate mathematical expressions
            if any(op in condition for op in [">=", "<=", "==", ">", "<", "!="]):
                try:
                    # Safe evaluation of simple mathematical conditions
                    return self._safe_eval_condition(condition)
                except:
                    logger.warning(f"Erro ao avaliar condi√ß√£o: {condition}")
                    return False

            return False

        except Exception as e:
            logger.error(f"Erro na avalia√ß√£o da condi√ß√£o: {e}")
            return False

    def _safe_eval_condition(self, condition: str) -> bool:
        """
        Avalia condi√ß√µes matem√°ticas simples de forma segura

        Args:
            condition (str): Condi√ß√£o para avaliar

        Returns:
            bool: Resultado da avalia√ß√£o
        """
        try:
            # Only allow safe mathematical operations and comparisons
            allowed_chars = set(
                "0123456789.><=! '\'ABCDEFGHIJKLMNOPQRSTUVWXYZ_abcdefghijklmnopqrstuvwxyz")

            if not all(c in allowed_chars for c in condition):
                logger.warning(
                    f"Caracteres n√£o permitidos na condi√ß√£o: {condition}")
                return False

            # Simple string replacements for evaluation
            if ">=" in condition:
                parts = condition.split(">=")
                if len(parts) == 2:
                    try:
                        left = float(parts[0].strip())
                        right = float(parts[1].strip())
                        return left >= right
                    except ValueError:
                        # Handle string comparisons
                        return parts[0].strip() == parts[1].strip()

            elif "<=" in condition:
                parts = condition.split("<=")
                if len(parts) == 2:
                    left = float(parts[0].strip())
                    right = float(parts[1].strip())
                    return left <= right

            elif "==" in condition:
                parts = condition.split("==")
                if len(parts) == 2:
                    left = parts[0].strip().strip("'\'")
                    right = parts[1].strip().strip("'\'")
                    return left == right

            elif ">" in condition:
                parts = condition.split(">")
                if len(parts) == 2:
                    left = float(parts[0].strip())
                    right = float(parts[1].strip())
                    return left > right

            elif "<" in condition:
                parts = condition.split("<")
                if len(parts) == 2:
                    left = float(parts[0].strip())
                    right = float(parts[1].strip())
                    return left < right

            return False

        except Exception as e:
            logger.error(f"Erro na avalia√ß√£o segura da condi√ß√£o: {e}")
            return False

    def _calculate_overall_confidence(self, validation_scores: Dict[str, Any], sentiment_analysis: Dict[str, Any], bias_analysis: Dict[str, Any], llm_analysis: Dict[str, Any]) -> float:
        """Calcula confian√ßa geral baseada em todas as an√°lises"""
        try:
            # Start with base validation confidence
            base_confidence = validation_scores.get("overall_confidence", 0.5)

            # Adjust based on sentiment analysis
            sentiment_confidence = sentiment_analysis.get("confidence", 0.5)
            sentiment_weight = 0.2

            # Adjust based on bias analysis (lower bias risk = higher confidence)
            # Invert risk to confidence
            bias_confidence = 1.0 - bias_analysis.get("overall_risk", 0.5)
            bias_weight = 0.3

            # Adjust based on LLM analysis
            llm_confidence = llm_analysis.get("llm_confidence", 0.5)
            llm_weight = 0.4

            # Weighted combination
            overall_confidence = (
                base_confidence * (1.0 - sentiment_weight - bias_weight - llm_weight) +
                sentiment_confidence * sentiment_weight +
                bias_confidence * bias_weight +
                llm_confidence * llm_weight
            )

            return min(max(overall_confidence, 0.0), 1.0)

        except Exception as e:
            logger.warning(f"Erro no c√°lculo de confian√ßa geral: {e}")
            return 0.5

    def _log_rules(self):
        """Log das regras configuradas"""
        logger.debug("Regras configuradas:")
        for i, rule in enumerate(self.rules):
            logger.debug(
                f"  {i+1}. {rule.get('name', 'sem_nome')}: {rule.get('condition', 'sem_condi√ß√£o')}")

    def add_rule(self, rule: Dict[str, Any]):
        """
        Adiciona uma nova regra

        Args:
            rule (Dict[str, Any]): Nova regra para adicionar
        """
        if self._validate_rule(rule):
            self.rules.append(rule)
            logger.info(
                f"Nova regra adicionada: {rule.get('name', 'sem_nome')}")
        else:
            logger.warning(f"Regra inv√°lida rejeitada: {rule}")

    def _validate_rule(self, rule: Dict[str, Any]) -> bool:
        """Valida se uma regra est√° bem formada"""
        return (
            isinstance(rule, dict) and
            "condition" in rule and
            "action" in rule and
            isinstance(rule["action"], dict)
        )


# --- IN√çCIO DO C√ìDIGO DE sentiment_analyzer.py --- #
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ARQV30 Enhanced v3.0 - External Sentiment Analyzer
M√≥dulo independente para an√°lise de sentimento e polaridade
"""


# Try to import TextBlob, fallback if not available
try:
    from textblob import TextBlob
    TEXTBLOB_AVAILABLE = True
except ImportError:
    TEXTBLOB_AVAILABLE = False
    logging.warning("TextBlob n√£o dispon√≠vel. Usando an√°lise b√°sica.")

# Try to import VADER, fallback if not available
try:
    from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer
    VADER_AVAILABLE = True
except ImportError:
    VADER_AVAILABLE = False
    logging.warning("VADER Sentiment n√£o dispon√≠vel.")

logger = logging.getLogger(__name__)


class ExternalSentimentAnalyzer:
    """Analisador de sentimento externo independente"""

    def __init__(self, config: Dict[str, Any]):
        """Inicializa o analisador de sentimento"""
        self.config = config.get("sentiment_analysis", {})
        self.enabled = self.config.get("enabled", True)
        self.use_vader = self.config.get("use_vader", True) and VADER_AVAILABLE
        self.use_textblob = self.config.get(
            "use_textblob", True) and TEXTBLOB_AVAILABLE
        self.polarity_weights = self.config.get("polarity_weights", {
            "positive": 1.1,
            "negative": 0.8,
            "neutral": 1.0
        })

        # Initialize VADER if available and enabled
        if self.use_vader:
            self.vader_analyzer = SentimentIntensityAnalyzer()

        # Palavras para an√°lise b√°sica quando bibliotecas n√£o est√£o dispon√≠veis
        self.positive_words = {"bom", "√≥timo", "excelente", "maravilhoso", "perfeito", "incr√≠vel", "fant√°stico", "amor", "feliz", "alegre",
                               "positivo", "sucesso", "ganho", "oportunidade", "melhor", "bem", "confi√°vel", "seguro", "verdadeiro", "justo", "aprovado"}
        self.negative_words = {"ruim", "p√©ssimo", "terr√≠vel", "horr√≠vel", "odiar", "triste", "raiva", "problema", "erro", "falha", "negativo",
                               "fracasso", "perda", "amea√ßa", "pior", "mal", "duvidoso", "inseguro", "falso", "injusto", "rejeitado", "vi√©s", "desinforma√ß√£o"}

        logger.info(
            f"‚úÖ External Sentiment Analyzer inicializado (VADER: {self.use_vader}, TextBlob: {self.use_textblob})")

    def analyze_sentiment(self, text: str) -> Dict[str, float]:
        """
        Analisa o sentimento do texto fornecido

        Args:
            text (str): Texto para an√°lise

        Returns:
            Dict[str, float]: Resultados da an√°lise de sentimento
        """
        if not self.enabled or not text or not text.strip():
            return self._get_neutral_sentiment()

        try:
            # Clean text
            cleaned_text = self._clean_text(text)

            results = {
                "polarity": 0.0,
                "subjectivity": 0.0,
                "compound": 0.0,
                "positive": 0.0,
                "negative": 0.0,
                "neutral": 0.0,
                "classification": "neutral",
                "confidence": 0.0,
                "analysis_methods": []
            }

            # TextBlob Analysis
            if self.use_textblob:
                textblob_results = self._analyze_with_textblob(cleaned_text)
                results.update(textblob_results)
                results["analysis_methods"].append("textblob")

            # VADER Analysis
            if self.use_vader:
                vader_results = self._analyze_with_vader(cleaned_text)
                # Combine VADER results with TextBlob
                results["compound"] = vader_results["compound"]
                results["positive"] = vader_results["pos"]
                results["negative"] = vader_results["neg"]
                results["neutral"] = vader_results["neu"]
                results["analysis_methods"].append("vader")

                # Use VADER for final classification if available
                results["classification"] = self._classify_sentiment_vader(
                    vader_results)

            # Apply polarity weights
            results = self._apply_polarity_weights(results)

            # Calculate final confidence
            results["confidence"] = self._calculate_confidence(results)

            logger.debug(
                f"Sentiment analysis completed: {results['classification']} (confidence: {results['confidence']:.3f})")

            return results

        except Exception as e:
            logger.error(f"Erro na an√°lise de sentimento: {e}")
            return self._get_neutral_sentiment()

    def _clean_text(self, text: str) -> str:
        """Limpa o texto para an√°lise"""
        if not text:
            return ""

        # Remove URLs
        text = re.sub(
            r"http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+", "", text)

        # Remove mentions and hashtags (keep the content)
        text = re.sub(r"[@#](\w+)", r"\\1", text)

        # Remove excessive whitespace
        text = re.sub(r"\\s+", " ", text).strip()

        return text

    def _analyze_with_textblob(self, text: str) -> Dict[str, float]:
        """An√°lise com TextBlob ou an√°lise b√°sica"""
        try:
            if TEXTBLOB_AVAILABLE:
                blob = TextBlob(text)
                return {
                    "polarity": blob.sentiment.polarity,  # -1 to 1
                    "subjectivity": blob.sentiment.subjectivity  # 0 to 1
                }
            else:
                # An√°lise b√°sica usando palavras-chave
                return self._basic_sentiment_analysis(text)
        except Exception as e:
            logger.warning(f"Erro na an√°lise de sentimento: {e}")
            return {"polarity": 0.0, "subjectivity": 0.0}

    def _basic_sentiment_analysis(self, text: str) -> Dict[str, float]:
        """An√°lise b√°sica de sentimento usando palavras-chave"""
        text_lower = text.lower()
        positive_count = sum(
            1 for word in self.positive_words if word in text_lower)
        negative_count = sum(
            1 for word in self.negative_words if word in text_lower)

        # Calcular polaridade b√°sica
        total_words = len(text_lower.split())
        if total_words == 0:
            return {"polarity": 0.0, "subjectivity": 0.0}

        polarity = (positive_count - negative_count) / max(total_words, 1)
        # Normalizar entre -1 e 1
        polarity = max(-1.0, min(1.0, polarity * 10))

        # Subjetividade baseada na quantidade de palavras emocionais
        subjectivity = min((positive_count + negative_count) /
                           max(total_words, 1) * 5, 1.0)

        return {"polarity": polarity, "subjectivity": subjectivity}

    def _analyze_with_vader(self, text: str) -> Dict[str, float]:
        """An√°lise com VADER"""
        try:
            return self.vader_analyzer.polarity_scores(text)
        except Exception as e:
            logger.warning(f"Erro no VADER: {e}")
            return {"compound": 0.0, "pos": 0.0, "neg": 0.0, "neu": 1.0}

    def _classify_sentiment_vader(self, vader_scores: Dict[str, float]) -> str:
        """Classifica sentimento baseado nos scores do VADER"""
        compound = vader_scores.get("compound", 0.0)

        if compound >= 0.05:
            return "positive"
        elif compound <= -0.05:
            return "negative"
        else:
            return "neutral"

    def _apply_polarity_weights(self, results: Dict[str, float]) -> Dict[str, float]:
        """Aplica pesos de polaridade configurados"""
        classification = results.get("classification", "neutral")
        weight = self.polarity_weights.get(classification, 1.0)

        # Adjust polarity and compound scores
        if "polarity" in results:
            results["polarity"] *= weight
        if "compound" in results:
            results["compound"] *= weight

        return results

    def _calculate_confidence(self, results: Dict[str, float]) -> float:
        """Calcula confian√ßa da an√°lise"""
        try:
            # Base confidence on the strength of sentiment indicators
            polarity_abs = abs(results.get("polarity", 0.0))
            compound_abs = abs(results.get("compound", 0.0))
            subjectivity = results.get("subjectivity", 0.0)

            # Higher absolute values indicate stronger sentiment (more confident)
            sentiment_strength = max(polarity_abs, compound_abs)

            # Subjectivity can indicate confidence (highly subjective = less reliable)
            subjectivity_penalty = subjectivity * 0.2

            # Method bonus (more methods = higher confidence)
            method_count = len(results.get("analysis_methods", []))
            method_bonus = min(method_count * 0.1, 0.2)

            confidence = min(sentiment_strength +
                             method_bonus - subjectivity_penalty, 1.0)
            confidence = max(confidence, 0.1)  # Minimum confidence

            return confidence

        except Exception as e:
            logger.warning(f"Erro no c√°lculo de confian√ßa: {e}")
            return 0.5

    def _get_neutral_sentiment(self) -> Dict[str, float]:
        """Retorna resultado neutro padr√£o"""
        return {
            "polarity": 0.0,
            "subjectivity": 0.0,
            "compound": 0.0,
            "positive": 0.0,
            "negative": 0.0,
            "neutral": 1.0,
            "classification": "neutral",
            "confidence": 0.1,
            "analysis_methods": []
        }


# --- IN√çCIO DO C√ìDIGO DE bias_disinformation_detector.py --- #
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ARQV30 Enhanced v3.0 - External Bias & Disinformation Detector
M√≥dulo independente para detec√ß√£o de vi√©s e desinforma√ß√£o
"""


logger = logging.getLogger(__name__)


class ExternalBiasDisinformationDetector:
    """Detector de vi√©s e desinforma√ß√£o externo independente"""

    def __init__(self, config: Dict[str, Any]):
        """Inicializa o detector de vi√©s e desinforma√ß√£o"""
        self.config = config.get("bias_detection", {})
        self.enabled = self.config.get("enabled", True)

        # Load configuration
        self.bias_keywords = self.config.get("bias_keywords", [])
        self.disinformation_patterns = self.config.get(
            "disinformation_patterns", [])
        self.rhetoric_devices = self.config.get("rhetoric_devices", [])

        logger.info(f"‚úÖ External Bias & Disinformation Detector inicializado")
        logger.debug(
            f"Bias keywords: {len(self.bias_keywords)}, Patterns: {len(self.disinformation_patterns)}")

    def detect_bias_disinformation(self, text: str) -> Dict[str, float]:
        """
        Detecta padr√µes de vi√©s e desinforma√ß√£o no texto

        Args:
            text (str): Texto para an√°lise

        Returns:
            Dict[str, float]: Resultados da detec√ß√£o
        """
        if not self.enabled or not text or not text.strip():
            return self._get_neutral_result()

        try:
            # Clean and prepare text
            cleaned_text = self._clean_text(text)
            text_lower = cleaned_text.lower()

            results = {
                "bias_score": 0.0,
                "disinformation_score": 0.0,
                "rhetoric_score": 0.0,
                "overall_risk": 0.0,
                "detected_bias_keywords": [],
                "detected_disinformation_patterns": [],
                "detected_rhetoric_devices": [],
                "confidence": 0.0,
                "analysis_details": {
                    "total_words": len(cleaned_text.split()),
                    "bias_matches": 0,
                    "disinformation_matches": 0,
                    "rhetoric_matches": 0
                }
            }

            # Detect bias keywords
            bias_analysis = self._detect_bias_keywords(text_lower)
            results.update(bias_analysis)

            # Detect disinformation patterns
            disinformation_analysis = self._detect_disinformation_patterns(
                text_lower)
            results.update(disinformation_analysis)

            # Detect rhetoric devices
            rhetoric_analysis = self._detect_rhetoric_devices(text_lower)
            results.update(rhetoric_analysis)

            # Calculate overall risk
            results["overall_risk"] = self._calculate_overall_risk(results)

            # Calculate confidence
            results["confidence"] = self._calculate_confidence(results)

            logger.debug(
                f"Bias/Disinformation analysis: risk={results['overall_risk']:.3f}, confidence={results['confidence']:.3f}")

            return results

        except Exception as e:
            logger.error(f"Erro na detec√ß√£o de vi√©s/desinforma√ß√£o: {e}")
            return self._get_neutral_result()

    def _clean_text(self, text: str) -> str:
        """Limpa o texto para an√°lise"""
        if not text:
            return ""

        # Remove URLs, mentions, hashtags but keep text structure
        text = re.sub(
            r"http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+", "", text)
        text = re.sub(r"[@#](\w+)", r"\\1", text)
        text = re.sub(r"\\s+", " ", text).strip()

        return text

    def _detect_bias_keywords(self, text_lower: str) -> Dict[str, Any]:
        """Detecta palavras-chave de vi√©s"""
        detected_keywords = []
        bias_score = 0.0

        for keyword in self.bias_keywords:
            if keyword.lower() in text_lower:
                detected_keywords.append(keyword)
                bias_score += 0.1  # Each bias keyword adds 0.1 to score

        # Normalize score (cap at 1.0)
        bias_score = min(bias_score, 1.0)

        return {
            "bias_score": bias_score,
            "detected_bias_keywords": detected_keywords,
            "analysis_details": {"bias_matches": len(detected_keywords)}
        }

    def _detect_disinformation_patterns(self, text_lower: str) -> Dict[str, Any]:
        """Detecta padr√µes de desinforma√ß√£o"""
        detected_patterns = []
        disinformation_score = 0.0

        for pattern in self.disinformation_patterns:
            if pattern.lower() in text_lower:
                detected_patterns.append(pattern)
                disinformation_score += 0.15  # Each pattern adds more weight

        # Additional pattern detection with regex
        # Look for vague authority claims
        authority_patterns = [
            r"especialistas? (?:afirmam?|dizem?|garantem?)",
            r"estudos? (?:comprovam?|mostram?|indicam?)",
            r"pesquisas? (?:revelam?|demonstram?|apontam?)",
            r"cientistas? (?:descobriram?|provaram?|confirmaram?)"
        ]

        # Look for strawman fallacy
        strawman_patterns = [
            r"(?:eles|eles dizem|a esquerda|a direita) querem? (?:destruir|acabar com|impor) (?:nossa cultura|nossos valores|a fam√≠lia)",
            r"(?:argumento do espantalho|distorcem|exageram) (?:o que eu disse|nossas palavras)"
        ]

        # Look for ad hominem attacks
        ad_hominem_patterns = [
            r"(?:ele|ela|voc√™) n√£o tem moral para falar",
            r"(?:ignorante|burro|mentiroso|hip√≥crita) (?:para acreditar|para defender)"
        ]

        # Look for false dichotomy
        false_dichotomy_patterns = [
            r"(?:ou √©|ou voc√™ est√° com|ou voc√™ apoia) (?:n√≥s|eles) (?:ou contra n√≥s|ou contra eles)",
            r"(?:s√≥ existem|apenas duas? op√ß√µes?)"
        ]

        # Look for appeal to emotion
        appeal_to_emotion_patterns = [
            r"(?:pense nas crian√ßas|e se fosse voc√™|voc√™ n√£o se importa)",
            r"(?:chocante|absurdo|inacredit√°vel|revoltante)"
        ]

        all_regex_patterns = authority_patterns + strawman_patterns + \
            ad_hominem_patterns + false_dichotomy_patterns + appeal_to_emotion_patterns

        for pattern in all_regex_patterns:
            matches = re.findall(pattern, text_lower)
            if matches:
                detected_patterns.extend(matches)
                disinformation_score += len(matches) * 0.1

        # Normalize score
        disinformation_score = min(disinformation_score, 1.0)

        return {
            "disinformation_score": disinformation_score,
            "detected_disinformation_patterns": detected_patterns,
            "analysis_details": {"disinformation_matches": len(detected_patterns)}
        }

    def _detect_rhetoric_devices(self, text_lower: str) -> Dict[str, Any]:
        """Detecta dispositivos ret√≥ricos"""
        detected_devices = []
        rhetoric_score = 0.0

        # Detect emotional manipulation patterns
        emotional_patterns = {
            "apelo ao medo": [r"perig(o|oso|osa)", r"risco", r"amea√ßa", r"cat√°strofe"],
            "apelo √† emo√ß√£o": [r"imaginem?", r"pensem?", r"sintam?"],
            "generaliza√ß√£o": [r"todos? (?:sabem?|fazem?)", r"ningu√©m", r"sempre", r"nunca"],
            "falsa dicotomia": [r"ou (?:voc√™|voc√™s?)", r"apenas duas? op√ß"]
        }

        for device_name, patterns in emotional_patterns.items():
            for pattern in patterns:
                if re.search(pattern, text_lower):
                    detected_devices.append(device_name)
                    rhetoric_score += 0.1
                    break  # Only count each device type once

        # Check configured rhetoric devices
        for device in self.rhetoric_devices:
            if device.lower() in text_lower:
                detected_devices.append(device)
                rhetoric_score += 0.1

        # Normalize score
        rhetoric_score = min(rhetoric_score, 1.0)

        return {
            "rhetoric_score": rhetoric_score,
            # Remove duplicates
            "detected_rhetoric_devices": list(set(detected_devices)),
            "analysis_details": {"rhetoric_matches": len(detected_devices)}
        }

    def _calculate_overall_risk(self, results: Dict[str, Any]) -> float:
        """Calcula o risco geral"""
        # Weighted combination of different risk factors
        bias_weight = 0.3
        disinformation_weight = 0.4
        rhetoric_weight = 0.3

        overall_risk = (
            results.get("bias_score", 0.0) * bias_weight +
            results.get("disinformation_score", 0.0) * disinformation_weight +
            results.get("rhetoric_score", 0.0) * rhetoric_weight
        )

        return min(overall_risk, 1.0)

    def _calculate_confidence(self, results: Dict[str, Any]) -> float:
        """Calcula a confian√ßa da an√°lise"""
        try:
            total_words = results.get(
                "analysis_details", {}).get("total_words", 1)
            total_matches = (
                len(results.get("detected_bias_keywords", [])) +
                len(results.get("detected_disinformation_patterns", [])) +
                len(results.get("detected_rhetoric_devices", []))
            )

            # Base confidence on detection density and text length
            if total_words < 10:
                return 0.3  # Low confidence for very short text

            detection_density = total_matches / total_words

            # Higher density = higher confidence in detection
            confidence = min(0.5 + (detection_density * 5), 1.0)

            # If no matches found, still have some confidence it's clean
            if total_matches == 0 and total_words > 20:
                confidence = 0.7
            elif total_matches == 0:
                confidence = 0.5

            return max(confidence, 0.1)

        except Exception as e:
            logger.warning(f"Erro no c√°lculo de confian√ßa: {e}")
            return 0.5

    def _get_neutral_result(self) -> Dict[str, float]:
        """Retorna resultado neutro padr√£o"""
        return {
            "bias_score": 0.0,
            "disinformation_score": 0.0,
            "rhetoric_score": 0.0,
            "overall_risk": 0.0,
            "detected_bias_keywords": [],
            "detected_disinformation_patterns": [],
            "detected_rhetoric_devices": [],
            "confidence": 0.1,
            "analysis_details": {
                "total_words": 0,
                "bias_matches": 0,
                "disinformation_matches": 0,
                "rhetoric_matches": 0
            }
        }


# --- IN√çCIO DO C√ìDIGO DE confidence_thresholds.py --- #
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ARQV30 Enhanced v3.0 - External Confidence Thresholds
Gerenciador de limiares de confian√ßa para o m√≥dulo externo
"""


logger = logging.getLogger(__name__)


class ExternalConfidenceThresholds:
    """Gerenciador de limiares de confian√ßa externo"""

    def __init__(self, config: Dict[str, Any]):
        """Inicializa os limiares de confian√ßa"""
        self.thresholds = config.get("thresholds", {})

        # Default thresholds if not provided
        self.default_thresholds = {
            "approval": 0.75,
            "rejection": 0.35,
            "high_confidence": 0.85,
            "low_confidence": 0.5,
            "sentiment_neutral": 0.1,
            "bias_high_risk": 0.7,
            "llm_minimum": 0.6
        }

        # Merge with defaults
        for key, default_value in self.default_thresholds.items():
            if key not in self.thresholds:
                self.thresholds[key] = default_value

        logger.info(
            f"‚úÖ External Confidence Thresholds inicializado com {len(self.thresholds)} limiares")
        logger.debug(f"Thresholds: {self.thresholds}")

    def get_threshold(self, score_type: str, default: Optional[float] = None) -> float:
        """
        Obt√©m o limiar para um tipo de pontua√ß√£o espec√≠fico

        Args:
            score_type (str): Tipo de pontua√ß√£o (e.g., "approval", "rejection")
            default (Optional[float]): Valor padr√£o se n√£o encontrado

        Returns:
            float: Limiar de confian√ßa
        """
        if score_type in self.thresholds:
            return self.thresholds[score_type]

        if default is not None:
            return default

        # Return a reasonable default based on score type
        if "approval" in score_type.lower():
            return 0.7
        elif "rejection" in score_type.lower():
            return 0.3
        elif "high" in score_type.lower():
            return 0.8
        elif "low" in score_type.lower():
            return 0.4
        else:
            return 0.5

    def should_approve(self, confidence: float) -> bool:
        """Verifica se deve aprovar baseado na confian√ßa"""
        return confidence >= self.get_threshold("approval")

    def should_reject(self, confidence: float) -> bool:
        """Verifica se deve rejeitar baseado na confian√ßa"""
        return confidence <= self.get_threshold("rejection")

    def is_ambiguous(self, confidence: float) -> bool:
        """
        Verifica se est√° em faixa amb√≠gua (entre rejection e approval)
        """
        rejection_threshold = self.get_threshold("rejection")
        approval_threshold = self.get_threshold("approval")
        return rejection_threshold < confidence < approval_threshold

    def is_high_confidence(self, confidence: float) -> bool:
        """Verifica se √© alta confian√ßa"""
        return confidence >= self.get_threshold("high_confidence")

    def is_low_confidence(self, confidence: float) -> bool:
        """Verifica se √© baixa confian√ßa"""
        return confidence <= self.get_threshold("low_confidence")

    def is_high_bias_risk(self, risk_score: float) -> bool:
        """Verifica se √© alto risco de vi√©s"""
        return risk_score >= self.get_threshold("bias_high_risk")

    def classify_confidence_level(self, confidence: float) -> str:
        """
        Classifica o n√≠vel de confian√ßa

        Args:
            confidence (float): Pontua√ß√£o de confian√ßa

        Returns:
            str: N√≠vel de confian√ßa ("high", "medium", "low")
        """
        if self.is_high_confidence(confidence):
            return "high"
        elif self.is_low_confidence(confidence):
            return "low"
        else:
            return "medium"

    def get_decision_recommendation(self, confidence: float, risk_score: float = 0.0) -> Dict[str, Any]:
        """
        Recomenda uma decis√£o baseada na confian√ßa e risco

        Args:
            confidence (float): Pontua√ß√£o de confian√ßa
            risk_score (float): Pontua√ß√£o de risco

        Returns:
            Dict[str, Any]: Recomenda√ß√£o de decis√£o
        """
        # High risk overrides high confidence
        if self.is_high_bias_risk(risk_score):
            return {
                "decision": "reject",
                "reason": "Alto risco de vi√©s/desinforma√ß√£o detectado",
                "confidence_level": self.classify_confidence_level(confidence),
                "risk_level": "high",
                "requires_llm_analysis": False
            }

        # Clear approval
        if self.should_approve(confidence):
            return {
                "decision": "approve",
                "reason": "Alta confian√ßa na qualidade do conte√∫do",
                "confidence_level": self.classify_confidence_level(confidence),
                "risk_level": "low" if risk_score < 0.3 else "medium",
                "requires_llm_analysis": False
            }

        # Clear rejection
        if self.should_reject(confidence):
            return {
                "decision": "reject",
                "reason": "Baixa confian√ßa na qualidade do conte√∫do",
                "confidence_level": self.classify_confidence_level(confidence),
                "risk_level": "low" if risk_score < 0.3 else "medium",
                "requires_llm_analysis": False
            }

        # Ambiguous case - might need LLM analysis
        return {
            "decision": "ambiguous",
            "reason": "Confian√ßa em faixa amb√≠gua - requer an√°lise adicional",
            "confidence_level": self.classify_confidence_level(confidence),
            "risk_level": "medium" if risk_score < 0.5 else "high",
            "requires_llm_analysis": True
        }

    def update_threshold(self, score_type: str, new_value: float):
        """
        Atualiza um limiar espec√≠fico

        Args:
            score_type (str): Tipo de pontua√ß√£o
            new_value (float): Novo valor do limiar
        """
        if 0.0 <= new_value <= 1.0:
            self.thresholds[score_type] = new_value
            logger.info(
                f"Threshold \'{score_type}\' atualizado para {new_value}")
        else:
            logger.warning(
                f"Valor inv√°lido para threshold '{score_type}': {new_value} (deve estar entre 0.0 e 1.0)")

    def get_all_thresholds(self) -> Dict[str, float]:
        """Retorna todos os limiares configurados"""
        return self.thresholds.copy()

    def validate_thresholds(self) -> bool:
        """
        Valida se os limiares est√£o configurados corretamente

        Returns:
            bool: True se v√°lidos, False caso contr√°rio
        """
        try:
            # Check that rejection < approval
            rejection = self.get_threshold("rejection")
            approval = self.get_threshold("approval")

            if rejection >= approval:
                logger.error(
                    f"Configura√ß√£o inv√°lida: rejection ({rejection}) deve ser menor que approval ({approval})")
                return False

            # Check that all thresholds are in valid range
            for key, value in self.thresholds.items():
                if not (0.0 <= value <= 1.0):
                    logger.error(
                        f"Threshold \'{key}\' fora do range v√°lido (0.0-1.0): {value}")
                    return False

            logger.info("‚úÖ Todos os thresholds s√£o v√°lidos")
            return True

        except Exception as e:
            logger.error(f"Erro na valida√ß√£o dos thresholds: {e}")
            return False


# --- IN√çCIO DO C√ìDIGO DE contextual_analyzer.py --- #
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ARQV30 Enhanced v3.0 - External Contextual Analyzer
Analisador contextual para o m√≥dulo externo de verifica√ß√£o
"""


logger = logging.getLogger(__name__)


class ExternalContextualAnalyzer:
    """Analisador contextual externo independente"""

    def __init__(self, config: Dict[str, Any]):
        """Inicializa o analisador contextual"""
        self.config = config.get("contextual_analysis", {})
        self.enabled = self.config.get("enabled", True)
        self.check_consistency = self.config.get("check_consistency", True)
        self.analyze_source_reliability = self.config.get(
            "analyze_source_reliability", True)
        self.verify_temporal_coherence = self.config.get(
            "verify_temporal_coherence", True)

        # Initialize context cache for cross-item analysis
        self.context_cache = {
            "processed_items": [],
            "source_patterns": {},
            "content_patterns": {},
            "temporal_markers": []
        }

        logger.info(f"‚úÖ External Contextual Analyzer inicializado")
        logger.debug(
            f"Configura√ß√µes: consistency={self.check_consistency}, source={self.analyze_source_reliability}, temporal={self.verify_temporal_coherence}")

    def analyze_context(self, item_data: Dict[str, Any], massive_data: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:
        """
        Analisa o item em contexto mais amplo

        Args:
            item_data (Dict[str, Any]): Dados do item individual
            massive_data (Optional[Dict[str, Any]]): Dados contextuais mais amplos

        Returns:
            Dict[str, Any]: An√°lise contextual
        """
        if not self.enabled:
            return self._get_neutral_result()

        try:
            # Initialize context analysis result
            context_result = {
                "contextual_confidence": 0.5,
                "consistency_score": 0.5,
                "source_reliability_score": 0.5,
                "temporal_coherence_score": 0.5,
                "context_flags": [],
                "context_insights": [],
                "adjustment_factor": 0.0
            }

            # Extract text content for analysis
            text_content = self._extract_text_content(item_data)

            # Perform different types of contextual analysis
            if self.check_consistency:
                consistency_analysis = self._analyze_consistency(
                    text_content, item_data, massive_data)
                context_result.update(consistency_analysis)

            if self.analyze_source_reliability:
                source_analysis = self._analyze_source_reliability(
                    item_data, massive_data)
                context_result.update(source_analysis)

            if self.verify_temporal_coherence:
                temporal_analysis = self._analyze_temporal_coherence(
                    text_content, item_data)
                context_result.update(temporal_analysis)

            # Calculate overall contextual confidence
            context_result["contextual_confidence"] = self._calculate_contextual_confidence(
                context_result)

            # Update context cache for future analysis
            self._update_context_cache(item_data, context_result)

            logger.debug(
                f"Context analysis: confidence={context_result['contextual_confidence']:.3f}")

            return context_result

        except Exception as e:
            logger.error(f"Erro na an√°lise contextual: {e}")
            return self._get_neutral_result()

    def _extract_text_content(self, item_data: Dict[str, Any]) -> str:
        """Extrai conte√∫do textual relevante do item"""
        content_fields = ["content", "text", "title", "description", "summary"]

        text_content = ""
        for field in content_fields:
            if field in item_data and item_data[field]:
                text_content += f" {item_data[field]}"

        return text_content.strip()

    def _analyze_consistency(self, text_content: str, item_data: Dict[str, Any], massive_data: Optional[Dict[str, Any]]) -> Dict[str, Any]:
        """Analisa consist√™ncia interna e externa"""
        consistency_result = {
            "consistency_score": 0.5,
            "consistency_flags": [],
            "consistency_insights": []
        }

        try:
            score = 0.5
            flags = []
            insights = []

            # Check internal consistency
            internal_score, internal_flags = self._check_internal_consistency(
                text_content)
            score = (score + internal_score) / 2
            flags.extend(internal_flags)

            # Check consistency with previous items if available
            if self.context_cache["processed_items"]:
                external_score, external_flags = self._check_external_consistency(
                    text_content, item_data)
                score = (score + external_score) / 2
                flags.extend(external_flags)

                if external_score < 0.3:
                    insights.append(
                        "Conte√∫do inconsistente com padr√µes anteriores")
                elif external_score > 0.8:
                    insights.append(
                        "Conte√∫do altamente consistente com padr√µes estabelecidos")

            consistency_result.update({
                "consistency_score": score,
                "consistency_flags": flags,
                "consistency_insights": insights
            })

        except Exception as e:
            logger.warning(f"Erro na an√°lise de consist√™ncia: {e}")

        return consistency_result

    def _check_internal_consistency(self, text_content: str) -> tuple:
        """Verifica consist√™ncia interna do texto"""
        score = 0.7  # Start with good assumption
        flags = []

        if not text_content or len(text_content.strip()) < 10:
            return 0.3, ["Conte√∫do muito curto para an√°lise de consist√™ncia"]

        # Check for contradictory statements
        contradiction_patterns = [
            (r"sempre.*nunca", "Contradi√ß√£o: \'sempre\' e \'nunca\' no mesmo contexto"),
            (r"todos?.*ningu√©m", "Contradi√ß√£o: generaliza√ß√£o conflitante"),
            (r"imposs√≠vel.*poss√≠vel", "Contradi√ß√£o: possibilidade conflitante"),
            (r"verdade.*mentira", "Contradi√ß√£o: veracidade conflitante")
        ]

        for pattern, flag_msg in contradiction_patterns:
            if re.search(pattern, text_content.lower()):
                score -= 0.2
                flags.append(flag_msg)

        # Check for temporal inconsistencies
        temporal_patterns = [
            r"ontem.*amanh√£",
            r"passado.*futuro.*hoje",
            r"antes.*depois.*simultaneamente"
        ]

        for pattern in temporal_patterns:
            if re.search(pattern, text_content.lower()):
                score -= 0.1
                flags.append("Poss√≠vel inconsist√™ncia temporal")

        return max(score, 0.0), flags

    def _check_external_consistency(self, text_content: str, item_data: Dict[str, Any]) -> tuple:
        """Verifica consist√™ncia com itens processados anteriormente"""
        score = 0.5
        flags = []

        try:
            # Compare with recent processed items
            # Last 5 items
            recent_items = self.context_cache["processed_items"][-5:]

            if not recent_items:
                return 0.5, []

            # Simple keyword-based similarity check
            current_words = set(text_content.lower().split())

            similarity_scores = []
            for prev_item in recent_items:
                prev_words = set(prev_item.get("text", "").lower().split())
                if prev_words:
                    intersection = len(current_words & prev_words)
                    union = len(current_words | prev_words)
                    similarity = intersection / union if union > 0 else 0
                    similarity_scores.append(similarity)

            if similarity_scores:
                avg_similarity = sum(similarity_scores) / \
                    len(similarity_scores)

                # Very high similarity might indicate duplication
                if avg_similarity > 0.9:
                    score = 0.3
                    flags.append(
                        "Conte√∫do muito similar a itens anteriores (poss√≠vel duplica√ß√£o)")
                # Very low similarity might be inconsistent
                elif avg_similarity < 0.1:
                    score = 0.4
                    flags.append(
                        "Conte√∫do muito diferente do padr√£o estabelecido")
                else:
                    score = 0.7  # Good consistency

        except Exception as e:
            logger.warning(f"Erro na verifica√ß√£o de consist√™ncia externa: {e}")

        return score, flags

    def _analyze_source_reliability(self, item_data: Dict[str, Any], massive_data: Optional[Dict[str, Any]]) -> Dict[str, Any]:
        """Analisa confiabilidade da fonte"""
        source_result = {
            "source_reliability_score": 0.5,
            "source_flags": [],
            "source_insights": []
        }

        try:
            score = 0.5
            flags = []
            insights = []

            # Extract source information
            source_info = self._extract_source_info(item_data)

            if not source_info:
                score = 0.3
                flags.append("Fonte n√£o identificada")
                return {**source_result, "source_reliability_score": score, "source_flags": flags}

            # Check source patterns
            source_domain = source_info.get("domain", "").lower()

            # Known reliable patterns
            reliable_indicators = [
                ".edu", ".gov", ".org",
                "academia", "university", "instituto",
                "pesquisa", "ciencia", "journal"
            ]

            unreliable_indicators = [
                "blog", "forum", "social",
                "fake", "rumor", "gossip"
            ]

            for indicator in reliable_indicators:
                if indicator in source_domain:
                    score += 0.2
                    insights.append(
                        f"Fonte cont√©m indicador confi√°vel: {indicator}")
                    break

            for indicator in unreliable_indicators:
                if indicator in source_domain:
                    score -= 0.3
                    flags.append(
                        f"Fonte cont√©m indicador de baixa confiabilidade: {indicator}")
                    break

            # Check source history in cache
            if source_domain in self.context_cache["source_patterns"]:
                source_stats = self.context_cache["source_patterns"][source_domain]
                avg_quality = source_stats.get("avg_quality", 0.5)

                if avg_quality > 0.7:
                    score += 0.1
                    insights.append("Fonte com hist√≥rico positivo")
                elif avg_quality < 0.4:
                    score -= 0.1
                    flags.append("Fonte com hist√≥rico problem√°tico")

            score = min(max(score, 0.0), 1.0)

            source_result.update({
                "source_reliability_score": score,
                "source_flags": flags,
                "source_insights": insights
            })

        except Exception as e:
            logger.warning(f"Erro na an√°lise de fonte: {e}")

        return source_result

    def _extract_source_info(self, item_data: Dict[str, Any]) -> Dict[str, str]:
        """Extrai informa√ß√µes da fonte"""
        source_fields = ["source", "url", "domain", "author", "publisher"]
        source_info = {}

        for field in source_fields:
            if field in item_data and item_data[field]:
                source_info[field] = str(item_data[field])

        # Extract domain from URL if available
        if "url" in source_info and "domain" not in source_info:
            try:
                import urllib.parse
                parsed = urllib.parse.urlparse(source_info["url"])
                source_info["domain"] = parsed.netloc
            except:
                pass

        return source_info

    def _analyze_temporal_coherence(self, text_content: str, item_data: Dict[str, Any]) -> Dict[str, Any]:
        """Analisa coer√™ncia temporal"""
        temporal_result = {
            "temporal_coherence_score": 0.5,
            "temporal_flags": [],
            "temporal_insights": []
        }

        try:
            score = 0.7
            flags = []
            insights = []

            # Extract temporal markers
            temporal_markers = self._extract_temporal_markers(text_content)

            # Check for temporal inconsistencies
            if len(temporal_markers) > 1:
                coherent, issues = self._check_temporal_coherence(
                    temporal_markers)
                if not coherent:
                    score -= 0.3
                    flags.extend(issues)
                else:
                    insights.append("Marcadores temporais coerentes")

            # Check against item timestamp if available
            if "timestamp" in item_data or "date" in item_data:
                item_time = item_data.get("timestamp") or item_data.get("date")
                temporal_consistency = self._check_item_temporal_consistency(
                    temporal_markers, item_time)
                if temporal_consistency < 0.5:
                    score -= 0.2
                    flags.append(
                        "Inconsist√™ncia entre conte√∫do e timestamp do item")

            temporal_result.update({
                "temporal_coherence_score": max(score, 0.0),
                "temporal_flags": flags,
                "temporal_insights": insights
            })

        except Exception as e:
            logger.warning(f"Erro na an√°lise temporal: {e}")

        return temporal_result

    def _extract_temporal_markers(self, text: str) -> List[str]:
        """Extrai marcadores temporais do texto"""
        temporal_patterns = [
            r"(?:ontem|hoje|amanh√£)",
            r"(?:esta|pr√≥xima|passada)\\s+(?:semana|segunda|ter√ßa|quarta|quinta|sexta|s√°bado|domingo)",
            r"(?:este|pr√≥ximo|passado)\\s+(?:m√™s|ano)",
            r"(?:janeiro|fevereiro|mar√ßo|abril|maio|junho|julho|agosto|setembro|outubro|novembro|dezembro)",
            r"(?:2019|2020|2021|2022|2023|2024|2025)",
            r"h√°\\s+\\d+\\s+(?:dias?|meses?|anos?)",
            r"em\\s+\\d+\\s+(?:dias?|meses?|anos?)"
        ]

        markers = []
        for pattern in temporal_patterns:
            matches = re.findall(pattern, text.lower())
            markers.extend(matches)

        return markers

    def _check_temporal_coherence(self, markers: List[str]) -> tuple:
        """Verifica coer√™ncia entre marcadores temporais"""
        # Simple coherence check - this could be made more sophisticated
        issues = []

        # Check for obvious contradictions
        if any("ontem" in m for m in markers) and any("amanh√£" in m for m in markers):
            issues.append(
                "Contradi√ß√£o temporal: \'ontem\' e \'amanh√£\' no mesmo contexto")

        # Check for year contradictions
        years = [m for m in markers if re.search(r"20\\d{2}", m)]
        if len(set(years)) > 2:
            issues.append(
                "M√∫ltiplos anos mencionados - poss√≠vel inconsist√™ncia")

        return len(issues) == 0, issues

    def _check_item_temporal_consistency(self, markers: List[str], item_time: str) -> float:
        """
        Verifica consist√™ncia temporal com timestamp do item

        Args:
            markers (List[str]): Marcadores temporais extra√≠dos do texto.
            item_time (str): Timestamp do item (ISO format ou similar).

        Returns:
            float: Pontua√ß√£o de consist√™ncia (0.0 a 1.0).
        """
        try:
            item_dt = datetime.fromisoformat(item_time) if isinstance(
                item_time, str) else item_time
            current_year = datetime.now().year
            item_year = item_dt.year

            score = 1.0

            # Check if markers mention years significantly different from item_year
            for marker in markers:
                year_match = re.search(r"20\\d{2}", marker)
                if year_match:
                    mentioned_year = int(year_match.group(0))
                    if abs(mentioned_year - item_year) > 2:  # More than 2 years difference
                        score -= 0.3
                        logger.debug(
                            f"Inconsist√™ncia temporal: item de {item_year}, menciona {mentioned_year}")

            # Check if item is very old but mentions recent events
            if (current_year - item_year) > 5 and any(str(current_year) in m for m in markers):
                score -= 0.4
                logger.debug(
                    f"Inconsist√™ncia temporal: item antigo ({item_year}) menciona ano atual ({current_year})")

            return max(score, 0.0)

        except Exception as e:
            logger.warning(
                f"Erro ao verificar consist√™ncia temporal do item: {e}")
            return 0.5

    def _calculate_contextual_confidence(self, result: Dict[str, Any]) -> float:
        """
        Calcula a confian√ßa contextual geral

        Args:
            result (Dict[str, Any]): Resultado da an√°lise contextual

        Returns:
            float: Confian√ßa contextual (0.0 a 1.0)
        """
        weights = {
            "consistency_score": 0.4,
            "source_reliability_score": 0.4,
            "temporal_coherence_score": 0.2
        }

        confidence = (
            result.get("consistency_score", 0.5) * weights["consistency_score"] +
            result.get("source_reliability_score", 0.5) * weights["source_reliability_score"] +
            result.get("temporal_coherence_score", 0.5) *
            weights["temporal_coherence_score"]
        )

        # Penalize if many flags were raised
        if result.get("context_flags"):
            confidence -= len(result["context_flags"]) * 0.05

        return min(max(confidence, 0.0), 1.0)

    def _update_context_cache(self, item_data: Dict[str, Any], context_result: Dict[str, Any]):
        """
        Atualiza o cache de contexto com o item processado

        Args:
            item_data (Dict[str, Any]): Dados do item original.
            context_result (Dict[str, Any]): Resultado da an√°lise contextual.
        """
        # Store a simplified version of the item for consistency checks
        self.context_cache["processed_items"].append({
            "id": item_data.get("id"),
            "text": self._extract_text_content(item_data),
            "timestamp": item_data.get("timestamp"),
            "source_domain": self._extract_source_info(item_data).get("domain"),
            "contextual_confidence": context_result.get("contextual_confidence")
        })

        # Keep cache size manageable
        if len(self.context_cache["processed_items"]) > 100:
            self.context_cache["processed_items"].pop(0)

        # Update source patterns
        source_domain = self._extract_source_info(item_data).get("domain")
        if source_domain:
            if source_domain not in self.context_cache["source_patterns"]:
                self.context_cache["source_patterns"][source_domain] = {
                    "total_items": 0,
                    "sum_quality": 0.0,
                    "avg_quality": 0.0
                }

            source_stats = self.context_cache["source_patterns"][source_domain]
            source_stats["total_items"] += 1
            source_stats["sum_quality"] += context_result.get(
                "source_reliability_score", 0.5)
            source_stats["avg_quality"] = source_stats["sum_quality"] / \
                source_stats["total_items"]

        # Update temporal markers
        temporal_markers = self._extract_temporal_markers(
            self._extract_text_content(item_data))
        self.context_cache["temporal_markers"].extend(temporal_markers)
        # Keep a limited number of recent markers
        self.context_cache["temporal_markers"] = self.context_cache["temporal_markers"][-500:]

    def _get_neutral_result(self) -> Dict[str, Any]:
        """Retorna resultado neutro padr√£o"""
        return {
            "contextual_confidence": 0.5,
            "consistency_score": 0.5,
            "source_reliability_score": 0.5,
            "temporal_coherence_score": 0.5,
            "context_flags": [],
            "context_insights": [],
            "adjustment_factor": 0.0
        }


# --- IN√çCIO DO C√ìDIGO DE llm_reasoning_service.py --- #
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ARQV30 Enhanced v3.0 - External LLM Reasoning Service
Servi√ßo de racioc√≠nio com LLMs para an√°lise aprofundada
"""


# Try to import LLM clients, fallback gracefully
try:
    import google.generativeai as genai
    GEMINI_AVAILABLE = True
except ImportError:
    GEMINI_AVAILABLE = False

try:
    import openai
    OPENAI_AVAILABLE = True
except ImportError:
    OPENAI_AVAILABLE = False

logger = logging.getLogger(__name__)


class ExternalLLMReasoningService:
    """Servi√ßo de racioc√≠nio com LLMs externo independente"""

    def __init__(self, config: Dict[str, Any]):
        """Inicializa o servi√ßo de LLM"""
        self.config = config.get("llm_reasoning", {})
        self.enabled = self.config.get("enabled", True)
        self.provider = self.config.get("provider", "gemini").lower()
        self.model = self.config.get("model", "gemini-1.5-flash")
        self.max_tokens = self.config.get("max_tokens", 1000)
        self.temperature = self.config.get("temperature", 0.3)
        self.confidence_threshold = self.config.get(
            "confidence_threshold", 0.6)

        self.client = None
        self._initialize_llm_client()

        logger.info(
            f"‚úÖ External LLM Reasoning Service inicializado (Provider: {self.provider}, Available: {self.client is not None})")

    def _initialize_llm_client(self):
        """Inicializa o cliente LLM baseado no provider configurado"""
        try:
            if self.provider == "gemini" and GEMINI_AVAILABLE:
                api_key = os.getenv("GEMINI_API_KEY")
                if api_key:
                    genai.configure(api_key=api_key)
                    self.client = genai.GenerativeModel(self.model)
                    logger.info(f"‚úÖ Gemini client inicializado: {self.model}")
                else:
                    logger.warning("‚ö†Ô∏è GEMINI_API_KEY n√£o configurada")

            elif self.provider == "openai" and OPENAI_AVAILABLE:
                api_key = os.getenv("OPENAI_API_KEY")
                if api_key:
                    openai.api_key = api_key
                    self.client = openai
                    logger.info(f"‚úÖ OpenAI client inicializado: {self.model}")
                else:
                    logger.warning("‚ö†Ô∏è OPENAI_API_KEY n√£o configurada")
            else:
                logger.warning(
                    f"‚ö†Ô∏è Provider \'{self.provider}\' n√£o dispon√≠vel ou n√£o configurado")

        except Exception as e:
            logger.error(f"Erro ao inicializar LLM client: {e}")
            self.client = None

    def analyze_with_llm(self, text: str, context: str = "") -> Dict[str, Any]:
        """
        Analisa o texto com LLM para racioc√≠nio aprofundado

        Args:
            text (str): Texto para an√°lise
            context (str): Contexto adicional

        Returns:
            Dict[str, Any]: Resultados da an√°lise LLM
        """
        if not self.enabled or not self.client or not text or not text.strip():
            return self._get_default_result()

        try:
            # Prepare prompt for analysis
            prompt = self._create_analysis_prompt(text, context)

            # Get LLM response
            if self.provider == "gemini":
                response = self._analyze_with_gemini(prompt)
            elif self.provider == "openai":
                response = self._analyze_with_openai(prompt)
            else:
                return self._get_default_result()

            # Parse and structure response
            analysis_result = self._parse_llm_response(response, text)

            logger.debug(
                f"LLM analysis completed: confidence={analysis_result.get('llm_confidence', 0):.3f}")

            return analysis_result

        except Exception as e:
            logger.error(f"Erro na an√°lise LLM: {e}")
            return self._get_default_result()

    def _create_analysis_prompt(self, text: str, context: str = "") -> str:
        """Cria o prompt para an√°lise LLM"""
        context_line = f"CONTEXTO ADICIONAL: {context}" if context else ""
        base_prompt = (
            f"Analise o seguinte texto de forma cr√≠tica e objetiva:\n\n"
            f"TEXTO PARA AN√ÅLISE:\n\"{text}\"\n\n"
            f"{context_line}\n\n"
            "Por favor, forne√ßa uma an√°lise estruturada e detalhada, avaliando os seguintes aspectos:\n\n"
            "1.  **QUALIDADE DO CONTE√öDO (0-10):**\n"
            "    -   **Clareza e Coer√™ncia:** A informa√ß√£o √© apresentada de forma l√≥gica e f√°cil de entender?\n"
            "    -   **Profundidade e Abrang√™ncia:** O t√≥pico √© abordado de maneira completa ou superficial?\n"
            "    -   **Originalidade:** O conte√∫do oferece novas perspectivas ou √© uma repeti√ß√£o de informa√ß√µes existentes?\n\n"
            "2.  **CONFIABILIDADE E FONTES (0-10):**\n"
            "    -   **Verificabilidade:** As afirma√ß√µes podem ser verificadas? H√° links ou refer√™ncias para fontes prim√°rias ou secund√°rias respeit√°veis?\n"
            "    -   **Atualidade:** A informa√ß√£o est√° atualizada? H√° datas de publica√ß√£o ou revis√£o?\n"
            "    -   **Reputa√ß√£o da Fonte:** A fonte (autor, ve√≠culo) √© conhecida por sua credibilidade e precis√£o?\n\n"
            "3.  **VI√âS E PARCIALIDADE (0-10, onde 0=neutro, 10=muito tendencioso):**\n"
            "    -   **Linguagem:** H√° uso de linguagem emotiva, carregada, ou termos que buscam influenciar a opini√£o do leitor?\n"
            "    -   **Perspectiva:** O conte√∫do apresenta m√∫ltiplos lados de uma quest√£o ou foca apenas em uma vis√£o unilateral?\n"
            "    -   **Omisso:** H√° informa√ß√µes relevantes que foram intencionalmente omitidas para favorecer uma narrativa?\n"
            "    -   **Generaliza√ß√µes:** H√° uso de generaliza√ß√µes excessivas ou estere√≥tipos?\n\n"
            "4.  **RISCO DE DESINFORMA√á√ÉO (0-10):**\n"
            "    -   **Fatos:** H√° afirma√ß√µes factuais que s√£o comprovadamente falsas ou enganosas?\n"
            "    -   **Padr√µes:** O conte√∫do exibe padr√µes conhecidos de desinforma√ß√£o (ex: clickbait, teorias da conspira√ß√£o, manipula√ß√£o de imagens/v√≠deos)?\n"
            "    -   **Contexto:** O conte√∫do √© apresentado fora de contexto para alterar sua percep√ß√£o?\n\n"
            "5.  **RECOMENDA√á√ÉO FINAL:**\n"
            "    -   **Status:** [APROVAR/REJEITAR/REVIS√ÉO_MANUAL]\n"
            "    -   **Raz√£o Principal:** [breve justificativa para o status]\n\n"
            "6.  **CONFIAN√áA DA AN√ÅLISE DO LLM:**\n"
            "    -   **Pontua√ß√£o:** [0-100]% - [justificativa da confian√ßa do pr√≥prio LLM na sua an√°lise]\n\n"
            "Forne√ßa sua resposta estritamente no seguinte formato, com cada item em uma nova linha:\n"
            "QUALIDADE: [pontua√ß√£o]/10 - [breve justificativa]\n"
            "CONFIABILIDADE: [pontua√ß√£o]/10 - [breve justificativa]\n"
            "VI√âS: [pontua√ß√£o]/10 - [breve justificativa]\n"
            "DESINFORMA√á√ÉO: [pontua√ß√£o]/10 - [breve justificativa]\n"
            "RECOMENDA√á√ÉO: [APROVAR/REJEITAR/REVIS√ÉO_MANUAL] - [raz√£o principal]\n"
            "CONFIAN√áA_AN√ÅLISE: [0-100]% - [justificativa da confian√ßa]"
        )

        return base_prompt

    def _analyze_with_gemini(self, prompt: str) -> str:
        """An√°lise com Gemini"""
        try:
            response = self.client.generate_content(
                prompt,
                generation_config=genai.types.GenerationConfig(
                    max_output_tokens=self.max_tokens,
                    temperature=self.temperature
                )
            )
            return response.text
        except Exception as e:
            logger.error(f"Erro no Gemini: {e}")
            raise

    def _analyze_with_openai(self, prompt: str) -> str:
        """An√°lise com OpenAI"""
        try:
            response = openai.ChatCompletion.create(
                model=self.model,
                messages=[{"role": "user", "content": prompt}],
                max_tokens=self.max_tokens,
                temperature=self.temperature
            )
            return response.choices[0].message.content
        except Exception as e:
            logger.error(f"Erro no OpenAI: {e}")
            raise

    def _parse_llm_response(self, response: str, original_text: str) -> Dict[str, Any]:
        """Parse da resposta LLM para formato estruturado"""
        try:
            import re

            # Initialize result structure
            result = {
                "llm_response": response,
                "quality_score": 5.0,
                "reliability_score": 5.0,
                "bias_score": 5.0,
                "disinformation_score": 5.0,
                "llm_recommendation": "REVIS√ÉO_MANUAL",
                "llm_confidence": 0.5,
                "analysis_reasoning": "",
                "provider": self.provider,
                "model": self.model
            }

            # Extract scores using regex
            patterns = {
                "quality_score": r"QUALIDADE:\\s*([0-9]+(?:\\.[0-9]+)?)",
                "reliability_score": r"CONFIABILIDADE:\\s*([0-9]+(?:\\.[0-9]+)?)",
                "bias_score": r"VI√âS:\\s*([0-9]+(?:\\.[0-9]+)?)",
                "disinformation_score": r"DESINFORMA√á√ÉO:\\s*([0-9]+(?:\\.[0-9]+)?)",
                "llm_recommendation": r"RECOMENDA√á√ÉO:\\s*(APROVAR|REJEITAR|REVIS√ÉO_MANUAL)",
                "llm_confidence": r"CONFIAN√áA_AN√ÅLISE:\\s*([0-9]+)%?"
            }

            for key, pattern in patterns.items():
                match = re.search(pattern, response, re.IGNORECASE)
                if match:
                    if key == "llm_confidence":
                        result[key] = min(float(match.group(1)) / 100.0, 1.0)
                    elif key == "llm_recommendation":
                        result[key] = match.group(1).upper()
                    else:
                        # Convert 0-10 scores to 0-1 range
                        score = min(float(match.group(1)) / 10.0, 1.0)
                        result[key] = score

            # Extract reasoning from response
            reasoning_parts = []
            for line in response.split("\\n"):
                if " - " in line and any(keyword in line.upper() for keyword in ["QUALIDADE", "CONFIABILIDADE", "VI√âS", "DESINFORMA√á√ÉO"]):
                    reasoning_parts.append(line.split(" - ", 1)[-1])

            result["analysis_reasoning"] = " | ".join(reasoning_parts)

            # Validate and adjust confidence based on consistency
            result["llm_confidence"] = self._validate_llm_confidence(result)

            return result

        except Exception as e:
            logger.warning(f"Erro no parsing da resposta LLM: {e}")
            # Return response as-is with default scores
            return {
                "llm_response": response,
                "quality_score": 0.5,
                "reliability_score": 0.5,
                "bias_score": 0.5,
                "disinformation_score": 0.5,
                "llm_recommendation": "REVIS√ÉO_MANUAL",
                "llm_confidence": 0.3,
                "analysis_reasoning": "Erro no parsing da resposta",
                "provider": self.provider,
                "model": self.model
            }

    def _validate_llm_confidence(self, result: Dict[str, Any]) -> float:
        """Valida e ajusta a confian√ßa baseada na consist√™ncia da an√°lise"""
        try:
            # Check consistency between recommendation and scores
            quality = result.get("quality_score", 0.5)
            reliability = result.get("reliability_score", 0.5)
            bias = result.get("bias_score", 0.5)
            disinformation = result.get("disinformation_score", 0.5)
            recommendation = result.get("llm_recommendation", "REVIS√ÉO_MANUAL")
            base_confidence = result.get("llm_confidence", 0.5)

            # Calculate expected recommendation based on scores
            avg_positive_scores = (quality + reliability) / 2.0
            avg_negative_scores = (bias + disinformation) / 2.0

            expected_approval = avg_positive_scores > 0.7 and avg_negative_scores < 0.4
            expected_rejection = avg_positive_scores < 0.4 or avg_negative_scores > 0.6

            # Check consistency
            consistency_bonus = 0.0
            if recommendation == "APROVAR" and expected_approval:
                consistency_bonus = 0.1
            elif recommendation == "REJEITAR" and expected_rejection:
                consistency_bonus = 0.1
            elif recommendation == "REVIS√ÉO_MANUAL":
                consistency_bonus = 0.05  # Neutral is always somewhat consistent

            # Adjust confidence
            adjusted_confidence = min(base_confidence + consistency_bonus, 1.0)
            adjusted_confidence = max(adjusted_confidence, 0.1)

            return adjusted_confidence

        except Exception as e:
            logger.warning(f"Erro na valida√ß√£o de confian√ßa: {e}")
            return 0.5

    def _get_default_result(self) -> Dict[str, Any]:
        """Retorna resultado padr√£o quando LLM n√£o est√° dispon√≠vel"""
        return {
            "llm_response": "LLM n√£o dispon√≠vel ou configurado",
            "quality_score": 0.5,
            "reliability_score": 0.5,
            "bias_score": 0.5,
            "disinformation_score": 0.5,
            "llm_recommendation": "REVIS√ÉO_MANUAL",
            "llm_confidence": 0.1,
            "analysis_reasoning": "An√°lise LLM n√£o dispon√≠vel",
            "provider": self.provider,
            "model": self.model
        }


# --- IN√çCIO DO C√ìDIGO DE external_review_agent.py --- #


    def _create_insufficient_content_result(self, item_data: Dict[str, Any]) -> Dict[str, Any]:
        """Cria resultado para item com conte√∫do insuficiente"""
        return {
            'item_id': item_data.get('id', 'sem_id'),
            'original_item': item_data,
            'processing_timestamp': datetime.now().isoformat(),
            'ai_review': {
                'status': 'rejected',
                'reason': 'Conte√∫do textual insuficiente para an√°lise',
                'final_confidence': 0.0,
                'error': True,
                'insufficient_content': True
            },
            'sentiment_analysis': {},
            'bias_disinformation_analysis': {},
            'llm_reasoning_analysis': {},
            'rule_decision': {}
        }

    def _create_error_result(self, item_data: Dict[str, Any], error_message: str) -> Dict[str, Any]:
        """Cria resultado de erro gen√©rico"""
        return {
            'item_id': item_data.get('id', 'sem_id'),
            'original_item': item_data,
            'processing_timestamp': datetime.now().isoformat(),
            'ai_review': {
                'status': 'rejected',
                'reason': f'Erro inesperado durante o processamento: {error_message}',
                'final_confidence': 0.0,
                'error': True
            },
            'sentiment_analysis': {},
            'bias_disinformation_analysis': {},
            'llm_reasoning_analysis': {},
            'rule_decision': {}
        }

    def _update_stats(self, status: str, processing_time: float):
        """Atualiza estat√≠sticas de processamento"""
        self.stats['total_processed'] += 1
        if status == 'approved':
            self.stats['approved'] += 1
        elif status == 'rejected':
            self.stats['rejected'] += 1
        self.stats['processing_times'].append(processing_time)

    def get_processing_stats(self) -> Dict[str, Any]:
        """
        Retorna estat√≠sticas de processamento

        Returns:
            Dict[str, Any]: Estat√≠sticas de processamento
        """
        total_time = (datetime.now() -
                      self.stats['start_time']).total_seconds()
        avg_processing_time = sum(self.stats['processing_times']) / len(
            self.stats['processing_times']) if self.stats['processing_times'] else 0

        return {
            'total_processed_items': self.stats['total_processed'],
            'approved_items': self.stats['approved'],
            'rejected_items': self.stats['rejected'],
            'total_processing_time_seconds': total_time,
            'average_item_processing_time_seconds': avg_processing_time,
            'uptime_seconds': total_time
        }

    def update_config(self, new_config: Dict[str, Any]):
        """
        Atualiza a configura√ß√£o do agente e seus subm√≥dulos

        Args:
            new_config (Dict[str, Any]): Nova configura√ß√£o a ser aplicada
        """
        self.config.update(new_config)
        self.sentiment_analyzer = ExternalSentimentAnalyzer(self.config)
        self.bias_detector = ExternalBiasDisinformationDetector(self.config)
        self.llm_service = ExternalLLMReasoningService(self.config)
        self.rule_engine = ExternalRuleEngine(self.config)
        self.contextual_analyzer = ExternalContextualAnalyzer(self.config)
        self.confidence_thresholds = ExternalConfidenceThresholds(self.config)
        logger.info("‚öôÔ∏è Configura√ß√£o do External Review Agent atualizada.")

    async def process_item_async(self, item_data: Dict[str, Any], massive_data: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:
        """
        Processa um item individual de forma ass√≠ncrona (exemplo)

        Args:
            item_data (Dict[str, Any]): Dados do item para an√°lise
            massive_data (Optional[Dict[str, Any]]): Contexto adicional

        Returns:
            Dict[str, Any]: Resultado completo da an√°lise
        """
        # Para simplificar, este √© um wrapper s√≠ncrono. Em um ambiente real,
        # as chamadas internas seriam awaitable.
        return self.process_item(item_data, massive_data)

    async def process_batch_async(self, batch_data: List[Dict[str, Any]], massive_data: Optional[Dict[str, Any]] = None) -> List[Dict[str, Any]]:
        """
        Processa um lote de itens de forma ass√≠ncrona

        Args:
            batch_data (List[Dict[str, Any]]): Lista de itens para an√°lise
            massive_data (Optional[Dict[str, Any]]): Contexto adicional

        Returns:
            List[Dict[str, Any]]: Lista de resultados de an√°lise
        """
        tasks = [self.process_item_async(item, massive_data)
                 for item in batch_data]
        return await asyncio.gather(*tasks)

# --- FIM DO C√ìDIGO DE external_review_agent.py --- #


    def _create_insufficient_content_result(self, item_data: Dict[str, Any]) -> Dict[str, Any]:
        """Cria resultado para item com conte√∫do insuficiente"""
        return {
            'item_id': item_data.get('id', 'sem_id'),
            'original_item': item_data,
            'processing_timestamp': datetime.now().isoformat(),
            'ai_review': {
                'status': 'rejected',
                'reason': 'Conte√∫do textual insuficiente para an√°lise',
                'final_confidence': 0.0,
                'error': True,
                'insufficient_content': True
            },
            'sentiment_analysis': {},
            'bias_disinformation_analysis': {},
            'llm_reasoning_analysis': {},
            'contextual_analysis': {},
            'rule_decision': {}
        }

    def _create_error_result(self, item_data: Dict[str, Any], error_message: str) -> Dict[str, Any]:
        """Cria resultado de erro gen√©rico"""
        return {
            'item_id': item_data.get('id', 'sem_id'),
            'original_item': item_data,
            'processing_timestamp': datetime.now().isoformat(),
            'ai_review': {
                'status': 'rejected',
                'reason': f'Erro inesperado durante o processamento: {error_message}',
                'final_confidence': 0.0,
                'error': True
            },
            'sentiment_analysis': {},
            'bias_disinformation_analysis': {},
            'llm_reasoning_analysis': {},
            'contextual_analysis': {},
            'rule_decision': {}
        }

    def _update_stats(self, status: str, processing_time: float):
        """Atualiza estat√≠sticas de processamento"""
        self.stats['total_processed'] += 1
        if status == 'approved':
            self.stats['approved'] += 1
        elif status == 'rejected':
            self.stats['rejected'] += 1
        self.stats['processing_times'].append(processing_time)

    def get_processing_stats(self) -> Dict[str, Any]:
        """
        Retorna estat√≠sticas de processamento

        Returns:
            Dict[str, Any]: Estat√≠sticas de processamento
        """
        total_time = (datetime.now() -
                      self.stats['start_time']).total_seconds()
        avg_processing_time = sum(self.stats['processing_times']) / len(
            self.stats['processing_times']) if self.stats['processing_times'] else 0

        return {
            'total_processed_items': self.stats['total_processed'],
            'approved_items': self.stats['approved'],
            'rejected_items': self.stats['rejected'],
            'total_processing_time_seconds': total_time,
            'average_item_processing_time_seconds': avg_processing_time,
            'uptime_seconds': total_time
        }

    def update_config(self, new_config: Dict[str, Any]):
        """
        Atualiza a configura√ß√£o do agente e seus subm√≥dulos

        Args:
            new_config (Dict[str, Any]): Nova configura√ß√£o a ser aplicada
        """
        self.config.update(new_config)
        self.sentiment_analyzer = ExternalSentimentAnalyzer(self.config)
        self.bias_detector = ExternalBiasDisinformationDetector(self.config)
        self.llm_service = ExternalLLMReasoningService(self.config)
        self.rule_engine = ExternalRuleEngine(self.config)
        self.contextual_analyzer = ExternalContextualAnalyzer(self.config)
        self.confidence_thresholds = ExternalConfidenceThresholds(self.config)
        logger.info("‚öôÔ∏è Configura√ß√£o do External Review Agent atualizada.")

    async def process_item_async(self, item_data: Dict[str, Any], massive_data: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:
        """
        Processa um item individual de forma ass√≠ncrona (exemplo)

        Args:
            item_data (Dict[str, Any]): Dados do item para an√°lise
            massive_data (Optional[Dict[str, Any]]): Contexto adicional

        Returns:
            Dict[str, Any]: Resultado completo da an√°lise
        """
        # Para simplificar, este √© um wrapper s√≠ncrono. Em um ambiente real,
        # as chamadas internas seriam awaitable.
        return self.process_item(item_data, massive_data)

    async def process_batch_async(self, batch_data: List[Dict[str, Any]], massive_data: Optional[Dict[str, Any]] = None) -> List[Dict[str, Any]]:
        """
        Processa um lote de itens de forma ass√≠ncrona

        Args:
            batch_data (List[Dict[str, Any]]): Lista de itens para an√°lise
            massive_data (Optional[Dict[str, Any]]): Contexto adicional

        Returns:
            List[Dict[str, Any]]: Lista de resultados de an√°lise
        """
        tasks = [self.process_item_async(item, massive_data)
                 for item in batch_data]
        return await asyncio.gather(*tasks)

# --- FIM DO C√ìDIGO DE external_review_agent.py --- #


#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ARQV30 Enhanced v3.0 - External Review Agent
Agente principal de revis√£o externa - ponto de entrada do m√≥dulo
"""


# Handle both relative and absolute imports
# from services.sentiment_analyzer import ExternalSentimentAnalyzer
# from services.bias_disinformation_detector import ExternalBiasDisinformationDetector
# from services.llm_reasoning_service import ExternalLLMReasoningService
# from services.rule_engine import ExternalRuleEngine
# from services.contextual_analyzer import ExternalContextualAnalyzer
# from services.confidence_thresholds import ExternalConfidenceThresholds

logger = logging.getLogger(__name__)


class ExternalReviewAgent:
    """Agente de revis√£o externa - orquestrador principal do m√≥dulo"""

    def __init__(self, config: Optional[Dict[str, Any]] = None, config_path: Optional[str] = None):
        """
        Inicializa o agente de revis√£o externa

        Args:
            config_path (Optional[str]): Caminho para arquivo de configura√ß√£o
        """
        # Inicializar logger da inst√¢ncia
        self.logger = logging.getLogger(
            f"{__name__}.{self.__class__.__name__}")

        if config is not None:
            self.config = config
        else:
            self.config = self._load_config(config_path)

        # Initialize all analysis services
        self.sentiment_analyzer = ExternalSentimentAnalyzer(self.config)
        self.bias_detector = ExternalBiasDisinformationDetector(self.config)
        self.llm_service = ExternalLLMReasoningService(self.config)
        self.rule_engine = ExternalRuleEngine(self.config)
        self.contextual_analyzer = ExternalContextualAnalyzer(self.config)
        self.confidence_thresholds = ExternalConfidenceThresholds(self.config)

        # Processing statistics
        self.stats = {
            'total_processed': 0,
            'approved': 0,
            'rejected': 0,
            'start_time': datetime.now(),
            'processing_times': []
        }

        logger.info(f"‚úÖ External Review Agent inicializado com sucesso")
        logger.info(f"üîß Configura√ß√µes carregadas: {len(self.config)} se√ß√µes")

    def _load_config(self, config_path: Optional[str] = None) -> Dict[str, Any]:
        """
        Carrega configura√ß√£o do m√≥dulo

        Prioriza o caminho do arquivo de configura√ß√£o, mas se n√£o for fornecido ou n√£o existir,
        tenta carregar de um caminho padr√£o ou retorna a configura√ß√£o padr√£o.
        """
        try:
            # Default config path
            if config_path is None:
                # Assumindo que o config.yaml est√° no mesmo diret√≥rio ou em um subdiret√≥rio 'config'
                current_dir = os.path.dirname(os.path.abspath(__file__))
                # Tenta encontrar config.yaml no mesmo diret√≥rio do enhanced_synthesis_engine.py
                config_path = os.path.join(current_dir, 'config.yaml')
                if not os.path.exists(config_path):
                    # Se n√£o encontrar, tenta no diret√≥rio 'config' acima
                    config_path = os.path.join(
                        current_dir, '..', 'config', 'default_config.yaml')

            # Load configuration file
            if os.path.exists(config_path):
                with open(config_path, 'r', encoding='utf-8') as f:
                    if config_path.endswith('.yaml') or config_path.endswith('.yml'):
                        config = yaml.safe_load(f)
                    else:
                        config = json.load(f)
                logger.info(f"‚úÖ Configura√ß√£o carregada: {config_path}")
                return config
            else:
                logger.warning(
                    f"‚ö†Ô∏è Arquivo de configura√ß√£o n√£o encontrado: {config_path}. Usando configura√ß√£o padr√£o.")
                return self._get_default_config()

        except Exception as e:
            logger.error(
                f"Erro ao carregar configura√ß√£o: {e}. Usando configura√ß√£o padr√£o.")
            return self._get_default_config()

    def _get_default_config(self) -> Dict[str, Any]:
        """Retorna configura√ß√£o padr√£o"""
        return {
            'thresholds': {
                'approval': 0.75,
                'rejection': 0.35,
                'high_confidence': 0.85,
                'low_confidence': 0.5,
                'bias_high_risk': 0.7
            },
            'sentiment_analysis': {'enabled': True},
            'bias_detection': {'enabled': True},
            'llm_reasoning': {'enabled': True},
            'contextual_analysis': {'enabled': True},
            'rules': []
        }

    def process_item(self, item_data: Dict[str, Any], massive_data: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:
        """
        Processa um item individual atrav√©s de todas as an√°lises com valida√ß√£o aprimorada

        Args:
            item_data (Dict[str, Any]): Dados do item para an√°lise
            massive_data (Optional[Dict[str, Any]]): Contexto adicional

        Returns:
            Dict[str, Any]: Resultado completo da an√°lise
        """
        start_time = datetime.now()

        try:
            item_id = item_data.get(
                'id', f'item_{self.stats["total_processed"]}')
            logger.info(f"üîç Iniciando an√°lise do item: {item_id}")

            # Valida√ß√£o pr√©via do item
            validation_result = self._validate_item_data(item_data)
            if not validation_result['valid']:
                logger.warning(
                    f"‚ö†Ô∏è Item inv√°lido: {validation_result['reason']}")
                return self._create_validation_error_result(item_data, validation_result['reason'])

            # Extract text content for analysis
            text_content = self._extract_text_content(item_data)

            if not text_content or len(text_content.strip()) < 5:
                logger.warning("‚ö†Ô∏è Item com conte√∫do textual insuficiente")
                return self._create_insufficient_content_result(item_data)

            # Initialize analysis results
            analysis_result = {
                'item_id': item_data.get('id', f'item_{self.stats["total_processed"]}'),
                'original_item': item_data,
                'processing_timestamp': start_time.isoformat(),
                # First 500 chars for reference
                'text_analyzed': text_content[:500],
            }

            # Step 1: Sentiment Analysis
            logger.debug("Executando an√°lise de sentimento...")
            sentiment_result = self.sentiment_analyzer.analyze_sentiment(
                text_content)
            analysis_result['sentiment_analysis'] = sentiment_result

            # Step 2: Bias & Disinformation Detection
            logger.debug("Executando detec√ß√£o de vi√©s/desinforma√ß√£o...")
            bias_result = self.bias_detector.detect_bias_disinformation(
                text_content)
            analysis_result['bias_disinformation_analysis'] = bias_result

            # Step 3: LLM Reasoning (for ambiguous cases)
            should_use_llm = self._should_use_llm_analysis(
                sentiment_result, bias_result)
            if should_use_llm:
                logger.debug("Executando an√°lise LLM...")
                context = self._create_llm_context(
                    analysis_result, massive_data)
                llm_result = self.llm_service.analyze_with_llm(
                    text_content, context)
                analysis_result['llm_reasoning_analysis'] = llm_result
            else:
                analysis_result['llm_reasoning_analysis'] = {
                    'llm_confidence': 0.5,
                    'llm_recommendation': 'N√ÉO_EXECUTADO',
                    'analysis_reasoning': 'LLM n√£o necess√°rio para este item'
                }

            # Step 4: Contextual Analysis
            logger.debug("Executando an√°lise contextual...")
            contextual_result = self.contextual_analyzer.analyze_context(
                item_data, massive_data)
            analysis_result['contextual_analysis'] = contextual_result

            # Step 5: Rule Engine Application
            logger.debug("Aplicando regras de neg√≥cio...")
            rule_result = self.rule_engine.apply_rules(analysis_result)
            analysis_result['rule_decision'] = rule_result

            # Step 6: Final Decision
            final_decision = self._make_final_decision(analysis_result)
            analysis_result['ai_review'] = final_decision

            # Update statistics
            processing_time = (datetime.now() - start_time).total_seconds()
            self._update_stats(final_decision['status'], processing_time)

            analysis_result['processing_time_seconds'] = processing_time

            logger.info(
                f"‚úÖ Item processado: {final_decision['status']} (confian√ßa: {final_decision['final_confidence']:.3f})")

            return analysis_result

        except Exception as e:
            logger.error(f"Erro no processamento do item: {e}")
            error_result = self._create_error_result(item_data, str(e))
            self._update_stats(
                'error', (datetime.now() - start_time).total_seconds())
            return error_result

    def _validate_item_data(self, item_data: Dict[str, Any]) -> Dict[str, Any]:
        """Valida dados do item antes do processamento"""
        if not isinstance(item_data, dict):
            return {'valid': False, 'reason': 'Item deve ser um dicion√°rio'}

        if not item_data:
            return {'valid': False, 'reason': 'Item vazio'}

        # Verificar se tem pelo menos um campo de conte√∫do
        content_fields = ['content', 'text', 'title',
                          'description', 'summary', 'body']
        has_content = any(
            field in item_data and item_data[field] for field in content_fields)

        if not has_content:
            return {'valid': False, 'reason': 'Item n√£o possui conte√∫do textual v√°lido'}

        return {'valid': True, 'reason': 'Item v√°lido'}

    def _extract_text_content(self, item_data: Dict[str, Any]) -> str:
        """Extrai conte√∫do textual do item com prioriza√ß√£o"""
        # Campos priorizados por import√¢ncia
        priority_fields = ['content', 'text',
                           'description', 'summary', 'title', 'body']

        text_parts = []
        for field in priority_fields:
            if field in item_data and item_data[field]:
                content = str(item_data[field]).strip()
                if content:
                    text_parts.append(content)

        # Adicionar campos extras se existirem
        extra_fields = ['subtitle', 'excerpt', 'abstract', 'caption']
        for field in extra_fields:
            if field in item_data and item_data[field]:
                content = str(item_data[field]).strip()
                if content:
                    text_parts.append(content)

        return ' '.join(text_parts).strip()

    def _should_use_llm_analysis(self, sentiment_result: Dict[str, Any], bias_result: Dict[str, Any]) -> bool:
        """
        Determina se deve usar an√°lise LLM

        Args:
            sentiment_result (Dict[str, Any]): Resultado da an√°lise de sentimento.
            bias_result (Dict[str, Any]): Resultado da an√°lise de vi√©s e desinforma√ß√£o.

        Returns:
            bool: True se a an√°lise LLM deve ser usada, False caso contr√°rio.
        """
        # Use LLM for ambiguous cases or high-risk content
        sentiment_confidence = sentiment_result.get('confidence', 0.5)
        bias_risk = bias_result.get('overall_risk', 0.0)

        # Low confidence sentiment or high bias risk = use LLM
        # Thresholds can be configured in ExternalReviewAgent's config
        # For now, using hardcoded values for demonstration
        return sentiment_confidence < 0.6 or bias_risk > 0.4

    def _create_llm_context(self, analysis_result: Dict[str, Any], massive_data: Optional[Dict[str, Any]]) -> str:
        """
        Cria contexto para an√°lise LLM

        Args:
            analysis_result (Dict[str, Any]): Resultados parciais da an√°lise.
            massive_data (Optional[Dict[str, Any]]): Dados contextuais mais amplos.

        Returns:
            str: String de contexto para o LLM.
        """
        context_parts = []

        # Add sentiment context
        sentiment = analysis_result.get('sentiment_analysis', {})
        if sentiment.get('classification') != 'neutral':
            context_parts.append(
                f"Sentimento detectado: {sentiment.get('classification', 'indefinido')}")

        # Add bias context
        bias = analysis_result.get('bias_disinformation_analysis', {})
        if bias.get('overall_risk', 0) > 0.3:
            context_parts.append(
                f"Risco de vi√©s detectado: {bias.get('overall_risk', 0):.2f}")

        # Add any available external context
        if massive_data:
            if 'topic' in massive_data:
                context_parts.append(f"T√≥pico: {massive_data['topic']}")

        return ' | '.join(context_parts) if context_parts else ""

    def _make_final_decision(self, analysis_result: Dict[str, Any]) -> Dict[str, Any]:
        """
        Toma decis√£o final baseada em todas as an√°lises

        Args:
            analysis_result (Dict[str, Any]): Resultados completos da an√°lise.

        Returns:
            Dict[str, Any]: Decis√£o final e detalhes.
        """
        try:
            # Get analysis results
            sentiment = analysis_result.get('sentiment_analysis', {})
            bias = analysis_result.get('bias_disinformation_analysis', {})
            llm = analysis_result.get('llm_reasoning_analysis', {})
            contextual = analysis_result.get('contextual_analysis', {})
            rule_decision = analysis_result.get('rule_decision', {})

            # Calculate composite confidence
            confidences = [
                sentiment.get('confidence', 0.5) * 0.2,  # 20% weight
                (1.0 - bias.get('overall_risk', 0.5)) *
                0.3,  # 30% weight (inverted risk)
                llm.get('llm_confidence', 0.5) * 0.3,  # 30% weight
                contextual.get('contextual_confidence', 0.5) *
                0.2  # 20% weight
            ]

            final_confidence = sum(confidences)

            # Apply rule engine decision if applicable
            if rule_decision.get('status') in ['approved', 'rejected']:
                status = rule_decision['status']
                reason = rule_decision['reason']
            else:
                # Use confidence thresholds for decision
                if self.confidence_thresholds.should_approve(final_confidence):
                    status = 'approved'
                    reason = 'Aprovado com base na an√°lise combinada'
                elif self.confidence_thresholds.should_reject(final_confidence):
                    status = 'rejected'
                    reason = 'Rejeitado com base na an√°lise combinada'
                else:
                    # Default to rejection for ambiguous cases (safer)
                    status = 'rejected'
                    reason = 'Rejeitado por ambiguidade - pol√≠tica de seguran√ßa'

            # Create comprehensive decision result
            decision = {
                'status': status,
                'reason': reason,
                'final_confidence': final_confidence,
                'confidence_breakdown': {
                    'sentiment_contribution': sentiment.get('confidence', 0.5) * 0.2,
                    'bias_contribution': (1.0 - bias.get('overall_risk', 0.5)) * 0.3,
                    'llm_contribution': llm.get('llm_confidence', 0.5) * 0.3,
                    'contextual_contribution': contextual.get('contextual_confidence', 0.5) * 0.2
                },
                'decision_factors': {
                    'sentiment_classification': sentiment.get('classification', 'neutral'),
                    'bias_risk_level': 'high' if bias.get('overall_risk', 0) > 0.6 else 'medium' if bias.get('overall_risk', 0) > 0.3 else 'low',
                    'llm_recommendation': llm.get('llm_recommendation', 'N√ÉO_EXECUTADO'),
                    'rule_triggered': rule_decision.get('triggered_rules', [])
                },
                'analysis_summary': {
                    'total_flags': (
                        len(bias.get('detected_bias_keywords', [])) +
                        len(bias.get('detected_disinformation_patterns', [])) +
                        len(contextual.get('context_flags', []))
                    ),
                    'sentiment_polarity': sentiment.get('polarity', 0.0),
                    'overall_risk_score': bias.get('overall_risk', 0.0),
                    'contextual_consistency': contextual.get('consistency_score', 0.5)
                },
                'processing_metadata': {
                    'timestamp': datetime.now().isoformat(),
                    'version': '3.0.0',
                    'confidence_threshold_used': self.confidence_thresholds.get_threshold('approval')
                }
            }

            return decision

        except Exception as e:
            logger.error(f"Erro na decis√£o final: {e}")
            return {
                'status': 'rejected',
                'reason': f'Erro no processamento: {str(e)}',
                'final_confidence': 0.0,
                'error': True
            }

    def _create_validation_error_result(self, item_data: Dict[str, Any], reason: str) -> Dict[str, Any]:
        """Cria resultado para item que falhou na valida√ß√£o"""
        return {
            'item_id': item_data.get('id', 'sem_id'),
            'original_item': item_data,
            'processing_timestamp': datetime.now().isoformat(),
            'ai_review': {
                'status': 'rejected',
                'reason': f'Erro de valida√ß√£o: {reason}',
                'final_confidence': 0.0,
                'error': True,
                'validation_error': True
            },
            'sentiment_analysis': {},
            'bias_disinformation_analysis': {},
            'llm_reasoning_analysis': {},
            'contextual_analysis': {},
            'rule_decision': {}
        }

    def _create_insufficient_content_result(self, item_data: Dict[str, Any]) -> Dict[str, Any]:
        """Cria resultado para item com conte√∫do insuficiente"""
        return {
            'item_id': item_data.get('id', 'sem_id'),
            'original_item': item_data,
            'processing_timestamp': datetime.now().isoformat(),
            'ai_review': {
                'status': 'rejected',
                'reason': 'Conte√∫do textual insuficiente para an√°lise',
                'final_confidence': 0.0,
                'error': True,
                'insufficient_content': True
            },
            'sentiment_analysis': {},
            'bias_disinformation_analysis': {},
            'llm_reasoning_analysis': {},
            'contextual_analysis': {},
            'rule_decision': {}
        }

    def _create_error_result(self, item_data: Dict[str, Any], error_message: str) -> Dict[str, Any]:
        """Cria resultado de erro gen√©rico"""
        return {
            'item_id': item_data.get('id', 'sem_id'),
            'original_item': item_data,
            'processing_timestamp': datetime.now().isoformat(),
            'ai_review': {
                'status': 'rejected',
                'reason': f'Erro inesperado durante o processamento: {error_message}',
                'final_confidence': 0.0,
                'error': True
            },
            'sentiment_analysis': {},
            'bias_disinformation_analysis': {},
            'llm_reasoning_analysis': {},
            'contextual_analysis': {},
            'rule_decision': {}
        }

    def _update_stats(self, status: str, processing_time: float):
        """Atualiza estat√≠sticas de processamento"""
        self.stats['total_processed'] += 1
        if status == 'approved':
            self.stats['approved'] += 1
        elif status == 'rejected':
            self.stats['rejected'] += 1
        self.stats['processing_times'].append(processing_time)

    def get_processing_stats(self) -> Dict[str, Any]:
        """
        Retorna estat√≠sticas de processamento

        Returns:
            Dict[str, Any]: Estat√≠sticas de processamento
        """
        total_time = (datetime.now() -
                      self.stats['start_time']).total_seconds()
        avg_processing_time = sum(self.stats['processing_times']) / len(
            self.stats['processing_times']) if self.stats['processing_times'] else 0

        return {
            'total_processed_items': self.stats['total_processed'],
            'approved_items': self.stats['approved'],
            'rejected_items': self.stats['rejected'],
            'total_processing_time_seconds': total_time,
            'average_item_processing_time_seconds': avg_processing_time,
            'uptime_seconds': total_time
        }

    def update_config(self, new_config: Dict[str, Any]):
        """
        Atualiza a configura√ß√£o do agente e seus subm√≥dulos

        Args:
            new_config (Dict[str, Any]): Nova configura√ß√£o a ser aplicada
        """
        self.config.update(new_config)
        self.sentiment_analyzer = ExternalSentimentAnalyzer(self.config)
        self.bias_detector = ExternalBiasDisinformationDetector(self.config)
        self.llm_service = ExternalLLMReasoningService(self.config)
        self.rule_engine = ExternalRuleEngine(self.config)
        self.contextual_analyzer = ExternalContextualAnalyzer(self.config)
        self.confidence_thresholds = ExternalConfidenceThresholds(self.config)
        logger.info("‚öôÔ∏è Configura√ß√£o do External Review Agent atualizada.")

    async def process_item_async(self, item_data: Dict[str, Any], massive_data: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:
        """
        Processa um item individual de forma ass√≠ncrona (exemplo)

        Args:
            item_data (Dict[str, Any]): Dados do item para an√°lise
            massive_data (Optional[Dict[str, Any]]): Contexto adicional

        Returns:
            Dict[str, Any]: Resultado completo da an√°lise
        """
        # Para simplificar, este √© um wrapper s√≠ncrono. Em um ambiente real,
        # as chamadas internas seriam awaitable.
        return self.process_item(item_data, massive_data)

    async def process_batch_async(self, batch_data: List[Dict[str, Any]], massive_data: Optional[Dict[str, Any]] = None) -> List[Dict[str, Any]]:
        """
        Processa um lote de itens de forma ass√≠ncrona

        Args:
            batch_data (List[Dict[str, Any]]): Lista de itens para an√°lise
            massive_data (Optional[Dict[str, Any]]): Contexto adicional

        Returns:
            List[Dict[str, Any]]: Lista de resultados de an√°lise
        """
        tasks = [self.process_item_async(item, massive_data)
                 for item in batch_data]
        return await asyncio.gather(*tasks)

# --- FIM DO C√ìDIGO DE external_review_agent.py --- #


#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ARQV30 Enhanced v3.0 - External Review Agent
Agente principal de revis√£o externa - ponto de entrada do m√≥dulo
"""


# Handle both relative and absolute imports
# from services.sentiment_analyzer import ExternalSentimentAnalyzer
# from services.bias_disinformation_detector import ExternalBiasDisinformationDetector
# from services.llm_reasoning_service import ExternalLLMReasoningService
# from services.rule_engine import ExternalRuleEngine
# from services.contextual_analyzer import ExternalContextualAnalyzer
# from services.confidence_thresholds import ExternalConfidenceThresholds

logger = logging.getLogger(__name__)


class ExternalReviewAgent:
    """Agente de revis√£o externa - orquestrador principal do m√≥dulo"""

    def __init__(self, config: Optional[Dict[str, Any]] = None, config_path: Optional[str] = None):
        """
        Inicializa o agente de revis√£o externa

        Args:
            config_path (Optional[str]): Caminho para arquivo de configura√ß√£o
        """
        # Inicializar logger da inst√¢ncia
        self.logger = logging.getLogger(
            f"{__name__}.{self.__class__.__name__}")

        if config is not None:
            self.config = config
        else:
            self.config = self._load_config(config_path)

        # Initialize all analysis services
        self.sentiment_analyzer = ExternalSentimentAnalyzer(self.config)
        self.bias_detector = ExternalBiasDisinformationDetector(self.config)
        self.llm_service = ExternalLLMReasoningService(self.config)
        self.rule_engine = ExternalRuleEngine(self.config)
        self.contextual_analyzer = ExternalContextualAnalyzer(self.config)
        self.confidence_thresholds = ExternalConfidenceThresholds(self.config)

        # Processing statistics
        self.stats = {
            'total_processed': 0,
            'approved': 0,
            'rejected': 0,
            'start_time': datetime.now(),
            'processing_times': []
        }

        logger.info(f"‚úÖ External Review Agent inicializado com sucesso")
        logger.info(f"üîß Configura√ß√µes carregadas: {len(self.config)} se√ß√µes")

    def _load_config(self, config_path: Optional[str] = None) -> Dict[str, Any]:
        """
        Carrega configura√ß√£o do m√≥dulo

        Prioriza o caminho do arquivo de configura√ß√£o, mas se n√£o for fornecido ou n√£o existir,
        tenta carregar de um caminho padr√£o ou retorna a configura√ß√£o padr√£o.
        """
        try:
            # Default config path
            if config_path is None:
                # Assumindo que o config.yaml est√° no mesmo diret√≥rio ou em um subdiret√≥rio 'config'
                current_dir = os.path.dirname(os.path.abspath(__file__))
                # Tenta encontrar config.yaml no mesmo diret√≥rio do enhanced_synthesis_engine.py
                config_path = os.path.join(current_dir, 'config.yaml')
                if not os.path.exists(config_path):
                    # Se n√£o encontrar, tenta no diret√≥rio 'config' acima
                    config_path = os.path.join(
                        current_dir, '..', 'config', 'default_config.yaml')

            # Load configuration file
            if os.path.exists(config_path):
                with open(config_path, 'r', encoding='utf-8') as f:
                    if config_path.endswith('.yaml') or config_path.endswith('.yml'):
                        config = yaml.safe_load(f)
                    else:
                        config = json.load(f)
                logger.info(f"‚úÖ Configura√ß√£o carregada: {config_path}")
                return config
            else:
                logger.warning(
                    f"‚ö†Ô∏è Arquivo de configura√ß√£o n√£o encontrado: {config_path}. Usando configura√ß√£o padr√£o.")
                return self._get_default_config()

        except Exception as e:
            logger.error(
                f"Erro ao carregar configura√ß√£o: {e}. Usando configura√ß√£o padr√£o.")
            return self._get_default_config()

    def _get_default_config(self) -> Dict[str, Any]:
        """Retorna configura√ß√£o padr√£o"""
        return {
            'thresholds': {
                'approval': 0.75,
                'rejection': 0.35,
                'high_confidence': 0.85,
                'low_confidence': 0.5,
                'bias_high_risk': 0.7
            },
            'sentiment_analysis': {'enabled': True},
            'bias_detection': {'enabled': True},
            'llm_reasoning': {'enabled': True},
            'contextual_analysis': {'enabled': True},
            'rules': []
        }

    def process_item(self, item_data: Dict[str, Any], massive_data: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:
        """
        Processa um item individual atrav√©s de todas as an√°lises com valida√ß√£o aprimorada

        Args:
            item_data (Dict[str, Any]): Dados do item para an√°lise
            massive_data (Optional[Dict[str, Any]]): Contexto adicional

        Returns:
            Dict[str, Any]: Resultado completo da an√°lise
        """
        start_time = datetime.now()

        try:
            item_id = item_data.get(
                'id', f'item_{self.stats["total_processed"]}')
            logger.info(f"üîç Iniciando an√°lise do item: {item_id}")

            # Valida√ß√£o pr√©via do item
            validation_result = self._validate_item_data(item_data)
            if not validation_result['valid']:
                logger.warning(
                    f"‚ö†Ô∏è Item inv√°lido: {validation_result['reason']}")
                return self._create_validation_error_result(item_data, validation_result['reason'])

            # Extract text content for analysis
            text_content = self._extract_text_content(item_data)

            if not text_content or len(text_content.strip()) < 5:
                logger.warning("‚ö†Ô∏è Item com conte√∫do textual insuficiente")
                return self._create_insufficient_content_result(item_data)

            # Initialize analysis results
            analysis_result = {
                'item_id': item_data.get('id', f'item_{self.stats["total_processed"]}'),
                'original_item': item_data,
                'processing_timestamp': start_time.isoformat(),
                # First 500 chars for reference
                'text_analyzed': text_content[:500],
            }

            # Step 1: Sentiment Analysis
            logger.debug("Executando an√°lise de sentimento...")
            sentiment_result = self.sentiment_analyzer.analyze_sentiment(
                text_content)
            analysis_result['sentiment_analysis'] = sentiment_result

            # Step 2: Bias & Disinformation Detection
            logger.debug("Executando detec√ß√£o de vi√©s/desinforma√ß√£o...")
            bias_result = self.bias_detector.detect_bias_disinformation(
                text_content)
            analysis_result['bias_disinformation_analysis'] = bias_result

            # Step 3: LLM Reasoning (for ambiguous cases)
            should_use_llm = self._should_use_llm_analysis(
                sentiment_result, bias_result)
            if should_use_llm:
                logger.debug("Executando an√°lise LLM...")
                context = self._create_llm_context(
                    analysis_result, massive_data)
                llm_result = self.llm_service.analyze_with_llm(
                    text_content, context)
                analysis_result['llm_reasoning_analysis'] = llm_result
            else:
                analysis_result['llm_reasoning_analysis'] = {
                    'llm_confidence': 0.5,
                    'llm_recommendation': 'N√ÉO_EXECUTADO',
                    'analysis_reasoning': 'LLM n√£o necess√°rio para este item'
                }

            # Step 4: Contextual Analysis
            logger.debug("Executando an√°lise contextual...")
            contextual_result = self.contextual_analyzer.analyze_context(
                item_data, massive_data)
            analysis_result['contextual_analysis'] = contextual_result

            # Step 5: Rule Engine Application
            logger.debug("Aplicando regras de neg√≥cio...")
            rule_result = self.rule_engine.apply_rules(analysis_result)
            analysis_result['rule_decision'] = rule_result

            # Step 6: Final Decision
            final_decision = self._make_final_decision(analysis_result)
            analysis_result['ai_review'] = final_decision

            # Update statistics
            processing_time = (datetime.now() - start_time).total_seconds()
            self._update_stats(final_decision['status'], processing_time)

            analysis_result['processing_time_seconds'] = processing_time

            logger.info(
                f"‚úÖ Item processado: {final_decision['status']} (confian√ßa: {final_decision['final_confidence']:.3f})")

            return analysis_result

        except Exception as e:
            logger.error(f"Erro no processamento do item: {e}")
            error_result = self._create_error_result(item_data, str(e))
            self._update_stats(
                'error', (datetime.now() - start_time).total_seconds())
            return error_result

    def _validate_item_data(self, item_data: Dict[str, Any]) -> Dict[str, Any]:
        """Valida dados do item antes do processamento"""
        if not isinstance(item_data, dict):
            return {'valid': False, 'reason': 'Item deve ser um dicion√°rio'}

        if not item_data:
            return {'valid': False, 'reason': 'Item vazio'}

        # Verificar se tem pelo menos um campo de conte√∫do
        content_fields = ['content', 'text', 'title',
                          'description', 'summary', 'body']
        has_content = any(
            field in item_data and item_data[field] for field in content_fields)

        if not has_content:
            return {'valid': False, 'reason': 'Item n√£o possui conte√∫do textual v√°lido'}

        return {'valid': True, 'reason': 'Item v√°lido'}

    def _extract_text_content(self, item_data: Dict[str, Any]) -> str:
        """Extrai conte√∫do textual do item com prioriza√ß√£o"""
        # Campos priorizados por import√¢ncia
        priority_fields = ['content', 'text',
                           'description', 'summary', 'title', 'body']

        text_parts = []
        for field in priority_fields:
            if field in item_data and item_data[field]:
                content = str(item_data[field]).strip()
                if content:
                    text_parts.append(content)

        # Adicionar campos extras se existirem
        extra_fields = ['subtitle', 'excerpt', 'abstract', 'caption']
        for field in extra_fields:
            if field in item_data and item_data[field]:
                content = str(item_data[field]).strip()
                if content:
                    text_parts.append(content)

        return ' '.join(text_parts).strip()

    def _should_use_llm_analysis(self, sentiment_result: Dict[str, Any], bias_result: Dict[str, Any]) -> bool:
        """
        Determina se deve usar an√°lise LLM

        Args:
            sentiment_result (Dict[str, Any]): Resultado da an√°lise de sentimento.
            bias_result (Dict[str, Any]): Resultado da an√°lise de vi√©s e desinforma√ß√£o.

        Returns:
            bool: True se a an√°lise LLM deve ser usada, False caso contr√°rio.
        """
        # Use LLM for ambiguous cases or high-risk content
        sentiment_confidence = sentiment_result.get('confidence', 0.5)
        bias_risk = bias_result.get('overall_risk', 0.0)

        # Low confidence sentiment or high bias risk = use LLM
        # Thresholds can be configured in ExternalReviewAgent's config
        # For now, using hardcoded values for demonstration
        return sentiment_confidence < 0.6 or bias_risk > 0.4

    def _create_llm_context(self, analysis_result: Dict[str, Any], massive_data: Optional[Dict[str, Any]]) -> str:
        """
        Cria contexto para an√°lise LLM

        Args:
            analysis_result (Dict[str, Any]): Resultados parciais da an√°lise.
            massive_data (Optional[Dict[str, Any]]): Dados contextuais mais amplos.

        Returns:
            str: String de contexto para o LLM.
        """
        context_parts = []

        # Add sentiment context
        sentiment = analysis_result.get('sentiment_analysis', {})
        if sentiment.get('classification') != 'neutral':
            context_parts.append(
                f"Sentimento detectado: {sentiment.get('classification', 'indefinido')}")

        # Add bias context
        bias = analysis_result.get('bias_disinformation_analysis', {})
        if bias.get('overall_risk', 0) > 0.3:
            context_parts.append(
                f"Risco de vi√©s detectado: {bias.get('overall_risk', 0):.2f}")

        # Add any available external context
        if massive_data:
            if 'topic' in massive_data:
                context_parts.append(f"T√≥pico: {massive_data['topic']}")

        return ' '.join(context_parts) if context_parts else ""

    def _make_final_decision(self, analysis_result: Dict[str, Any]) -> Dict[str, Any]:
        """
        Toma decis√£o final baseada em todas as an√°lises

        Args:
            analysis_result (Dict[str, Any]): Resultados completos da an√°lise.

        Returns:
            Dict[str, Any]: Decis√£o final e detalhes.
        """
        try:
            # Get analysis results
            sentiment = analysis_result.get('sentiment_analysis', {})
            bias = analysis_result.get('bias_disinformation_analysis', {})
            llm = analysis_result.get('llm_reasoning_analysis', {})
            contextual = analysis_result.get('contextual_analysis', {})
            rule_decision = analysis_result.get('rule_decision', {})

            # Calculate composite confidence
            confidences = [
                sentiment.get('confidence', 0.5) * 0.2,  # 20% weight
                (1.0 - bias.get('overall_risk', 0.5)) *
                0.3,  # 30% weight (inverted risk)
                llm.get('llm_confidence', 0.5) * 0.3,  # 30% weight
                contextual.get('contextual_confidence', 0.5) *
                0.2  # 20% weight
            ]

            final_confidence = sum(confidences)

            # Apply rule engine decision if applicable
            if rule_decision.get('status') in ['approved', 'rejected']:
                status = rule_decision['status']
                reason = rule_decision['reason']
            else:
                # Use confidence thresholds for decision
                if self.confidence_thresholds.should_approve(final_confidence):
                    status = 'approved'
                    reason = 'Aprovado com base na an√°lise combinada'
                elif self.confidence_thresholds.should_reject(final_confidence):
                    status = 'rejected'
                    reason = 'Rejeitado com base na an√°lise combinada'
                else:
                    # Default to rejection for ambiguous cases (safer)
                    status = 'rejected'
                    reason = 'Rejeitado por ambiguidade - pol√≠tica de seguran√ßa'

            # Create comprehensive decision result
            decision = {
                'status': status,
                'reason': reason,
                'final_confidence': final_confidence,
                'confidence_breakdown': {
                    'sentiment_contribution': sentiment.get('confidence', 0.5) * 0.2,
                    'bias_contribution': (1.0 - bias.get('overall_risk', 0.5)) * 0.3,
                    'llm_contribution': llm.get('llm_confidence', 0.5) * 0.3,
                    'contextual_contribution': contextual.get('contextual_confidence', 0.5) * 0.2
                },
                'decision_factors': {
                    'sentiment_classification': sentiment.get('classification', 'neutral'),
                    'bias_risk_level': 'high' if bias.get('overall_risk', 0) > 0.6 else 'medium' if bias.get('overall_risk', 0) > 0.3 else 'low',
                    'llm_recommendation': llm.get('llm_recommendation', 'N√ÉO_EXECUTADO'),
                    'rule_triggered': rule_decision.get('triggered_rules', [])
                },
                'analysis_summary': {
                    'total_flags': (
                        len(bias.get('detected_bias_keywords', [])) +
                        len(bias.get('detected_disinformation_patterns', [])) +
                        len(contextual.get('context_flags', []))
                    ),
                    'sentiment_polarity': sentiment.get('polarity', 0.0),
                    'overall_risk_score': bias.get('overall_risk', 0.0),
                    'contextual_consistency': contextual.get('consistency_score', 0.5)
                },
                'processing_metadata': {
                    'timestamp': datetime.now().isoformat(),
                    'version': '3.0.0',
                    'confidence_threshold_used': self.confidence_thresholds.get_threshold('approval')
                }
            }

            return decision

        except Exception as e:
            logger.error(f"Erro na decis√£o final: {e}")
            return {
                'status': 'rejected',
                'reason': f'Erro no processamento: {str(e)}',
                'final_confidence': 0.0,
                'error': True
            }

    def _create_validation_error_result(self, item_data: Dict[str, Any], reason: str) -> Dict[str, Any]:
        """Cria resultado para item que falhou na valida√ß√£o"""
        return {
            'item_id': item_data.get('id', 'sem_id'),
            'original_item': item_data,
            'processing_timestamp': datetime.now().isoformat(),
            'ai_review': {
                'status': 'rejected',
                'reason': f'Erro de valida√ß√£o: {reason}',
                'final_confidence': 0.0,
                'error': True,
                'validation_error': True
            },
            'sentiment_analysis': {},
            'bias_disinformation_analysis': {},
            'llm_reasoning_analysis': {},
            'contextual_analysis': {},
            'rule_decision': {}
        }

    def _create_insufficient_content_result(self, item_data: Dict[str, Any]) -> Dict[str, Any]:
        """Cria resultado para item com conte√∫do insuficiente"""
        return {
            'item_id': item_data.get('id', 'sem_id'),
            'original_item': item_data,
            'processing_timestamp': datetime.now().isoformat(),
            'ai_review': {
                'status': 'rejected',
                'reason': 'Conte√∫do textual insuficiente para an√°lise',
                'final_confidence': 0.0,
                'error': True,
                'insufficient_content': True
            },
            'sentiment_analysis': {},
            'bias_disinformation_analysis': {},
            'llm_reasoning_analysis': {},
            'contextual_analysis': {},
            'rule_decision': {}
        }

    def _create_error_result(self, item_data: Dict[str, Any], error_message: str) -> Dict[str, Any]:
        """Cria resultado de erro gen√©rico"""
        return {
            'item_id': item_data.get('id', 'sem_id'),
            'original_item': item_data,
            'processing_timestamp': datetime.now().isoformat(),
            'ai_review': {
                'status': 'rejected',
                'reason': f'Erro inesperado durante o processamento: {error_message}',
                'final_confidence': 0.0,
                'error': True
            },
            'sentiment_analysis': {},
            'bias_disinformation_analysis': {},
            'llm_reasoning_analysis': {},
            'contextual_analysis': {},
            'rule_decision': {}
        }

    def _update_stats(self, status: str, processing_time: float):
        """Atualiza estat√≠sticas de processamento"""
        self.stats['total_processed'] += 1
        if status == 'approved':
            self.stats['approved'] += 1
        elif status == 'rejected':
            self.stats['rejected'] += 1
        self.stats['processing_times'].append(processing_time)

    def get_processing_stats(self) -> Dict[str, Any]:
        """
        Retorna estat√≠sticas de processamento

        Returns:
            Dict[str, Any]: Estat√≠sticas de processamento
        """
        total_time = (datetime.now() -
                      self.stats['start_time']).total_seconds()
        avg_processing_time = sum(self.stats['processing_times']) / len(
            self.stats['processing_times']) if self.stats['processing_times'] else 0

        return {
            'total_processed_items': self.stats['total_processed'],
            'approved_items': self.stats['approved'],
            'rejected_items': self.stats['rejected'],
            'total_processing_time_seconds': total_time,
            'average_item_processing_time_seconds': avg_processing_time,
            'uptime_seconds': total_time
        }

    def update_config(self, new_config: Dict[str, Any]):
        """
        Atualiza a configura√ß√£o do agente e seus subm√≥dulos

        Args:
            new_config (Dict[str, Any]): Nova configura√ß√£o a ser aplicada
        """
        self.config.update(new_config)
        self.sentiment_analyzer = ExternalSentimentAnalyzer(self.config)
        self.bias_detector = ExternalBiasDisinformationDetector(self.config)
        self.llm_service = ExternalLLMReasoningService(self.config)
        self.rule_engine = ExternalRuleEngine(self.config)
        self.contextual_analyzer = ExternalContextualAnalyzer(self.config)
        self.confidence_thresholds = ExternalConfidenceThresholds(self.config)
        logger.info("‚öôÔ∏è Configura√ß√£o do External Review Agent atualizada.")

    async def process_item_async(self, item_data: Dict[str, Any], massive_data: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:
        """
        Processa um item individual de forma ass√≠ncrona (exemplo)

        Args:
            item_data (Dict[str, Any]): Dados do item para an√°lise
            massive_data (Optional[Dict[str, Any]]): Contexto adicional

        Returns:
            Dict[str, Any]: Resultado completo da an√°lise
        """
        # Para simplificar, este √© um wrapper s√≠ncrono. Em um ambiente real,
        # as chamadas internas seriam awaitable.
        return self.process_item(item_data, massive_data)

    async def process_batch_async(self, batch_data: List[Dict[str, Any]], massive_data: Optional[Dict[str, Any]] = None) -> List[Dict[str, Any]]:
        """
        Processa um lote de itens de forma ass√≠ncrona

        Args:
            batch_data (List[Dict[str, Any]]): Lista de itens para an√°lise
            massive_data (Optional[Dict[str, Any]]): Contexto adicional

        Returns:
            List[Dict[str, Any]]: Lista de resultados de an√°lise
        """
        tasks = [self.process_item_async(item, massive_data)
                 for item in batch_data]
        return await asyncio.gather(*tasks)

# --- FIM DO C√ìDIGO DE external_review_agent.py --- #


#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ARQV30 Enhanced v3.0 - External Review Agent
Agente principal de revis√£o externa - ponto de entrada do m√≥dulo
"""


# Handle both relative and absolute imports
# from services.sentiment_analyzer import ExternalSentimentAnalyzer
# from services.bias_disinformation_detector import ExternalBiasDisinformationDetector
# from services.llm_reasoning_service import ExternalLLMReasoningService
# from services.rule_engine import ExternalRuleEngine
# from services.contextual_analyzer import ExternalContextualAnalyzer
# from services.confidence_thresholds import ExternalConfidenceThresholds

logger = logging.getLogger(__name__)


class ExternalReviewAgent:
    """Agente de revis√£o externa - orquestrador principal do m√≥dulo"""

    def __init__(self, config: Optional[Dict[str, Any]] = None, config_path: Optional[str] = None):
        """
        Inicializa o agente de revis√£o externa

        Args:
            config_path (Optional[str]): Caminho para arquivo de configura√ß√£o
        """
        # Inicializar logger da inst√¢ncia
        self.logger = logging.getLogger(
            f"{__name__}.{self.__class__.__name__}")

        if config is not None:
            self.config = config
        else:
            self.config = self._load_config(config_path)

        # Initialize all analysis services
        self.sentiment_analyzer = ExternalSentimentAnalyzer(self.config)
        self.bias_detector = ExternalBiasDisinformationDetector(self.config)
        self.llm_service = ExternalLLMReasoningService(self.config)
        self.rule_engine = ExternalRuleEngine(self.config)
        self.contextual_analyzer = ExternalContextualAnalyzer(self.config)
        self.confidence_thresholds = ExternalConfidenceThresholds(self.config)

        # Processing statistics
        self.stats = {
            'total_processed': 0,
            'approved': 0,
            'rejected': 0,
            'start_time': datetime.now(),
            'processing_times': []
        }

        logger.info(f"‚úÖ External Review Agent inicializado com sucesso")
        logger.info(f"üîß Configura√ß√µes carregadas: {len(self.config)} se√ß√µes")

    def _load_config(self, config_path: Optional[str] = None) -> Dict[str, Any]:
        """
        Carrega configura√ß√£o do m√≥dulo

        Prioriza o caminho do arquivo de configura√ß√£o, mas se n√£o for fornecido ou n√£o existir,
        tenta carregar de um caminho padr√£o ou retorna a configura√ß√£o padr√£o.
        """
        try:
            # Default config path
            if config_path is None:
                # Assumindo que o config.yaml est√° no mesmo diret√≥rio ou em um subdiret√≥rio 'config'
                current_dir = os.path.dirname(os.path.abspath(__file__))
                # Tenta encontrar config.yaml no mesmo diret√≥rio do enhanced_synthesis_engine.py
                config_path = os.path.join(current_dir, 'config.yaml')
                if not os.path.exists(config_path):
                    # Se n√£o encontrar, tenta no diret√≥rio 'config' acima
                    config_path = os.path.join(
                        current_dir, '..', 'config', 'default_config.yaml')

            # Load configuration file
            if os.path.exists(config_path):
                with open(config_path, 'r', encoding='utf-8') as f:
                    if config_path.endswith('.yaml') or config_path.endswith('.yml'):
                        config = yaml.safe_load(f)
                    else:
                        config = json.load(f)
                logger.info(f"‚úÖ Configura√ß√£o carregada: {config_path}")
                return config
            else:
                logger.warning(
                    f"‚ö†Ô∏è Arquivo de configura√ß√£o n√£o encontrado: {config_path}. Usando configura√ß√£o padr√£o.")
                return self._get_default_config()

        except Exception as e:
            logger.error(
                f"Erro ao carregar configura√ß√£o: {e}. Usando configura√ß√£o padr√£o.")
            return self._get_default_config()

    def _get_default_config(self) -> Dict[str, Any]:
        """
        Retorna configura√ß√£o padr√£o
        """
        return {
            'thresholds': {
                'approval': 0.75,
                'rejection': 0.35,
                'high_confidence': 0.85,
                'low_confidence': 0.5,
                'bias_high_risk': 0.7
            },
            'sentiment_analysis': {'enabled': True},
            'bias_detection': {'enabled': True},
            'llm_reasoning': {'enabled': True},
            'contextual_analysis': {'enabled': True},
            'rules': []
        }

    def process_item(self, item_data: Dict[str, Any], massive_data: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:
        """
        Processa um item individual atrav√©s de todas as an√°lises com valida√ß√£o aprimorada

        Args:
            item_data (Dict[str, Any]): Dados do item para an√°lise
            massive_data (Optional[Dict[str, Any]]): Contexto adicional

        Returns:
            Dict[str, Any]: Resultado completo da an√°lise
        """
        start_time = datetime.now()

        try:
            item_id = item_data.get(
                'id', f'item_{self.stats["total_processed"]}')
            logger.info(f"üîç Iniciando an√°lise do item: {item_id}")

            # Valida√ß√£o pr√©via do item
            validation_result = self._validate_item_data(item_data)
            if not validation_result['valid']:
                logger.warning(
                    f"‚ö†Ô∏è Item inv√°lido: {validation_result['reason']}")
                return self._create_validation_error_result(item_data, validation_result['reason'])

            # Extract text content for analysis
            text_content = self._extract_text_content(item_data)

            if not text_content or len(text_content.strip()) < 5:
                logger.warning("‚ö†Ô∏è Item com conte√∫do textual insuficiente")
                return self._create_insufficient_content_result(item_data)

            # Initialize analysis results
            analysis_result = {
                'item_id': item_data.get('id', f'item_{self.stats["total_processed"]}'),
                'original_item': item_data,
                'processing_timestamp': start_time.isoformat(),
                # First 500 chars for reference
                'text_analyzed': text_content[:500],
            }

            # Step 1: Sentiment Analysis
            logger.debug("Executando an√°lise de sentimento...")
            sentiment_result = self.sentiment_analyzer.analyze_sentiment(
                text_content)
            analysis_result['sentiment_analysis'] = sentiment_result

            # Step 2: Bias & Disinformation Detection
            logger.debug("Executando detec√ß√£o de vi√©s/desinforma√ß√£o...")
            bias_result = self.bias_detector.detect_bias_disinformation(
                text_content)
            analysis_result['bias_disinformation_analysis'] = bias_result

            # Step 3: LLM Reasoning (for ambiguous cases)
            should_use_llm = self._should_use_llm_analysis(
                sentiment_result, bias_result)
            if should_use_llm:
                logger.debug("Executando an√°lise LLM...")
                context = self._create_llm_context(
                    analysis_result, massive_data)
                llm_result = self.llm_service.analyze_with_llm(
                    text_content, context)
                analysis_result['llm_reasoning_analysis'] = llm_result
            else:
                analysis_result['llm_reasoning_analysis'] = {
                    'llm_confidence': 0.5,
                    'llm_recommendation': 'N√ÉO_EXECUTADO',
                    'analysis_reasoning': 'LLM n√£o necess√°rio para este item'
                }

            # Step 4: Contextual Analysis
            logger.debug("Executando an√°lise contextual...")
            contextual_result = self.contextual_analyzer.analyze_context(
                item_data, massive_data)
            analysis_result['contextual_analysis'] = contextual_result

            # Step 5: Rule Engine Application
            logger.debug("Aplicando regras de neg√≥cio...")
            rule_result = self.rule_engine.apply_rules(analysis_result)
            analysis_result['rule_decision'] = rule_result

            # Step 6: Final Decision
            final_decision = self._make_final_decision(analysis_result)
            analysis_result['ai_review'] = final_decision

            # Update statistics
            processing_time = (datetime.now() - start_time).total_seconds()
            self._update_stats(final_decision['status'], processing_time)

            analysis_result['processing_time_seconds'] = processing_time

            logger.info(
                f"‚úÖ Item processado: {final_decision['status']} (confian√ßa: {final_decision['final_confidence']:.3f})")

            return analysis_result

        except Exception as e:
            logger.error(f"Erro no processamento do item: {e}")
            error_result = self._create_error_result(item_data, str(e))
            self._update_stats(
                'error', (datetime.now() - start_time).total_seconds())
            return error_result

    def _validate_item_data(self, item_data: Dict[str, Any]) -> Dict[str, Any]:
        """Valida dados do item antes do processamento"""
        if not isinstance(item_data, dict):
            return {'valid': False, 'reason': 'Item deve ser um dicion√°rio'}

        if not item_data:
            return {'valid': False, 'reason': 'Item vazio'}

        # Verificar se tem pelo menos um campo de conte√∫do
        content_fields = ['content', 'text', 'title',
                          'description', 'summary', 'body']
        has_content = any(
            field in item_data and item_data[field] for field in content_fields)

        if not has_content:
            return {'valid': False, 'reason': 'Item n√£o possui conte√∫do textual v√°lido'}

        return {'valid': True, 'reason': 'Item v√°lido'}

    def _extract_text_content(self, item_data: Dict[str, Any]) -> str:
        """Extrai conte√∫do textual do item com prioriza√ß√£o"""
        # Campos priorizados por import√¢ncia
        priority_fields = ['content', 'text',
                           'description', 'summary', 'title', 'body']

        text_parts = []
        for field in priority_fields:
            if field in item_data and item_data[field]:
                content = str(item_data[field]).strip()
                if content:
                    text_parts.append(content)

        # Adicionar campos extras se existirem
        extra_fields = ['subtitle', 'excerpt', 'abstract', 'caption']
        for field in extra_fields:
            if field in item_data and item_data[field]:
                content = str(item_data[field]).strip()
                if content:
                    text_parts.append(content)

        return ' '.join(text_parts).strip()

    def _should_use_llm_analysis(self, sentiment_result: Dict[str, Any], bias_result: Dict[str, Any]) -> bool:
        """
        Determina se deve usar an√°lise LLM

        Args:
            sentiment_result (Dict[str, Any]): Resultado da an√°lise de sentimento.
            bias_result (Dict[str, Any]): Resultado da an√°lise de vi√©s e desinforma√ß√£o.

        Returns:
            bool: True se a an√°lise LLM deve ser usada, False caso contr√°rio.
        """
        # Use LLM for ambiguous cases or high-risk content
        sentiment_confidence = sentiment_result.get('confidence', 0.5)
        bias_risk = bias_result.get('overall_risk', 0.0)

        # Low confidence sentiment or high bias risk = use LLM
        # Thresholds can be configured in ExternalReviewAgent's config
        # For now, using hardcoded values for demonstration
        return sentiment_confidence < 0.6 or bias_risk > 0.4

    def _create_llm_context(self, analysis_result: Dict[str, Any], massive_data: Optional[Dict[str, Any]]) -> str:
        """
        Cria contexto para an√°lise LLM

        Args:
            analysis_result (Dict[str, Any]): Resultados parciais da an√°lise.
            massive_data (Optional[Dict[str, Any]]): Dados contextuais mais amplos.

        Returns:
            str: String de contexto para o LLM.
        """
        context_parts = []

        # Add sentiment context
        sentiment = analysis_result.get('sentiment_analysis', {})
        if sentiment.get('classification') != 'neutral':
            context_parts.append(
                f"Sentimento detectado: {sentiment.get('classification', 'indefinido')}")

        # Add bias context
        bias = analysis_result.get('bias_disinformation_analysis', {})
        if bias.get('overall_risk', 0) > 0.3:
            context_parts.append(
                f"Risco de vi√©s detectado: {bias.get('overall_risk', 0):.2f}")

        # Add any available external context
        if massive_data:
            if 'topic' in massive_data:
                context_parts.append(f"T√≥pico: {massive_data['topic']}")

        return ' | '.join(context_parts) if context_parts else ""

    def _make_final_decision(self, analysis_result: Dict[str, Any]) -> Dict[str, Any]:
        """
        Toma decis√£o final baseada em todas as an√°lises

        Args:
            analysis_result (Dict[str, Any]): Resultados completos da an√°lise.

        Returns:
            Dict[str, Any]: Decis√£o final e detalhes.
        """
        try:
            # Get analysis results
            sentiment = analysis_result.get('sentiment_analysis', {})
            bias = analysis_result.get('bias_disinformation_analysis', {})
            llm = analysis_result.get('llm_reasoning_analysis', {})
            contextual = analysis_result.get('contextual_analysis', {})
            rule_decision = analysis_result.get('rule_decision', {})

            # Calculate composite confidence
            confidences = [
                sentiment.get('confidence', 0.5) * 0.2,  # 20% weight
                (1.0 - bias.get('overall_risk', 0.5)) *
                0.3,  # 30% weight (inverted risk)
                llm.get('llm_confidence', 0.5) * 0.3,  # 30% weight
                contextual.get('contextual_confidence', 0.5) *
                0.2  # 20% weight
            ]

            final_confidence = sum(confidences)

            # Apply rule engine decision if applicable
            if rule_decision.get('status') in ['approved', 'rejected']:
                status = rule_decision['status']
                reason = rule_decision['reason']
            else:
                # Use confidence thresholds for decision
                if self.confidence_thresholds.should_approve(final_confidence):
                    status = 'approved'
                    reason = 'Aprovado com base na an√°lise combinada'
                elif self.confidence_thresholds.should_reject(final_confidence):
                    status = 'rejected'
                    reason = 'Rejeitado com base na an√°lise combinada'
                else:
                    # Default to rejection for ambiguous cases (safer)
                    status = 'rejected'
                    reason = 'Rejeitado por ambiguidade - pol√≠tica de seguran√ßa'

            # Create comprehensive decision result
            decision = {
                'status': status,
                'reason': reason,
                'final_confidence': final_confidence,
                'confidence_breakdown': {
                    'sentiment_contribution': sentiment.get('confidence', 0.5) * 0.2,
                    'bias_contribution': (1.0 - bias.get('overall_risk', 0.5)) * 0.3,
                    'llm_contribution': llm.get('llm_confidence', 0.5) * 0.3,
                    'contextual_contribution': contextual.get('contextual_confidence', 0.5) * 0.2
                },
                'decision_factors': {
                    'sentiment_classification': sentiment.get('classification', 'neutral'),
                    'bias_risk_level': 'high' if bias.get('overall_risk', 0) > 0.6 else 'medium' if bias.get('overall_risk', 0) > 0.3 else 'low',
                    'llm_recommendation': llm.get('llm_recommendation', 'N√ÉO_EXECUTADO'),
                    'rule_triggered': rule_decision.get('triggered_rules', [])
                },
                'analysis_summary': {
                    'total_flags': (
                        len(bias.get('detected_bias_keywords', [])) +
                        len(bias.get('detected_disinformation_patterns', [])) +
                        len(contextual.get('context_flags', []))
                    ),
                    'sentiment_polarity': sentiment.get('polarity', 0.0),
                    'overall_risk_score': bias.get('overall_risk', 0.0),
                    'contextual_consistency': contextual.get('consistency_score', 0.5)
                },
                'processing_metadata': {
                    'timestamp': datetime.now().isoformat(),
                    'version': '3.0.0',
                    'confidence_threshold_used': self.confidence_thresholds.get_threshold('approval')
                }
            }

            return decision

        except Exception as e:
            logger.error(f"Erro na decis√£o final: {e}")
            return {
                'status': 'rejected',
                'reason': f'Erro no processamento: {str(e)}',
                'final_confidence': 0.0,
                'error': True
            }

    def _create_validation_error_result(self, item_data: Dict[str, Any], reason: str) -> Dict[str, Any]:
        """Cria resultado para item que falhou na valida√ß√£o"""
        return {
            'item_id': item_data.get('id', 'sem_id'),
            'original_item': item_data,
            'processing_timestamp': datetime.now().isoformat(),
            'ai_review': {
                'status': 'rejected',
                'reason': f'Erro de valida√ß√£o: {reason}',
                'final_confidence': 0.0,
                'error': True,
                'validation_error': True
            },
            'sentiment_analysis': {},
            'bias_disinformation_analysis': {},
            'llm_reasoning_analysis': {},
            'contextual_analysis': {},
            'rule_decision': {}
        }

    def _create_insufficient_content_result(self, item_data: Dict[str, Any]) -> Dict[str, Any]:
        """Cria resultado para item com conte√∫do insuficiente"""
        return {
            'item_id': item_data.get('id', 'sem_id'),
            'original_item': item_data,
            'processing_timestamp': datetime.now().isoformat(),
            'ai_review': {
                'status': 'rejected',
                'reason': 'Conte√∫do textual insuficiente para an√°lise',
                'final_confidence': 0.0,
                'error': True,
                'insufficient_content': True
            },
            'sentiment_analysis': {},
            'bias_disinformation_analysis': {},
            'llm_reasoning_analysis': {},
            'contextual_analysis': {},
            'rule_decision': {}
        }

    def _create_error_result(self, item_data: Dict[str, Any], error_message: str) -> Dict[str, Any]:
        """Cria resultado de erro gen√©rico"""
        return {
            'item_id': item_data.get('id', 'sem_id'),
            'original_item': item_data,
            'processing_timestamp': datetime.now().isoformat(),
            'ai_review': {
                'status': 'rejected',
                'reason': f'Erro inesperado durante o processamento: {error_message}',
                'final_confidence': 0.0,
                'error': True
            },
            'sentiment_analysis': {},
            'bias_disinformation_analysis': {},
            'llm_reasoning_analysis': {},
            'contextual_analysis': {},
            'rule_decision': {}
        }

    def _update_stats(self, status: str, processing_time: float):
        """
        Atualiza estat√≠sticas de processamento
        """
        self.stats['total_processed'] += 1
        if status == 'approved':
            self.stats['approved'] += 1
        elif status == 'rejected':
            self.stats['rejected'] += 1
        self.stats['processing_times'].append(processing_time)

    def get_processing_stats(self) -> Dict[str, Any]:
        """
        Retorna estat√≠sticas de processamento

        Returns:
            Dict[str, Any]: Estat√≠sticas de processamento
        """
        total_time = (datetime.now() -
                      self.stats['start_time']).total_seconds()
        avg_processing_time = sum(self.stats['processing_times']) / len(
            self.stats['processing_times']) if self.stats['processing_times'] else 0

        return {
            'total_processed_items': self.stats['total_processed'],
            'approved_items': self.stats['approved'],
            'rejected_items': self.stats['rejected'],
            'total_processing_time_seconds': total_time,
            'average_item_processing_time_seconds': avg_processing_time,
            'uptime_seconds': total_time
        }

    def update_config(self, new_config: Dict[str, Any]):
        """
        Atualiza a configura√ß√£o do agente e seus subm√≥dulos

        Args:
            new_config (Dict[str, Any]): Nova configura√ß√£o a ser aplicada
        """
        self.config.update(new_config)
        self.sentiment_analyzer = ExternalSentimentAnalyzer(self.config)
        self.bias_detector = ExternalBiasDisinformationDetector(self.config)
        self.llm_service = ExternalLLMReasoningService(self.config)
        self.rule_engine = ExternalRuleEngine(self.config)
        self.contextual_analyzer = ExternalContextualAnalyzer(self.config)
        self.confidence_thresholds = ExternalConfidenceThresholds(self.config)
        logger.info("‚öôÔ∏è Configura√ß√£o do External Review Agent atualizada.")

    async def process_item_async(self, item_data: Dict[str, Any], massive_data: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:
        """
        Processa um item individual de forma ass√≠ncrona (exemplo)

        Args:
            item_data (Dict[str, Any]): Dados do item para an√°lise
            massive_data (Optional[Dict[str, Any]]): Contexto adicional

        Returns:
            Dict[str, Any]: Resultado completo da an√°lise
        """
        # Para simplificar, este √© um wrapper s√≠ncrono. Em um ambiente real,
        # as chamadas internas seriam awaitable.
        return self.process_item(item_data, massive_data)

    async def process_batch_async(self, batch_data: List[Dict[str, Any]], massive_data: Optional[Dict[str, Any]] = None) -> List[Dict[str, Any]]:
        """
        Processa um lote de itens de forma ass√≠ncrona

        Args:
            batch_data (List[Dict[str, Any]]): Lista de itens para an√°lise
            massive_data (Optional[Dict[str, Any]]): Contexto adicional

        Returns:
            List[Dict[str, Any]]: Lista de resultados de an√°lise
        """
        tasks = [self.process_item_async(item, massive_data)
                 for item in batch_data]
        return await asyncio.gather(*tasks)

# --- FIM DO C√ìDIGO DE external_review_agent.py --- #


# --- IN√çCIO DO C√ìDIGO DE rule_engine.py --- #
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ARQV30 Enhanced v3.0 - External Rule Engine
Motor de regras para o m√≥dulo externo de verifica√ß√£o por IA
"""


logger = logging.getLogger(__name__)


class ExternalRuleEngine:
    """Motor de regras externo independente"""

    def __init__(self, config: Dict[str, Any]):
        """
        Inicializa o motor de regras
        """
        self.rules = config.get("rules", [])

        # Ensure we have default rules if none provided
        if not self.rules:
            self.rules = self._get_default_rules()

        logger.info(
            f"‚úÖ External Rule Engine inicializado com {len(self.rules)} regras")
        self._log_rules()

    def _get_default_rules(self) -> List[Dict[str, Any]]:
        """
        Retorna regras padr√£o se nenhuma for configurada
        """
        return [
            {
                "name": "high_confidence_approval",
                "condition": "overall_confidence >= 0.85",
                "action": {
                    "status": "approved",
                    "reason": "Alta confian√ßa no conte√∫do",
                    "confidence_adjustment": 0.0
                }
            },
            {
                "name": "low_confidence_rejection",
                "condition": "overall_confidence <= 0.35",
                "action": {
                    "status": "rejected",
                    "reason": "Confian√ßa muito baixa",
                    "confidence_adjustment": -0.1
                }
            },
            {
                "name": "high_risk_bias_rejection",
                "condition": "overall_risk >= 0.7",
                "action": {
                    "status": "rejected",
                    "reason": "Alto risco de vi√©s/desinforma√ß√£o detectado",
                    "confidence_adjustment": -0.2
                }
            },
            {
                "name": "llm_rejection_override",
                "condition": "llm_recommendation == 'REJEITAR'",
                "action": {
                    "status": "rejected",
                    "reason": "Rejeitado por an√°lise LLM",
                    "confidence_adjustment": -0.1
                }
            }
        ]

    def apply_rules(self, item_data: Dict[str, Any]) -> Dict[str, Any]:
        """
        Aplica regras aos dados do item

        Args:
            item_data (Dict[str, Any]): Dados do item para an√°lise

        Returns:
            Dict[str, Any]: Resultado da aplica√ß√£o das regras
        """
        try:
            # Initialize decision result
            decision = {
                "status": "approved",  # Default to approved if no rules trigger
                "reason": "Nenhuma regra espec√≠fica ativada",
                "confidence_adjustment": 0.0,
                "triggered_rules": []
            }

            # Extract relevant scores from item_data
            validation_scores = item_data.get("validation_scores", {})
            sentiment_analysis = item_data.get("sentiment_analysis", {})
            bias_analysis = item_data.get("bias_disinformation_analysis", {})
            llm_analysis = item_data.get("llm_reasoning_analysis", {})

            # Calculate overall confidence and risk
            overall_confidence = self._calculate_overall_confidence(
                validation_scores, sentiment_analysis, bias_analysis, llm_analysis)
            overall_risk = bias_analysis.get("overall_risk", 0.0)
            llm_recommendation = llm_analysis.get(
                "llm_recommendation", "REVIS√ÉO_MANUAL")

            # Apply each rule in order
            for rule in self.rules:
                if self._evaluate_condition(rule, overall_confidence, overall_risk, llm_recommendation, item_data):
                    rule_name = rule.get("name", "unknown_rule")
                    action = rule.get("action", {})

                    # Update decision
                    decision["status"] = action.get("status", "approved")
                    decision["reason"] = action.get(
                        "reason", f"Regra '{rule_name}' ativada")
                    decision["confidence_adjustment"] = action.get(
                        "confidence_adjustment", 0.0)
                    decision["triggered_rules"].append(rule_name)

                    logger.debug(
                        f"Regra '{rule_name}' ativada: {decision['status']} - {decision['reason']}")

                    # Stop at first matching rule (rules should be ordered by priority)
                    break

            return decision

        except Exception as e:
            logger.error(f"Erro ao aplicar regras: {e}")
            return {
                "status": "rejected",  # Fail safe - reject on error
                "reason": f"Erro no processamento de regras: {str(e)}",
                "confidence_adjustment": -0.3,
                "triggered_rules": ["error_fallback"]
            }

    def _evaluate_condition(self, rule: Dict[str, Any], overall_confidence: float, overall_risk: float, llm_recommendation: str, item_data: Dict[str, Any]) -> bool:
        """
        Avalia se a condi√ß√£o de uma regra √© atendida

        Args:
            rule: Regra para avaliar
            overall_confidence: Confian√ßa geral calculada
            overall_risk: Risco geral calculado
            llm_recommendation: Recomenda√ß√£o do LLM
            item_data: Dados completos do item

        Returns:
            bool: True se a condi√ß√£o for atendida
        """
        try:
            condition = rule.get("condition", "")

            if not condition:
                return False

            # Simple condition evaluation
            # Replace variables in condition string
            condition = condition.replace(
                "overall_confidence", str(overall_confidence))
            condition = condition.replace("overall_risk", str(overall_risk))
            condition = condition.replace(
                "llm_recommendation", f"'{llm_recommendation}'")

            # Evaluate mathematical expressions
            if any(op in condition for op in [">=", "<=", "==", ">", "<", "!="]):
                try:
                    # Safe evaluation of simple mathematical conditions
                    return self._safe_eval_condition(condition)
                except:
                    logger.warning(f"Erro ao avaliar condi√ß√£o: {condition}")
                    return False

            return False

        except Exception as e:
            logger.error(f"Erro na avalia√ß√£o da condi√ß√£o: {e}")
            return False

    def _safe_eval_condition(self, condition: str) -> bool:
        """
        Avalia condi√ß√µes matem√°ticas simples de forma segura

        Args:
            condition (str): Condi√ß√£o para avaliar

        Returns:
            bool: Resultado da avalia√ß√£o
        """
        try:
            # Only allow safe mathematical operations and comparisons
            allowed_chars = set(
                "0123456789.><=!ABCDEFGHIJKLMNOPQRSTUVWXYZ_abcdefghijklmnopqrstuvwxyz")

            if not all(c in allowed_chars for c in condition):
                logger.warning(
                    f"Caracteres n√£o permitidos na condi√ß√£o: {condition}")
                return False

            # Simple string replacements for evaluation
            if ">=" in condition:
                parts = condition.split(">=")
                if len(parts) == 2:
                    try:
                        left = float(parts[0].strip())
                        right = float(parts[1].strip())
                        return left >= right
                    except ValueError:
                        # Handle string comparisons
                        return parts[0].strip() == parts[1].strip()

            elif "<=" in condition:
                parts = condition.split("<=")
                if len(parts) == 2:
                    left = float(parts[0].strip())
                    right = float(parts[1].strip())
                    return left <= right

            elif "==" in condition:
                parts = condition.split("==")
                if len(parts) == 2:
                    left = parts[0].strip().strip("'\"")
                    right = parts[1].strip().strip("'\"")
                    return left == right

            elif ">" in condition:
                parts = condition.split(">")
                if len(parts) == 2:
                    left = float(parts[0].strip())
                    right = float(parts[1].strip())
                    return left > right

            elif "<" in condition:
                parts = condition.split("<")
                if len(parts) == 2:
                    left = float(parts[0].strip())
                    right = float(parts[1].strip())
                    return left < right

            return False

        except Exception as e:
            logger.error(f"Erro na avalia√ß√£o segura da condi√ß√£o: {e}")
            return False

    def _calculate_overall_confidence(self, validation_scores: Dict[str, Any], sentiment_analysis: Dict[str, Any], bias_analysis: Dict[str, Any], llm_analysis: Dict[str, Any]) -> float:
        """
        Calcula confian√ßa geral baseada em todas as an√°lises
        """
        try:
            # Start with base validation confidence
            base_confidence = validation_scores.get("overall_confidence", 0.5)

            # Adjust based on sentiment analysis
            sentiment_confidence = sentiment_analysis.get("confidence", 0.5)
            sentiment_weight = 0.2

            # Adjust based on bias analysis (lower bias risk = higher confidence)
            # Invert risk to confidence
            bias_confidence = 1.0 - bias_analysis.get("overall_risk", 0.5)
            bias_weight = 0.3

            # Adjust based on LLM analysis
            llm_confidence = llm_analysis.get("llm_confidence", 0.5)
            llm_weight = 0.4

            # Weighted combination
            overall_confidence = (
                base_confidence * (1.0 - sentiment_weight - bias_weight - llm_weight) +
                sentiment_confidence * sentiment_weight +
                bias_confidence * bias_weight +
                llm_confidence * llm_weight
            )

            return min(max(overall_confidence, 0.0), 1.0)

        except Exception as e:
            logger.warning(f"Erro no c√°lculo de confian√ßa geral: {e}")
            return 0.5

    def _log_rules(self):
        """
        Log das regras configuradas
        """
        logger.debug("Regras configuradas:")
        for i, rule in enumerate(self.rules):
            logger.debug(
                f"  {i+1}. {rule.get('name', 'sem_nome')}: {rule.get('condition', 'sem_condi√ß√£o')}")

    def add_rule(self, rule: Dict[str, Any]):
        """
        Adiciona uma nova regra

        Args:
            rule (Dict[str, Any]): Nova regra para adicionar
        """
        if self._validate_rule(rule):
            self.rules.append(rule)
            logger.info(
                f"Nova regra adicionada: {rule.get('name', 'sem_nome')}")
        else:
            logger.warning(f"Regra inv√°lida rejeitada: {rule}")

    def _validate_rule(self, rule: Dict[str, Any]) -> bool:
        """
        Valida se uma regra est√° bem formada
        """
        return (
            isinstance(rule, dict) and
            "condition" in rule and
            "action" in rule and
            isinstance(rule["action"], dict)
        )


# --- IN√çCIO DO C√ìDIGO DE sentiment_analyzer.py --- #
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ARQV30 Enhanced v3.0 - External Sentiment Analyzer
M√≥dulo independente para an√°lise de sentimento e polaridade
"""


# Try to import TextBlob, fallback if not available
try:
    from textblob import TextBlob
    TEXTBLOB_AVAILABLE = True
except ImportError:
    TEXTBLOB_AVAILABLE = False
    logging.warning("TextBlob n√£o dispon√≠vel. Usando an√°lise b√°sica.")

# Try to import VADER, fallback if not available
try:
    from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer
    VADER_AVAILABLE = True
except ImportError:
    VADER_AVAILABLE = False
    logging.warning("VADER Sentiment n√£o dispon√≠vel.")

logger = logging.getLogger(__name__)


class ExternalSentimentAnalyzer:
    """Analisador de sentimento externo independente"""

    def __init__(self, config: Dict[str, Any]):
        """
        Inicializa o analisador de sentimento
        """
        self.config = config.get("sentiment_analysis", {})
        self.enabled = self.config.get("enabled", True)
        self.use_vader = self.config.get("use_vader", True) and VADER_AVAILABLE
        self.use_textblob = self.config.get(
            "use_textblob", True) and TEXTBLOB_AVAILABLE
        self.polarity_weights = self.config.get("polarity_weights", {
            "positive": 1.1,
            "negative": 0.8,
            "neutral": 1.0
        })

        # Initialize VADER if available and enabled
        if self.use_vader:
            self.vader_analyzer = SentimentIntensityAnalyzer()

        # Palavras para an√°lise b√°sica quando bibliotecas n√£o est√£o dispon√≠veis
        self.positive_words = {"bom", "√≥timo", "excelente", "maravilhoso", "perfeito", "incr√≠vel", "fant√°stico", "amor", "feliz", "alegre",
                               "positivo", "sucesso", "ganho", "oportunidade", "melhor", "bem", "confi√°vel", "seguro", "verdadeiro", "justo", "aprovado"}
        self.negative_words = {"ruim", "p√©ssimo", "terr√≠vel", "horr√≠vel", "odiar", "triste", "raiva", "problema", "erro", "falha", "negativo",
                               "fracasso", "perda", "amea√ßa", "pior", "mal", "duvidoso", "inseguro", "falso", "injusto", "rejeitado", "vi√©s", "desinforma√ß√£o"}

        logger.info(
            f"‚úÖ External Sentiment Analyzer inicializado (VADER: {self.use_vader}, TextBlob: {self.use_textblob})")

    def analyze_sentiment(self, text: str) -> Dict[str, float]:
        """
        Analisa o sentimento do texto fornecido

        Args:
            text (str): Texto para an√°lise

        Returns:
            Dict[str, float]: Resultados da an√°lise de sentimento
        """
        if not self.enabled or not text or not text.strip():
            return self._get_neutral_sentiment()

        try:
            # Clean text
            cleaned_text = self._clean_text(text)

            results = {
                "polarity": 0.0,
                "subjectivity": 0.0,
                "compound": 0.0,
                "positive": 0.0,
                "negative": 0.0,
                "neutral": 0.0,
                "classification": "neutral",
                "confidence": 0.0,
                "analysis_methods": []
            }

            # TextBlob Analysis
            if self.use_textblob:
                textblob_results = self._analyze_with_textblob(cleaned_text)
                results.update(textblob_results)
                results["analysis_methods"].append("textblob")

            # VADER Analysis
            if self.use_vader:
                vader_results = self._analyze_with_vader(cleaned_text)
                # Combine VADER results with TextBlob
                results["compound"] = vader_results["compound"]
                results["positive"] = vader_results["pos"]
                results["negative"] = vader_results["neg"]
                results["neutral"] = vader_results["neu"]
                results["analysis_methods"].append("vader")

                # Use VADER for final classification if available
                results["classification"] = self._classify_sentiment_vader(
                    vader_results)

            # Apply polarity weights
            results = self._apply_polarity_weights(results)

            # Calculate final confidence
            results["confidence"] = self._calculate_confidence(results)

            logger.debug(f"Sentiment analysis completed: {results['classification']} (confidence: {results['confidence']:.3f})")

            return results

        except Exception as e:
            logger.error(f"Erro na an√°lise de sentimento: {e}")
            return self._get_neutral_sentiment()

    def _clean_text(self, text: str) -> str:
        """
        Limpa o texto para an√°lise
        """
        if not text:
            return ""

        # Remove URLs
        text = re.sub(
            r"http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+", "", text)

        # Remove mentions and hashtags (keep the content)
        text = re.sub(r"[@#](\w+)", r"\\1", text)

        # Remove excessive whitespace
        text = re.sub(r"\\s+", " ", text).strip()

        return text

    def _analyze_with_textblob(self, text: str) -> Dict[str, float]:
        """
        An√°lise com TextBlob ou an√°lise b√°sica
        """
        try:
            if TEXTBLOB_AVAILABLE:
                blob = TextBlob(text)
                return {
                    "polarity": blob.sentiment.polarity,  # -1 to 1
                    "subjectivity": blob.sentiment.subjectivity  # 0 to 1
                }
            else:
                # An√°lise b√°sica usando palavras-chave
                return self._basic_sentiment_analysis(text)
        except Exception as e:
            logger.warning(f"Erro na an√°lise de sentimento: {e}")
            return {"polarity": 0.0, "subjectivity": 0.0}

    def _basic_sentiment_analysis(self, text: str) -> Dict[str, float]:
        """
        An√°lise b√°sica de sentimento usando palavras-chave
        """
        text_lower = text.lower()
        positive_count = sum(
            1 for word in self.positive_words if word in text_lower)
        negative_count = sum(
            1 for word in self.negative_words if word in text_lower)

        # Calcular polaridade b√°sica
        total_words = len(text_lower.split())
        if total_words == 0:
            return {"polarity": 0.0, "subjectivity": 0.0}

        polarity = (positive_count - negative_count) / max(total_words, 1)
        # Normalizar entre -1 e 1
        polarity = max(-1.0, min(1.0, polarity * 10))

        # Subjetividade baseada na quantidade de palavras emocionais
        subjectivity = min((positive_count + negative_count) /
                           max(total_words, 1) * 5, 1.0)

        return {"polarity": polarity, "subjectivity": subjectivity}

    def _analyze_with_vader(self, text: str) -> Dict[str, float]:
        """
        An√°lise com VADER
        """
        try:
            return self.vader_analyzer.polarity_scores(text)
        except Exception as e:
            logger.warning(f"Erro no VADER: {e}")
            return {"compound": 0.0, "pos": 0.0, "neg": 0.0, "neu": 1.0}

    def _classify_sentiment_vader(self, vader_scores: Dict[str, float]) -> str:
        """
        Classifica sentimento baseado nos scores do VADER
        """
        compound = vader_scores.get("compound", 0.0)

        if compound >= 0.05:
            return "positive"
        elif compound <= -0.05:
            return "negative"
        else:
            return "neutral"

    def _apply_polarity_weights(self, results: Dict[str, float]) -> Dict[str, float]:
        """
        Aplica pesos de polaridade configurados
        """
        classification = results.get("classification", "neutral")
        weight = self.polarity_weights.get(classification, 1.0)

        # Adjust polarity and compound scores
        if "polarity" in results:
            results["polarity"] *= weight
        if "compound" in results:
            results["compound"] *= weight

        return results

    def _calculate_confidence(self, results: Dict[str, float]) -> float:
        """
        Calcula confian√ßa da an√°lise
        """
        try:
            # Base confidence on the strength of sentiment indicators
            polarity_abs = abs(results.get("polarity", 0.0))
            compound_abs = abs(results.get("compound", 0.0))
            subjectivity = results.get("subjectivity", 0.0)

            # Higher absolute values indicate stronger sentiment (more confident)
            sentiment_strength = max(polarity_abs, compound_abs)

            # Subjectivity can indicate confidence (highly subjective = less reliable)
            subjectivity_penalty = subjectivity * 0.2

            # Method bonus (more methods = higher confidence)
            method_count = len(results.get("analysis_methods", []))
            method_bonus = min(method_count * 0.1, 0.2)

            confidence = min(sentiment_strength +
                             method_bonus - subjectivity_penalty, 1.0)
            confidence = max(confidence, 0.1)  # Minimum confidence

            return confidence

        except Exception as e:
            logger.warning(f"Erro no c√°lculo de confian√ßa: {e}")
            return 0.5

    def _get_neutral_sentiment(self) -> Dict[str, float]:
        """
        Retorna resultado neutro padr√£o
        """
        return {
            "polarity": 0.0,
            "subjectivity": 0.0,
            "compound": 0.0,
            "positive": 0.0,
            "negative": 0.0,
            "neutral": 1.0,
            "classification": "neutral",
            "confidence": 0.1,
            "analysis_methods": []
        }


# --- IN√çCIO DO C√ìDIGO DE bias_disinformation_detector.py --- #
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ARQV30 Enhanced v3.0 - External Bias & Disinformation Detector
M√≥dulo independente para detec√ß√£o de vi√©s e desinforma√ß√£o
"""


logger = logging.getLogger(__name__)


class ExternalBiasDisinformationDetector:
    """Detector de vi√©s e desinforma√ß√£o externo independente"""

    def __init__(self, config: Dict[str, Any]):
        """
        Inicializa o detector de vi√©s e desinforma√ß√£o
        """
        self.config = config.get("bias_detection", {})
        self.enabled = self.config.get("enabled", True)

        # Load configuration
        self.bias_keywords = self.config.get("bias_keywords", [])
        self.disinformation_patterns = self.config.get(
            "disinformation_patterns", [])
        self.rhetoric_devices = self.config.get("rhetoric_devices", [])

        logger.info(f"‚úÖ External Bias & Disinformation Detector inicializado")
        logger.debug(
            f"Bias keywords: {len(self.bias_keywords)}, Patterns: {len(self.disinformation_patterns)}")

    def detect_bias_disinformation(self, text: str) -> Dict[str, float]:
        """
        Detecta padr√µes de vi√©s e desinforma√ß√£o no texto

        Args:
            text (str): Texto para an√°lise

        Returns:
            Dict[str, float]: Resultados da detec√ß√£o
        """
        if not self.enabled or not text or not text.strip():
            return self._get_neutral_result()

        try:
            # Clean and prepare text
            cleaned_text = self._clean_text(text)
            text_lower = cleaned_text.lower()

            results = {
                "bias_score": 0.0,
                "disinformation_score": 0.0,
                "rhetoric_score": 0.0,
                "overall_risk": 0.0,
                "detected_bias_keywords": [],
                "detected_disinformation_patterns": [],
                "detected_rhetoric_devices": [],
                "confidence": 0.0,
                "analysis_details": {
                    "total_words": len(cleaned_text.split()),
                    "bias_matches": 0,
                    "disinformation_matches": 0,
                    "rhetoric_matches": 0
                }
            }

            # Detect bias keywords
            bias_analysis = self._detect_bias_keywords(text_lower)
            results.update(bias_analysis)

            # Detect disinformation patterns
            disinformation_analysis = self._detect_disinformation_patterns(
                text_lower)
            results.update(disinformation_analysis)

            # Detect rhetoric devices
            rhetoric_analysis = self._detect_rhetoric_devices(text_lower)
            results.update(rhetoric_analysis)

            # Calculate overall risk
            results["overall_risk"] = self._calculate_overall_risk(results)

            # Calculate confidence
            results["confidence"] = self._calculate_confidence(results)

            logger.debug(f"Bias/Disinformation analysis: risk={results['overall_risk']:.3f}, confidence={results['confidence']:.3f}")

            return results

        except Exception as e:
            logger.error(f"Erro na detec√ß√£o de vi√©s/desinforma√ß√£o: {e}")
            return self._get_neutral_result()

    def _clean_text(self, text: str) -> str:
        """
        Limpa o texto para an√°lise
        """
        if not text:
            return ""

        # Remove URLs, mentions, hashtags but keep text structure
        text = re.sub(
            r"http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+", "", text)
        text = re.sub(r"[@#](\w+)", r"\\1", text)
        text = re.sub(r"\\s+", " ", text).strip()

        return text

    def _detect_bias_keywords(self, text_lower: str) -> Dict[str, Any]:
        """
        Detecta palavras-chave de vi√©s
        """
        detected_keywords = []
        bias_score = 0.0

        for keyword in self.bias_keywords:
            if keyword.lower() in text_lower:
                detected_keywords.append(keyword)
                bias_score += 0.1  # Each bias keyword adds 0.1 to score

        # Normalize score (cap at 1.0)
        bias_score = min(bias_score, 1.0)

        return {
            "bias_score": bias_score,
            "detected_bias_keywords": detected_keywords,
            "analysis_details": {"bias_matches": len(detected_keywords)}
        }

    def _detect_disinformation_patterns(self, text_lower: str) -> Dict[str, Any]:
        """
        Detecta padr√µes de desinforma√ß√£o
        """
        detected_patterns = []
        disinformation_score = 0.0

        for pattern in self.disinformation_patterns:
            if pattern.lower() in text_lower:
                detected_patterns.append(pattern)
                disinformation_score += 0.15  # Each pattern adds more weight

        # Additional pattern detection with regex
        # Look for vague authority claims
        authority_patterns = [
            r"especialistas? (?:afirmam?|dizem?|garantem?)",
            r"estudos? (?:comprovam?|mostram?|indicam?)",
            r"pesquisas? (?:revelam?|demonstram?|apontam?)",
            r"cientistas? (?:descobriram?|provaram?|confirmaram?)"
        ]

        # Look for strawman fallacy
        strawman_patterns = [
            r"(?:eles|eles dizem|a esquerda|a direita) querem? (?:destruir|acabar com|impor) (?:nossa cultura|nossos valores|a fam√≠lia)",
            r"(?:argumento do espantalho|distorcem|exageram) (?:o que eu disse|nossas palavras)"
        ]

        # Look for ad hominem attacks
        ad_hominem_patterns = [
            r"(?:ele|ela|voc√™) n√£o tem moral para falar",
            r"(?:ignorante|burro|mentiroso|hip√≥crita) (?:para acreditar|para defender)"
        ]

        # Look for false dichotomy
        false_dichotomy_patterns = [
            r"(?:ou √©|ou voc√™ est√° com|ou voc√™ apoia) (?:n√≥s|eles) (?:ou contra n√≥s|ou contra eles)",
            r"(?:s√≥ existem|apenas duas? op√ß√µes?)"
        ]

        # Look for appeal to emotion
        appeal_to_emotion_patterns = [
            r"(?:pense nas crian√ßas|e se fosse voc√™|voc√™ n√£o se importa)",
            r"(?:chocante|absurdo|inacredit√°vel|revoltante)"
        ]

        all_regex_patterns = authority_patterns + strawman_patterns + \
            ad_hominem_patterns + false_dichotomy_patterns + appeal_to_emotion_patterns

        for pattern in all_regex_patterns:
            matches = re.findall(pattern, text_lower)
            if matches:
                detected_patterns.extend(matches)
                disinformation_score += len(matches) * 0.1

        # Normalize score
        disinformation_score = min(disinformation_score, 1.0)

        return {
            "disinformation_score": disinformation_score,
            "detected_disinformation_patterns": detected_patterns,
            "analysis_details": {"disinformation_matches": len(detected_patterns)}
        }

    def _detect_rhetoric_devices(self, text_lower: str) -> Dict[str, Any]:
        """
        Detecta dispositivos ret√≥ricos
        """
        detected_devices = []
        rhetoric_score = 0.0

        # Detect emotional manipulation patterns
        emotional_patterns = {
            "apelo ao medo": [r"perig(o|oso|osa)", r"risco", r"amea√ßa", r"cat√°strofe"],
            "apelo √† emo√ß√£o": [r"imaginem?", r"pensem?", r"sintam?"],
            "generaliza√ß√£o": [r"todos? (?:sabem?|fazem?)", r"ningu√©m", r"sempre", r"nunca"],
            "falsa dicotomia": [r"ou (?:voc√™|voc√™s?)", r"apenas duas? op√ß"]
        }

        for device_name, patterns in emotional_patterns.items():
            for pattern in patterns:
                if re.search(pattern, text_lower):
                    detected_devices.append(device_name)
                    rhetoric_score += 0.1
                    break  # Only count each device type once

        # Check configured rhetoric devices
        for device in self.rhetoric_devices:
            if device.lower() in text_lower:
                detected_devices.append(device)
                rhetoric_score += 0.1

        # Normalize score
        rhetoric_score = min(rhetoric_score, 1.0)

        return {
            "rhetoric_score": rhetoric_score,
            # Remove duplicates
            "detected_rhetoric_devices": list(set(detected_devices)),
            "analysis_details": {"rhetoric_matches": len(detected_devices)}
        }

    def _calculate_overall_risk(self, results: Dict[str, Any]) -> float:
        """
        Calcula o risco geral
        """
        # Weighted combination of different risk factors
        bias_weight = 0.3
        disinformation_weight = 0.4
        rhetoric_weight = 0.3

        overall_risk = (
            results.get("bias_score", 0.0) * bias_weight +
            results.get("disinformation_score", 0.0) * disinformation_weight +
            results.get("rhetoric_score", 0.0) * rhetoric_weight
        )

        return min(overall_risk, 1.0)

    def _calculate_confidence(self, results: Dict[str, Any]) -> float:
        """
        Calcula a confian√ßa da an√°lise
        """
        try:
            total_words = results.get(
                "analysis_details", {}).get("total_words", 1)
            total_matches = (
                len(results.get("detected_bias_keywords", [])) +
                len(results.get("detected_disinformation_patterns", [])) +
                len(results.get("detected_rhetoric_devices", []))
            )

            # Base confidence on detection density and text length
            if total_words < 10:
                return 0.3  # Low confidence for very short text

            detection_density = total_matches / total_words

            # Higher density = higher confidence in detection
            confidence = min(0.5 + (detection_density * 5), 1.0)

            # If no matches found, still have some confidence it's clean
            if total_matches == 0 and total_words > 20:
                confidence = 0.7
            elif total_matches == 0:
                confidence = 0.5

            return max(confidence, 0.1)

        except Exception as e:
            logger.warning(f"Erro no c√°lculo de confian√ßa: {e}")
            return 0.5

    def _get_neutral_result(self) -> Dict[str, float]:
        """
        Retorna resultado neutro padr√£o
        """
        return {
            "bias_score": 0.0,
            "disinformation_score": 0.0,
            "rhetoric_score": 0.0,
            "overall_risk": 0.0,
            "detected_bias_keywords": [],
            "detected_disinformation_patterns": [],
            "detected_rhetoric_devices": [],
            "confidence": 0.1,
            "analysis_details": {
                "total_words": 0,
                "bias_matches": 0,
                "disinformation_matches": 0,
                "rhetoric_matches": 0
            }
        }


# --- IN√çCIO DO C√ìDIGO DE confidence_thresholds.py --- #
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ARQV30 Enhanced v3.0 - External Confidence Thresholds
Gerenciador de limiares de confian√ßa para o m√≥dulo externo
"""


logger = logging.getLogger(__name__)


class ExternalConfidenceThresholds:
    """Gerenciador de limiares de confian√ßa externo"""

    def __init__(self, config: Dict[str, Any]):
        """
        Inicializa os limiares de confian√ßa
        """
        self.thresholds = config.get("thresholds", {})

        # Default thresholds if not provided
        self.default_thresholds = {
            "approval": 0.75,
            "rejection": 0.35,
            "high_confidence": 0.85,
            "low_confidence": 0.5,
            "sentiment_neutral": 0.1,
            "bias_high_risk": 0.7,
            "llm_minimum": 0.6
        }

        # Merge with defaults
        for key, default_value in self.default_thresholds.items():
            if key not in self.thresholds:
                self.thresholds[key] = default_value

        logger.info(
            f"‚úÖ External Confidence Thresholds inicializado com {len(self.thresholds)} limiares")
        logger.debug(f"Thresholds: {self.thresholds}")

    def get_threshold(self, score_type: str, default: Optional[float] = None) -> float:
        """
        Obt√©m o limiar para um tipo de pontua√ß√£o espec√≠fico

        Args:
            score_type (str): Tipo de pontua√ß√£o (e.g., "approval", "rejection")
            default (Optional[float]): Valor padr√£o se n√£o encontrado

        Returns:
            float: Limiar de confian√ßa
        """
        if score_type in self.thresholds:
            return self.thresholds[score_type]

        if default is not None:
            return default

        # Return a reasonable default based on score type
        if "approval" in score_type.lower():
            return 0.7
        elif "rejection" in score_type.lower():
            return 0.3
        elif "high" in score_type.lower():
            return 0.8
        elif "low" in score_type.lower():
            return 0.4
        else:
            return 0.5

    def should_approve(self, confidence: float) -> bool:
        """
        Verifica se deve aprovar baseado na confian√ßa
        """
        return confidence >= self.get_threshold("approval")

    def should_reject(self, confidence: float) -> bool:
        """
        Verifica se deve rejeitar baseado na confian√ßa
        """
        return confidence <= self.get_threshold("rejection")

    def is_ambiguous(self, confidence: float) -> bool:
        """
        Verifica se est√° em faixa amb√≠gua (entre rejection e approval)
        """
        rejection_threshold = self.get_threshold("rejection")
        approval_threshold = self.get_threshold("approval")
        return rejection_threshold < confidence < approval_threshold

    def is_high_confidence(self, confidence: float) -> bool:
        """
        Verifica se √© alta confian√ßa
        """
        return confidence >= self.get_threshold("high_confidence")

    def is_low_confidence(self, confidence: float) -> bool:
        """
        Verifica se √© baixa confian√ßa
        """
        return confidence <= self.get_threshold("low_confidence")

    def is_high_bias_risk(self, risk_score: float) -> bool:
        """
        Verifica se √© alto risco de vi√©s
        """
        return risk_score >= self.get_threshold("bias_high_risk")

    def classify_confidence_level(self, confidence: float) -> str:
        """
        Classifica o n√≠vel de confian√ßa

        Args:
            confidence (float): Pontua√ß√£o de confian√ßa

        Returns:
            str: N√≠vel de confian√ßa ("high", "medium", "low")
        """
        if self.is_high_confidence(confidence):
            return "high"
        elif self.is_low_confidence(confidence):
            return "low"
        else:
            return "medium"

    def get_decision_recommendation(self, confidence: float, risk_score: float = 0.0) -> Dict[str, Any]:
        """
        Recomenda uma decis√£o baseada na confian√ßa e risco

        Args:
            confidence (float): Pontua√ß√£o de confian√ßa
            risk_score (float): Pontua√ß√£o de risco

        Returns:
            Dict[str, Any]: Recomenda√ß√£o de decis√£o
        """
        # High risk overrides high confidence
        if self.is_high_bias_risk(risk_score):
            return {
                "decision": "reject",
                "reason": "Alto risco de vi√©s/desinforma√ß√£o detectado",
                "confidence_level": self.classify_confidence_level(confidence),
                "risk_level": "high",
                "requires_llm_analysis": False
            }

        # Clear approval
        if self.should_approve(confidence):
            return {
                "decision": "approve",
                "reason": "Alta confian√ßa na qualidade do conte√∫do",
                "confidence_level": self.classify_confidence_level(confidence),
                "risk_level": "low" if risk_score < 0.3 else "medium",
                "requires_llm_analysis": False
            }

        # Clear rejection
        if self.should_reject(confidence):
            return {
                "decision": "reject",
                "reason": "Baixa confian√ßa na qualidade do conte√∫do",
                "confidence_level": self.classify_confidence_level(confidence),
                "risk_level": "low" if risk_score < 0.3 else "medium",
                "requires_llm_analysis": False
            }

        # Ambiguous case - might need LLM analysis
        return {
            "decision": "ambiguous",
            "reason": "Confian√ßa em faixa amb√≠gua - requer an√°lise adicional",
            "confidence_level": self.classify_confidence_level(confidence),
            "risk_level": "medium" if risk_score < 0.5 else "high",
            "requires_llm_analysis": True
        }

    def update_threshold(self, score_type: str, new_value: float):
        """
        Atualiza um limiar espec√≠fico

        Args:
            score_type (str): Tipo de pontua√ß√£o
            new_value (float): Novo valor do limiar
        """
        if 0.0 <= new_value <= 1.0:
            self.thresholds[score_type] = new_value
            logger.info(
                f"Threshold '{score_type}' atualizado para {new_value}")
        else:
            logger.warning(
                f"Valor inv√°lido para threshold '{score_type}': {new_value} (deve estar entre 0.0 e 1.0)")

    def get_all_thresholds(self) -> Dict[str, float]:
        """
        Retorna todos os limiares configurados
        """
        return self.thresholds.copy()

    def validate_thresholds(self) -> bool:
        """
        Valida se os limiares est√£o configurados corretamente

        Returns:
            bool: True se v√°lidos, False caso contr√°rio
        """
        try:
            # Check that rejection < approval
            rejection = self.get_threshold("rejection")
            approval = self.get_threshold("approval")

            if rejection >= approval:
                logger.error(
                    f"Configura√ß√£o inv√°lida: rejection ({rejection}) deve ser menor que approval ({approval})")
                return False

            # Check that all thresholds are in valid range
            for key, value in self.thresholds.items():
                if not (0.0 <= value <= 1.0):
                    logger.error(
                        f"Threshold '{key}' fora do range v√°lido (0.0-1.0): {value}")
                    return False

            logger.info("‚úÖ Todos os thresholds s√£o v√°lidos")
            return True

        except Exception as e:
            logger.error(f"Erro na valida√ß√£o dos thresholds: {e}")
            return False


# --- IN√çCIO DO C√ìDIGO DE contextual_analyzer.py --- #
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ARQV30 Enhanced v3.0 - External Contextual Analyzer
Analisador contextual para o m√≥dulo externo de verifica√ß√£o
"""


logger = logging.getLogger(__name__)


class ExternalContextualAnalyzer:
    """Analisador contextual externo independente"""

    def __init__(self, config: Dict[str, Any]):
        """
        Inicializa o analisador contextual
        """
        self.config = config.get("contextual_analysis", {})
        self.enabled = self.config.get("enabled", True)
        self.check_consistency = self.config.get("check_consistency", True)
        self.analyze_source_reliability = self.config.get(
            "analyze_source_reliability", True)
        self.verify_temporal_coherence = self.config.get(
            "verify_temporal_coherence", True)

        # Initialize context cache for cross-item analysis
        self.context_cache = {
            "processed_items": [],
            "source_patterns": {},
            "content_patterns": {},
            "temporal_markers": []
        }

        logger.info(f"‚úÖ External Contextual Analyzer inicializado")
        logger.debug(
            f"Configura√ß√µes: consistency={self.check_consistency}, source={self.analyze_source_reliability}, temporal={self.verify_temporal_coherence}")

    def analyze_context(self, item_data: Dict[str, Any], massive_data: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:
        """
        Analisa o item em contexto mais amplo

        Args:
            item_data (Dict[str, Any]): Dados do item individual
            massive_data (Optional[Dict[str, Any]]): Dados contextuais mais amplos

        Returns:
            Dict[str, Any]: An√°lise contextual
        """
        if not self.enabled:
            return self._get_neutral_result()

        try:
            # Initialize context analysis result
            context_result = {
                "contextual_confidence": 0.5,
                "consistency_score": 0.5,
                "source_reliability_score": 0.5,
                "temporal_coherence_score": 0.5,
                "context_flags": [],
                "context_insights": [],
                "adjustment_factor": 0.0
            }

            # Extract text content for analysis
            text_content = self._extract_text_content(item_data)

            # Perform different types of contextual analysis
            if self.check_consistency:
                consistency_analysis = self._analyze_consistency(
                    text_content, item_data, massive_data)
                context_result.update(consistency_analysis)

            if self.analyze_source_reliability:
                source_analysis = self._analyze_source_reliability(
                    item_data, massive_data)
                context_result.update(source_analysis)

            if self.verify_temporal_coherence:
                temporal_analysis = self._analyze_temporal_coherence(
                    text_content, item_data)
                context_result.update(temporal_analysis)

            # Calculate overall contextual confidence
            context_result["contextual_confidence"] = self._calculate_contextual_confidence(
                context_result)

            # Update context cache for future analysis
            self._update_context_cache(item_data, context_result)

            logger.debug(f"Context analysis: confidence={context_result['contextual_confidence']:.3f}")

            return context_result

        except Exception as e:
            logger.error(f"Erro na an√°lise contextual: {e}")
            return self._get_neutral_result()

    def _extract_text_content(self, item_data: Dict[str, Any]) -> str:
        """
        Extrai conte√∫do textual relevante do item
        """
        content_fields = ["content", "text", "title", "description", "summary"]

        text_content = ""
        for field in content_fields:
            if field in item_data and item_data[field]:
                text_content += f" {item_data[field]}"

        return text_content.strip()

    def _analyze_consistency(self, text_content: str, item_data: Dict[str, Any], massive_data: Optional[Dict[str, Any]]) -> Dict[str, Any]:
        """
        Analisa consist√™ncia interna e externa
        """
        consistency_result = {
            "consistency_score": 0.5,
            "consistency_flags": [],
            "consistency_insights": []
        }

        try:
            score = 0.5
            flags = []
            insights = []

            # Check internal consistency
            internal_score, internal_flags = self._check_internal_consistency(
                text_content)
            score = (score + internal_score) / 2
            flags.extend(internal_flags)

            # Check consistency with previous items if available
            if self.context_cache["processed_items"]:
                external_score, external_flags = self._check_external_consistency(
                    text_content, item_data)
                score = (score + external_score) / 2
                flags.extend(external_flags)

                if external_score < 0.3:
                    insights.append(
                        "Conte√∫do inconsistente com padr√µes anteriores")
                elif external_score > 0.8:
                    insights.append(
                        "Conte√∫do altamente consistente com padr√µes estabelecidos")

            consistency_result.update({
                "consistency_score": score,
                "consistency_flags": flags,
                "consistency_insights": insights
            })

        except Exception as e:
            logger.warning(f"Erro na an√°lise de consist√™ncia: {e}")

        return consistency_result

    def _check_internal_consistency(self, text_content: str) -> tuple:
        """
        Verifica consist√™ncia interna do texto
        """
        score = 0.7  # Start with good assumption
        flags = []

        if not text_content or len(text_content.strip()) < 10:
            return 0.3, ["Conte√∫do muito curto para an√°lise de consist√™ncia"]

        # Check for contradictory statements
        contradiction_patterns = [
            (r"sempre.*nunca", "Contradi√ß√£o: 'sempre' e 'nunca' no mesmo contexto"),
            (r"todos?.*ningu√©m", "Contradi√ß√£o: generaliza√ß√£o conflitante"),
            (r"imposs√≠vel.*poss√≠vel", "Contradi√ß√£o: possibilidade conflitante"),
            (r"verdade.*mentira", "Contradi√ß√£o: veracidade conflitante")
        ]

        for pattern, flag_msg in contradiction_patterns:
            if re.search(pattern, text_content.lower()):
                score -= 0.2
                flags.append(flag_msg)

        # Check for temporal inconsistencies
        temporal_patterns = [
            r"ontem.*amanh√£",
            r"passado.*futuro.*hoje",
            r"antes.*depois.*simultaneamente"
        ]

        for pattern in temporal_patterns:
            if re.search(pattern, text_content.lower()):
                score -= 0.1
                flags.append("Poss√≠vel inconsist√™ncia temporal")

        return max(score, 0.0), flags

    def _check_external_consistency(self, text_content: str, item_data: Dict[str, Any]) -> tuple:
        """
        Verifica consist√™ncia com itens processados anteriormente
        """
        score = 0.5
        flags = []

        try:
            # Compare with recent processed items
            # Last 5 items
            recent_items = self.context_cache["processed_items"][-5:]

            if not recent_items:
                return 0.5, []

            # Simple keyword-based similarity check
            current_words = set(text_content.lower().split())

            similarity_scores = []
            for prev_item in recent_items:
                prev_words = set(prev_item.get("text", "").lower().split())
                if prev_words:
                    intersection = len(current_words & prev_words)
                    union = len(current_words | prev_words)
                    similarity = intersection / union if union > 0 else 0
                    similarity_scores.append(similarity)

            if similarity_scores:
                avg_similarity = sum(similarity_scores) / \
                    len(similarity_scores)

                # Very high similarity might indicate duplication
                if avg_similarity > 0.9:
                    score = 0.3
                    flags.append(
                        "Conte√∫do muito similar a itens anteriores (poss√≠vel duplica√ß√£o)")
                # Very low similarity might be inconsistent
                elif avg_similarity < 0.1:
                    score = 0.4
                    flags.append(
                        "Conte√∫do muito diferente do padr√£o estabelecido")
                else:
                    score = 0.7  # Good consistency

        except Exception as e:
            logger.warning(f"Erro na verifica√ß√£o de consist√™ncia externa: {e}")

        return score, flags

    def _analyze_source_reliability(self, item_data: Dict[str, Any], massive_data: Optional[Dict[str, Any]]) -> Dict[str, Any]:
        """
        Analisa confiabilidade da fonte
        """
        source_result = {
            "source_reliability_score": 0.5,
            "source_flags": [],
            "source_insights": []
        }

        try:
            score = 0.5
            flags = []
            insights = []

            # Extract source information
            source_info = self._extract_source_info(item_data)

            if not source_info:
                score = 0.3
                flags.append("Fonte n√£o identificada")
                return {**source_result, "source_reliability_score": score, "source_flags": flags}

            # Check source patterns
            source_domain = source_info.get("domain", "").lower()

            # Known reliable patterns
            reliable_indicators = [
                ".edu", ".gov", ".org",
                "academia", "university", "instituto",
                "pesquisa", "ciencia", "journal"
            ]

            unreliable_indicators = [
                "blog", "forum", "social",
                "fake", "rumor", "gossip"
            ]

            for indicator in reliable_indicators:
                if indicator in source_domain:
                    score += 0.2
                    insights.append(
                        f"Fonte cont√©m indicador confi√°vel: {indicator}")
                    break

            for indicator in unreliable_indicators:
                if indicator in source_domain:
                    score -= 0.3
                    flags.append(
                        f"Fonte cont√©m indicador de baixa confiabilidade: {indicator}")
                    break

            # Check source history in cache
            if source_domain in self.context_cache["source_patterns"]:
                source_stats = self.context_cache["source_patterns"][source_domain]
                avg_quality = source_stats.get("avg_quality", 0.5)

                if avg_quality > 0.7:
                    score += 0.1
                    insights.append("Fonte com hist√≥rico positivo")
                elif avg_quality < 0.4:
                    score -= 0.1
                    flags.append("Fonte com hist√≥rico problem√°tico")

            score = min(max(score, 0.0), 1.0)

            source_result.update({
                "source_reliability_score": score,
                "source_flags": flags,
                "source_insights": insights
            })

        except Exception as e:
            logger.warning(f"Erro na an√°lise de fonte: {e}")

        return source_result

    def _extract_source_info(self, item_data: Dict[str, Any]) -> Dict[str, str]:
        """
        Extrai informa√ß√µes da fonte
        """
        source_fields = ["source", "url", "domain", "author", "publisher"]
        source_info = {}

        for field in source_fields:
            if field in item_data and item_data[field]:
                source_info[field] = str(item_data[field])

        # Extract domain from URL if available
        if "url" in source_info and "domain" not in source_info:
            try:
                import urllib.parse
                parsed = urllib.parse.urlparse(source_info["url"])
                source_info["domain"] = parsed.netloc
            except:
                pass

        return source_info

    def _analyze_temporal_coherence(self, text_content: str, item_data: Dict[str, Any]) -> Dict[str, Any]:
        """
        Analisa coer√™ncia temporal
        """
        temporal_result = {
            "temporal_coherence_score": 0.5,
            "temporal_flags": [],
            "temporal_insights": []
        }

        try:
            score = 0.7
            flags = []
            insights = []

            # Extract temporal markers
            temporal_markers = self._extract_temporal_markers(text_content)

            # Check for temporal inconsistencies
            if len(temporal_markers) > 1:
                coherent, issues = self._check_temporal_coherence(
                    temporal_markers)
                if not coherent:
                    score -= 0.3
                    flags.extend(issues)
                else:
                    insights.append("Marcadores temporais coerentes")

            # Check against item timestamp if available
            if "timestamp" in item_data or "date" in item_data:
                item_time = item_data.get("timestamp") or item_data.get("date")
                temporal_consistency = self._check_item_temporal_consistency(
                    temporal_markers, item_time)
                if temporal_consistency < 0.5:
                    score -= 0.2
                    flags.append(
                        "Inconsist√™ncia entre conte√∫do e timestamp do item")

            temporal_result.update({
                "temporal_coherence_score": max(score, 0.0),
                "temporal_flags": flags,
                "temporal_insights": insights
            })

        except Exception as e:
            logger.warning(f"Erro na an√°lise temporal: {e}")

        return temporal_result

    def _extract_temporal_markers(self, text: str) -> List[str]:
        """
        Extrai marcadores temporais do texto
        """
        temporal_patterns = [
            r"(?:ontem|hoje|amanh√£)",
            r"(?:esta|pr√≥xima|passada)\\s+(?:semana|segunda|ter√ßa|quarta|quinta|sexta|s√°bado|domingo)",
            r"(?:este|pr√≥ximo|passado)\\s+(?:m√™s|ano)",
            r"(?:janeiro|fevereiro|mar√ßo|abril|maio|junho|julho|agosto|setembro|outubro|novembro|dezembro)",
            r"(?:2019|2020|2021|2022|2023|2024|2025)",
            r"h√°\\s+\\d+\\s+(?:dias?|meses?|anos?)",
            r"em\\s+\\d+\\s+(?:dias?|meses?|anos?)"
        ]

        markers = []
        for pattern in temporal_patterns:
            matches = re.findall(pattern, text.lower())
            markers.extend(matches)

        return markers

    def _check_temporal_coherence(self, markers: List[str]) -> tuple:
        """
        Verifica coer√™ncia entre marcadores temporais
        """
        # Simple coherence check - this could be made more sophisticated
        issues = []

        # Check for obvious contradictions
        if any("ontem" in m for m in markers) and any("amanh√£" in m for m in markers):
            issues.append(
                "Contradi√ß√£o temporal: 'ontem' e 'amanh√£' no mesmo contexto")

        # Check for year contradictions
        years = [m for m in markers if re.search(r"20\\d{2}", m)]
        if len(set(years)) > 2:
            issues.append(
                "M√∫ltiplos anos mencionados - poss√≠vel inconsist√™ncia")

        return len(issues) == 0, issues

    def _check_item_temporal_consistency(self, markers: List[str], item_time: str) -> float:
        """
        Verifica consist√™ncia temporal com timestamp do item
        """
        try:
            # Simple check - this could be enhanced
            current_year = datetime.now().year

            # Check if markers mention current year
            mentions_current_year = any(
                str(current_year) in m for m in markers)

            # Check if item_time is recent
            item_datetime = datetime.fromisoformat(
                item_time) if isinstance(item_time, str) else item_time
            # Within last year
            is_recent = (datetime.now() - item_datetime) < timedelta(days=365)

            score = 0.5
            if mentions_current_year and is_recent:
                score += 0.2
            elif not mentions_current_year and not is_recent:
                score += 0.1  # Consistent in being old
            elif mentions_current_year and not is_recent:
                score -= 0.3  # Inconsistent - mentions current year but item is old
            elif not mentions_current_year and is_recent:
                score -= 0.1  # Inconsistent - doesn't mention current year but item is recent

            return max(score, 0.0)

        except Exception as e:
            logger.warning(
                f"Erro na verifica√ß√£o de consist√™ncia temporal do item: {e}")
            return 0.5

    def _calculate_contextual_confidence(self, context_result: Dict[str, Any]) -> float:
        """
        Calcula a confian√ßa contextual geral
        """
        consistency_score = context_result.get("consistency_score", 0.5)
        source_reliability_score = context_result.get(
            "source_reliability_score", 0.5)
        temporal_coherence_score = context_result.get(
            "temporal_coherence_score", 0.5)

        # Weighted average
        confidence = (
            consistency_score * 0.4 +
            source_reliability_score * 0.4 +
            temporal_coherence_score * 0.2
        )

        # Apply penalty for flags
        if context_result.get("context_flags"):
            confidence -= len(context_result["context_flags"]) * 0.05

        return min(max(confidence, 0.1), 1.0)

    def _update_context_cache(self, item_data: Dict[str, Any], context_result: Dict[str, Any]):
        """
        Atualiza o cache de contexto com o item processado
        """
        self.context_cache["processed_items"].append({
            "id": item_data.get("id"),
            "text": self._extract_text_content(item_data),
            "timestamp": item_data.get("timestamp") or datetime.now().isoformat(),
            "source_domain": self._extract_source_info(item_data).get("domain"),
            "quality_score": context_result.get("contextual_confidence", 0.5)
        })

        # Keep cache size manageable
        if len(self.context_cache["processed_items"]) > 100:
            self.context_cache["processed_items"].pop(0)

        # Update source patterns
        source_domain = self._extract_source_info(item_data).get("domain")
        if source_domain:
            if source_domain not in self.context_cache["source_patterns"]:
                self.context_cache["source_patterns"][source_domain] = {
                    "total_items": 0,
                    "total_quality": 0.0,
                    "avg_quality": 0.0
                }

            source_stats = self.context_cache["source_patterns"][source_domain]
            source_stats["total_items"] += 1
            source_stats["total_quality"] += context_result.get(
                "contextual_confidence", 0.5)
            source_stats["avg_quality"] = source_stats["total_quality"] / \
                source_stats["total_items"]

    def _get_neutral_result(self) -> Dict[str, Any]:
        """
        Retorna um resultado neutro padr√£o para an√°lise contextual
        """
        return {
            "contextual_confidence": 0.1,
            "consistency_score": 0.1,
            "source_reliability_score": 0.1,
            "temporal_coherence_score": 0.1,
            "context_flags": ["An√°lise contextual desabilitada ou falhou"],
            "context_insights": [],
            "adjustment_factor": 0.0
        }


# --- IN√çCIO DO C√ìDIGO DE llm_reasoning_service.py --- #
#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ARQV30 Enhanced v3.0 - External LLM Reasoning Service
Servi√ßo de racioc√≠nio com LLMs para an√°lise aprofundada
"""


# Try to import LLM clients, fallback gracefully
try:
    import google.generativeai as genai
    GEMINI_AVAILABLE = True
except ImportError:
    GEMINI_AVAILABLE = False

try:
    import openai
    OPENAI_AVAILABLE = True
except ImportError:
    OPENAI_AVAILABLE = False

logger = logging.getLogger(__name__)


class ExternalLLMReasoningService:
    """Servi√ßo de racioc√≠nio com LLMs externo independente"""

    def __init__(self, config: Dict[str, Any]):
        """
        Inicializa o servi√ßo de LLM
        """
        self.config = config.get("llm_reasoning", {})
        self.enabled = self.config.get("enabled", True)
        self.provider = self.config.get("provider", "gemini").lower()
        self.model = self.config.get("model", "gemini-1.5-flash")
        self.max_tokens = self.config.get("max_tokens", 1000)
        self.temperature = self.config.get("temperature", 0.3)
        self.confidence_threshold = self.config.get(
            "confidence_threshold", 0.6)

        self.client = None
        self._initialize_llm_client()

        logger.info(
            f"‚úÖ External LLM Reasoning Service inicializado (Provider: {self.provider}, Available: {self.client is not None})")

    def _initialize_llm_client(self):
        """
        Inicializa o cliente LLM baseado no provider configurado
        """
        try:
            if self.provider == "gemini" and GEMINI_AVAILABLE:
                api_key = os.getenv("GEMINI_API_KEY")
                if api_key:
                    genai.configure(api_key=api_key)
                    self.client = genai.GenerativeModel(self.model)
                    logger.info(f"‚úÖ Gemini client inicializado: {self.model}")
                else:
                    logger.warning("‚ö†Ô∏è GEMINI_API_KEY n√£o configurada")

            elif self.provider == "openai" and OPENAI_AVAILABLE:
                api_key = os.getenv("OPENAI_API_KEY")
                if api_key:
                    openai.api_key = api_key
                    self.client = openai
                    logger.info(f"‚úÖ OpenAI client inicializado: {self.model}")
                else:
                    logger.warning("‚ö†Ô∏è OPENAI_API_KEY n√£o configurada")
            else:
                logger.warning(
                    f"‚ö†Ô∏è Provider \'{self.provider}\' n√£o dispon√≠vel ou n√£o configurado")

        except Exception as e:
            logger.error(f"Erro ao inicializar LLM client: {e}")
            self.client = None

    def analyze_with_llm(self, text: str, context: str = "") -> Dict[str, Any]:
        """
        Analisa o texto com LLM para racioc√≠nio aprofundado

        Args:
            text (str): Texto para an√°lise
            context (str): Contexto adicional

        Returns:
            Dict[str, Any]: Resultados da an√°lise LLM
        """
        if not self.enabled or not self.client or not text or not text.strip():
            return self._get_default_result()

        try:
            # Prepare prompt for analysis
            prompt = self._create_analysis_prompt(text, context)

            # Get LLM response
            if self.provider == "gemini":
                response = self._analyze_with_gemini(prompt)
            elif self.provider == "openai":
                response = self._analyze_with_openai(prompt)
            else:
                return self._get_default_result()

            # Parse and structure response
            analysis_result = self._parse_llm_response(response, text)

            logger.debug(f"LLM analysis completed: confidence={analysis_result.get('llm_confidence', 0):.3f}")

            return analysis_result

        except Exception as e:
            logger.error(f"Erro na an√°lise LLM: {e}")
            return self._get_default_result()

    def _create_analysis_prompt(self, text: str, context: str = "") -> str:
        """
        Cria o prompt para an√°lise LLM
        """
        base_prompt = f"""Analise o seguinte texto de forma cr√≠tica e objetiva:

TEXTO PARA AN√ÅLISE:
\"{text}\"

{f'CONTEXTO ADICIONAL: {context}' if context else ''}

Por favor, forne√ßa uma an√°lise estruturada e detalhada, avaliando os seguintes aspectos:

1.  **QUALIDADE DO CONTE√öDO (0-10):**
    -   **Clareza e Coer√™ncia:** A informa√ß√£o √© apresentada de forma l√≥gica e f√°cil de entender?
    -   **Profundidade e Abrang√™ncia:** O t√≥pico √© abordado de maneira completa ou superficial?
    -   **Originalidade:** O conte√∫do oferece novas perspectivas ou √© uma repeti√ß√£o de informa√ß√µes existentes?

2.  **CONFIABILIDADE E FONTES (0-10):**
    -   **Verificabilidade:** As afirma√ß√µes podem ser verificadas? H√° links ou refer√™ncias para fontes prim√°rias ou secund√°rias respeit√°veis?
    -   **Atualidade:** A informa√ß√£o est√° atualizada? H√° datas de publica√ß√£o ou revis√£o?
    -   **Reputa√ß√£o da Fonte:** A fonte (autor, ve√≠culo) √© conhecida por sua credibilidade e precis√£o?

3.  **VI√âS E PARCIALIDADE (0-10, onde 0=neutro, 10=muito tendencioso):**
    -   **Linguagem:** H√° uso de linguagem emotiva, carregada, ou termos que buscam influenciar a opini√£o do leitor?
    -   **Perspectiva:** O conte√∫do apresenta m√∫ltiplos lados de uma quest√£o ou foca apenas em uma vis√£o unilateral?
    -   **Omisso:** H√° informa√ß√µes relevantes que foram intencionalmente omitidas para favorecer uma narrativa?
    -   **Generaliza√ß√µes:** H√° uso de generaliza√ß√µes excessivas ou estere√≥tipos?

4.  **RISCO DE DESINFORMA√á√ÉO (0-10):**
    -   **Fatos:** H√° afirma√ß√µes factuais que s√£o comprovadamente falsas ou enganosas?
    -   **Padr√µes:** O conte√∫do exibe padr√µes conhecidos de desinforma√ß√£o (ex: clickbait, teorias da conspira√ß√£o, manipula√ß√£o de imagens/v√≠deos)?
    -   **Contexto:** O conte√∫do √© apresentado fora de contexto para alterar sua percep√ß√£o?

5.  **RECOMENDA√á√ÉO FINAL:**
    -   **Status:** [APROVAR/REJEITAR/REVIS√ÉO_MANUAL]
    -   **Raz√£o Principal:** [breve justificativa para o status]

6.  **CONFIAN√áA DA AN√ÅLISE DO LLM:**
    -   **Pontua√ß√£o:** [0-100]% - [justificativa da confian√ßa do pr√≥prio LLM na sua an√°lise]

Forne√ßa sua resposta estritamente no seguinte formato, com cada item em uma nova linha:
QUALIDADE: [pontua√ß√£o]/10 - [breve justificativa]
CONFIABILIDADE: [pontua√ß√£o]/10 - [breve justificativa]
VI√âS: [pontua√ß√£o]/10 - [breve justificativa]
DESINFORMA√á√ÉO: [pontua√ß√£o]/10 - [breve justificativa]
RECOMENDA√á√ÉO: [APROVAR/REJEITAR/REVIS√ÉO_MANUAL] - [raz√£o principal]
CONFIAN√áA_AN√ÅLISE: [0-100]% - [justificativa da confian√ßa]"""

        return base_prompt

    def _analyze_with_gemini(self, prompt: str) -> str:
        """
        An√°lise com Gemini
        """
        try:
            response = self.client.generate_content(
                prompt,
                generation_config=genai.types.GenerationConfig(
                    max_output_tokens=self.max_tokens,
                    temperature=self.temperature
                )
            )
            return response.text
        except Exception as e:
            logger.error(f"Erro no Gemini: {e}")
            raise

    def _analyze_with_openai(self, prompt: str) -> str:
        """
        An√°lise com OpenAI
        """
        try:
            response = openai.ChatCompletion.create(
                model=self.model,
                messages=[{"role": "user", "content": prompt}],
                max_tokens=self.max_tokens,
                temperature=self.temperature
            )
            return response.choices[0].message.content
        except Exception as e:
            logger.error(f"Erro no OpenAI: {e}")
            raise

    def _parse_llm_response(self, response: str, original_text: str) -> Dict[str, Any]:
        """
        Parse da resposta LLM para formato estruturado
        """
        try:
            import re

            # Initialize result structure
            result = {
                "llm_response": response,
                "quality_score": 5.0,
                "reliability_score": 5.0,
                "bias_score": 5.0,
                "disinformation_score": 5.0,
                "llm_recommendation": "REVIS√ÉO_MANUAL",
                "llm_confidence": 0.5,
                "analysis_reasoning": "",
                "provider": self.provider,
                "model": self.model
            }

            # Extract scores using regex
            patterns = {
                "quality_score": r"QUALIDADE:\\s*([0-9]+(?:\\.[0-9]+)?)",
                "reliability_score": r"CONFIABILIDADE:\\s*([0-9]+(?:\\.[0-9]+)?)",
                "bias_score": r"VI√âS:\\s*([0-9]+(?:\\.[0-9]+)?)",
                "disinformation_score": r"DESINFORMA√á√ÉO:\\s*([0-9]+(?:\\.[0-9]+)?)",
                "llm_recommendation": r"RECOMENDA√á√ÉO:\\s*(APROVAR|REJEITAR|REVIS√ÉO_MANUAL)",
                "llm_confidence": r"CONFIAN√áA_AN√ÅLISE:\\s*([0-9]+)%?"
            }

            for key, pattern in patterns.items():
                match = re.search(pattern, response, re.IGNORECASE)
                if match:
                    if key == "llm_confidence":
                        result[key] = min(float(match.group(1)) / 100.0, 1.0)
                    elif key == "llm_recommendation":
                        result[key] = match.group(1).upper()
                    else:
                        # Convert 0-10 scores to 0-1 range
                        score = min(float(match.group(1)) / 10.0, 1.0)
                        result[key] = score

            # Extract reasoning from response
            reasoning_parts = []
            for line in response.split("\\n"):
                if " - " in line and any(keyword in line.upper() for keyword in ["QUALIDADE", "CONFIABILIDADE", "VI√âS", "DESINFORMA√á√ÉO"]):
                    reasoning_parts.append(line.split(" - ", 1)[-1])

            result["analysis_reasoning"] = " | ".join(reasoning_parts)

            # Validate and adjust confidence based on consistency
            result["llm_confidence"] = self._validate_llm_confidence(result)

            return result

        except Exception as e:
            logger.warning(f"Erro no parsing da resposta LLM: {e}")
            # Return response as-is with default scores
            return {
                "llm_response": response,
                "quality_score": 0.5,
                "reliability_score": 0.5,
                "bias_score": 0.5,
                "disinformation_score": 0.5,
                "llm_recommendation": "REVIS√ÉO_MANUAL",
                "llm_confidence": 0.3,
                "analysis_reasoning": "Erro no parsing da resposta",
                "provider": self.provider,
                "model": self.model
            }

    def _validate_llm_confidence(self, result: Dict[str, Any]) -> float:
        """
        Valida e ajusta a confian√ßa baseada na consist√™ncia da an√°lise
        """
        try:
            # Check consistency between recommendation and scores
            quality = result.get("quality_score", 0.5)
            reliability = result.get("reliability_score", 0.5)
            bias = result.get("bias_score", 0.5)
            disinformation = result.get("disinformation_score", 0.5)
            recommendation = result.get("llm_recommendation", "REVIS√ÉO_MANUAL")
            base_confidence = result.get("llm_confidence", 0.5)

            # Calculate expected recommendation based on scores
            avg_positive_scores = (quality + reliability) / 2.0
            avg_negative_scores = (bias + disinformation) / 2.0

            expected_approval = avg_positive_scores > 0.7 and avg_negative_scores < 0.4
            expected_rejection = avg_positive_scores < 0.4 or avg_negative_scores > 0.6

            # Check consistency
            consistency_bonus = 0.0
            if recommendation == "APROVAR" and expected_approval:
                consistency_bonus = 0.1
            elif recommendation == "REJEITAR" and expected_rejection:
                consistency_bonus = 0.1
            elif recommendation == "REVIS√ÉO_MANUAL":
                consistency_bonus = 0.05  # Neutral is always somewhat consistent

            # Adjust confidence
            adjusted_confidence = min(base_confidence + consistency_bonus, 1.0)
            adjusted_confidence = max(adjusted_confidence, 0.1)

            return adjusted_confidence

        except Exception as e:
            logger.warning(f"Erro na valida√ß√£o de confian√ßa: {e}")
            return 0.5

    def _get_default_result(self) -> Dict[str, Any]:
        """
        Retorna resultado padr√£o quando LLM n√£o est√° dispon√≠vel
        """
        return {
            "llm_response": "LLM n√£o dispon√≠vel ou configurado",
            "quality_score": 0.5,
            "reliability_score": 0.5,
            "bias_score": 0.5,
            "disinformation_score": 0.5,
            "llm_recommendation": "REVIS√ÉO_MANUAL",
            "llm_confidence": 0.1,
            "analysis_reasoning": "An√°lise LLM n√£o dispon√≠vel",
            "provider": self.provider,
            "model": self.model
        }


# --- IN√çCIO DO C√ìDIGO DE external_review_agent.py --- #
